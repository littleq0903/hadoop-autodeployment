<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
   <title>The Apache HBase&#153; Reference Guide</title><link rel="stylesheet" type="text/css" href="css/freebsd_docbook.css"><meta name="generator" content="DocBook XSL-NS Stylesheets V1.76.1"><meta name="description" content="This is the official reference guide of Apache HBase&#153;, a distributed, versioned, big data store built on top of Apache Hadoop&#153; and Apache ZooKeeper&#153;."></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><div class="book" title="The Apache HBase&#153; Reference Guide"><div class="titlepage"><div><div><h1 class="title"><a name="book"></a><a class="link" href="http://www.hbase.org" target="_top">
    The Apache HBase&#153; Reference Guide
    </a></h1></div><div><h2 class="subtitle"><a class="link" href="http://www.hbase.org" target="_top">
           <span class="inlinemediaobject"><img src="images/hbase_logo.png" align="middle"></span>
       </a>
    </h2></div><div><p class="copyright">Copyright &copy; 2014 Apache Software Foundation.
        All Rights Reserved.  Apache Hadoop, Hadoop, MapReduce, HDFS, Zookeeper, HBase, and the HBase project logo are trademarks of the Apache Software Foundation.
        </p></div><div><div class="revhistory"><table border="1" width="100%" summary="Revision history"><tr><th align="left" valign="top" colspan="2"><b>Revision History</b></th></tr><tr><td align="left">Revision 
          0.98.1-hadoop2
        </td><td align="left">
          2014-03-29T17:12
        </td></tr></table></div></div><div><div class="abstract" title="Abstract"><p class="title"><b>Abstract</b></p><p>This is the official reference guide of
    <a class="link" href="http://www.hbase.org" target="_top">Apache HBase&#153;</a>,
    a distributed, versioned, big data store built on top of
    <a class="link" href="http://hadoop.apache.org/" target="_top">Apache Hadoop&#153;</a> and
    <a class="link" href="http://zookeeper.apache.org/" target="_top">Apache ZooKeeper&#153;</a>.
      </p></div></div></div><hr></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="preface"><a href="#preface">Preface</a></span></dt><dt><span class="chapter"><a href="#getting_started">1. Getting Started</a></span></dt><dd><dl><dt><span class="section"><a href="#d366e83">1.1. Introduction</a></span></dt><dt><span class="section"><a href="#quickstart">1.2. Quick Start</a></span></dt></dl></dd><dt><span class="chapter"><a href="#configuration">2. Apache HBase Configuration</a></span></dt><dd><dl><dt><span class="section"><a href="#basic.prerequisites">2.1. Basic Prerequisites</a></span></dt><dt><span class="section"><a href="#standalone_dist">2.2. HBase run modes: Standalone and Distributed</a></span></dt><dt><span class="section"><a href="#config.files">2.3. Configuration Files</a></span></dt><dt><span class="section"><a href="#example_config">2.4. Example Configurations</a></span></dt><dt><span class="section"><a href="#important_configurations">2.5. The Important Configurations</a></span></dt></dl></dd><dt><span class="chapter"><a href="#upgrading">3. Upgrading</a></span></dt><dd><dl><dt><span class="section"><a href="#hbase.versioning">3.1. HBase version numbers</a></span></dt><dt><span class="section"><a href="#upgrade0.98">3.2. Upgrading from 0.96.x to 0.98.x</a></span></dt><dt><span class="section"><a href="#upgrade0.96">3.3. Upgrading from 0.94.x to 0.96.x</a></span></dt><dt><span class="section"><a href="#upgrade0.94">3.4. Upgrading from 0.92.x to 0.94.x</a></span></dt><dt><span class="section"><a href="#upgrade0.92">3.5. Upgrading from 0.90.x to 0.92.x</a></span></dt><dt><span class="section"><a href="#upgrade0.90">3.6. Upgrading to HBase 0.90.x from 0.20.x or 0.89.x</a></span></dt></dl></dd><dt><span class="chapter"><a href="#shell">4. The Apache HBase Shell</a></span></dt><dd><dl><dt><span class="section"><a href="#scripting">4.1. Scripting</a></span></dt><dt><span class="section"><a href="#shell_tricks">4.2. Shell Tricks</a></span></dt></dl></dd><dt><span class="chapter"><a href="#datamodel">5. Data Model</a></span></dt><dd><dl><dt><span class="section"><a href="#conceptual.view">5.1. Conceptual View</a></span></dt><dt><span class="section"><a href="#physical.view">5.2. Physical View</a></span></dt><dt><span class="section"><a href="#namespace">5.3. Namespace</a></span></dt><dt><span class="section"><a href="#table">5.4. Table</a></span></dt><dt><span class="section"><a href="#row">5.5. Row</a></span></dt><dt><span class="section"><a href="#columnfamily">5.6. Column Family</a></span></dt><dt><span class="section"><a href="#cells">5.7. Cells</a></span></dt><dt><span class="section"><a href="#data_model_operations">5.8. Data Model Operations</a></span></dt><dt><span class="section"><a href="#versions">5.9. Versions</a></span></dt><dt><span class="section"><a href="#dm.sort">5.10. Sort Order</a></span></dt><dt><span class="section"><a href="#dm.column.metadata">5.11. Column Metadata</a></span></dt><dt><span class="section"><a href="#joins">5.12. Joins</a></span></dt><dt><span class="section"><a href="#acid">5.13. ACID</a></span></dt></dl></dd><dt><span class="chapter"><a href="#schema">6. HBase and Schema Design</a></span></dt><dd><dl><dt><span class="section"><a href="#schema.creation">6.1. 
      Schema Creation
  </a></span></dt><dt><span class="section"><a href="#number.of.cfs">6.2. 
      On the number of column families
  </a></span></dt><dt><span class="section"><a href="#rowkey.design">6.3. Rowkey Design</a></span></dt><dt><span class="section"><a href="#schema.versions">6.4. 
  Number of Versions
  </a></span></dt><dt><span class="section"><a href="#supported.datatypes">6.5. 
  Supported Datatypes
  </a></span></dt><dt><span class="section"><a href="#schema.joins">6.6. Joins</a></span></dt><dt><span class="section"><a href="#ttl">6.7. Time To Live (TTL)</a></span></dt><dt><span class="section"><a href="#cf.keep.deleted">6.8. 
  Keeping Deleted Cells
  </a></span></dt><dt><span class="section"><a href="#secondary.indexes">6.9. 
  Secondary Indexes and Alternate Query Paths
  </a></span></dt><dt><span class="section"><a href="#constraints">6.10. Constraints</a></span></dt><dt><span class="section"><a href="#schema.casestudies">6.11. Schema Design Case Studies</a></span></dt><dt><span class="section"><a href="#schema.ops">6.12. Operational and Performance Configuration Options</a></span></dt></dl></dd><dt><span class="chapter"><a href="#mapreduce">7. HBase and MapReduce</a></span></dt><dd><dl><dt><span class="section"><a href="#splitter">7.1. Map-Task Splitting</a></span></dt><dt><span class="section"><a href="#mapreduce.example">7.2. HBase MapReduce Examples</a></span></dt><dt><span class="section"><a href="#mapreduce.htable.access">7.3. Accessing Other HBase Tables in a MapReduce Job</a></span></dt><dt><span class="section"><a href="#mapreduce.specex">7.4. Speculative Execution</a></span></dt></dl></dd><dt><span class="chapter"><a href="#security">8. Secure Apache HBase</a></span></dt><dd><dl><dt><span class="section"><a href="#hbase.secure.configuration">8.1. Secure Client Access to Apache HBase</a></span></dt><dt><span class="section"><a href="#hbase.secure.simpleconfiguration">8.2. Simple User Access to Apache HBase</a></span></dt><dt><span class="section"><a href="#hbase.tags">8.3. Tags</a></span></dt><dt><span class="section"><a href="#hbase.accesscontrol.configuration">8.4. Access Control</a></span></dt><dt><span class="section"><a href="#hbase.secure.bulkload">8.5. Secure Bulk Load</a></span></dt><dt><span class="section"><a href="#hbase.visibility.labels">8.6. Visibility Labels</a></span></dt><dt><span class="section"><a href="#hbase.encryption.server">8.7. Transparent Server Side Encryption</a></span></dt></dl></dd><dt><span class="chapter"><a href="#architecture">9. Architecture</a></span></dt><dd><dl><dt><span class="section"><a href="#arch.overview">9.1. Overview</a></span></dt><dt><span class="section"><a href="#arch.catalog">9.2. Catalog Tables</a></span></dt><dt><span class="section"><a href="#client">9.3. Client</a></span></dt><dt><span class="section"><a href="#client.filter">9.4. Client Request Filters</a></span></dt><dt><span class="section"><a href="#master">9.5. Master</a></span></dt><dt><span class="section"><a href="#regionserver.arch">9.6. RegionServer</a></span></dt><dt><span class="section"><a href="#regions.arch">9.7. Regions</a></span></dt><dt><span class="section"><a href="#arch.bulk.load">9.8. Bulk Loading</a></span></dt><dt><span class="section"><a href="#arch.hdfs">9.9. HDFS</a></span></dt></dl></dd><dt><span class="chapter"><a href="#external_apis">10. Apache HBase External APIs</a></span></dt><dd><dl><dt><span class="section"><a href="#nonjava.jvm">10.1. Non-Java Languages Talking to the JVM</a></span></dt><dt><span class="section"><a href="#rest">10.2. REST</a></span></dt><dt><span class="section"><a href="#thrift">10.3. Thrift</a></span></dt><dt><span class="section"><a href="#c">10.4. C/C++ Apache HBase Client</a></span></dt></dl></dd><dt><span class="chapter"><a href="#cp">11. Apache HBase Coprocessors</a></span></dt><dt><span class="chapter"><a href="#performance">12. Apache HBase Performance Tuning</a></span></dt><dd><dl><dt><span class="section"><a href="#perf.os">12.1. Operating System</a></span></dt><dt><span class="section"><a href="#perf.network">12.2. Network</a></span></dt><dt><span class="section"><a href="#jvm">12.3. Java</a></span></dt><dt><span class="section"><a href="#perf.configurations">12.4. HBase Configurations</a></span></dt><dt><span class="section"><a href="#perf.zookeeper">12.5. ZooKeeper</a></span></dt><dt><span class="section"><a href="#perf.schema">12.6. Schema Design</a></span></dt><dt><span class="section"><a href="#perf.general">12.7. HBase General Patterns</a></span></dt><dt><span class="section"><a href="#perf.writing">12.8. Writing to HBase</a></span></dt><dt><span class="section"><a href="#perf.reading">12.9. Reading from HBase</a></span></dt><dt><span class="section"><a href="#perf.deleting">12.10. Deleting from HBase</a></span></dt><dt><span class="section"><a href="#perf.hdfs">12.11. HDFS</a></span></dt><dt><span class="section"><a href="#perf.ec2">12.12. Amazon EC2</a></span></dt><dt><span class="section"><a href="#perf.hbase.mr.cluster">12.13. Collocating HBase and MapReduce</a></span></dt><dt><span class="section"><a href="#perf.casestudy">12.14. Case Studies</a></span></dt></dl></dd><dt><span class="chapter"><a href="#trouble">13. Troubleshooting and Debugging Apache HBase</a></span></dt><dd><dl><dt><span class="section"><a href="#trouble.general">13.1. General Guidelines</a></span></dt><dt><span class="section"><a href="#trouble.log">13.2. Logs</a></span></dt><dt><span class="section"><a href="#trouble.resources">13.3. Resources</a></span></dt><dt><span class="section"><a href="#trouble.tools">13.4. Tools</a></span></dt><dt><span class="section"><a href="#trouble.client">13.5. Client</a></span></dt><dt><span class="section"><a href="#trouble.mapreduce">13.6. MapReduce</a></span></dt><dt><span class="section"><a href="#trouble.namenode">13.7. NameNode</a></span></dt><dt><span class="section"><a href="#trouble.network">13.8. Network</a></span></dt><dt><span class="section"><a href="#trouble.rs">13.9. RegionServer</a></span></dt><dt><span class="section"><a href="#trouble.master">13.10. Master</a></span></dt><dt><span class="section"><a href="#trouble.zookeeper">13.11. ZooKeeper</a></span></dt><dt><span class="section"><a href="#trouble.ec2">13.12. Amazon EC2</a></span></dt><dt><span class="section"><a href="#trouble.versions">13.13. HBase and Hadoop version issues</a></span></dt><dt><span class="section"><a href="#trouble.tests">13.14. Running unit or integration tests</a></span></dt><dt><span class="section"><a href="#trouble.casestudy">13.15. Case Studies</a></span></dt><dt><span class="section"><a href="#trouble.crypto">13.16. Cryptographic Features</a></span></dt></dl></dd><dt><span class="chapter"><a href="#casestudies">14. Apache HBase Case Studies</a></span></dt><dd><dl><dt><span class="section"><a href="#casestudies.overview">14.1. Overview</a></span></dt><dt><span class="section"><a href="#casestudies.schema">14.2. Schema Design</a></span></dt><dt><span class="section"><a href="#casestudies.perftroub">14.3. Performance/Troubleshooting</a></span></dt></dl></dd><dt><span class="chapter"><a href="#ops_mgt">15. Apache HBase Operational Management</a></span></dt><dd><dl><dt><span class="section"><a href="#tools">15.1. HBase Tools and Utilities</a></span></dt><dt><span class="section"><a href="#ops.regionmgt">15.2. Region Management</a></span></dt><dt><span class="section"><a href="#node.management">15.3. Node Management</a></span></dt><dt><span class="section"><a href="#hbase_metrics">15.4. HBase Metrics</a></span></dt><dt><span class="section"><a href="#ops.monitoring">15.5. HBase Monitoring</a></span></dt><dt><span class="section"><a href="#cluster_replication">15.6. Cluster Replication</a></span></dt><dt><span class="section"><a href="#ops.backup">15.7. HBase Backup</a></span></dt><dt><span class="section"><a href="#ops.snapshots">15.8. HBase Snapshots</a></span></dt><dt><span class="section"><a href="#ops.capacity">15.9. Capacity Planning and Region Sizing</a></span></dt><dt><span class="section"><a href="#table.rename">15.10. Table Rename</a></span></dt></dl></dd><dt><span class="chapter"><a href="#developer">16. Building and Developing Apache HBase</a></span></dt><dd><dl><dt><span class="section"><a href="#repos">16.1. Apache HBase Repositories</a></span></dt><dt><span class="section"><a href="#ides">16.2. IDEs</a></span></dt><dt><span class="section"><a href="#build">16.3. Building Apache HBase</a></span></dt><dt><span class="section"><a href="#releasing">16.4. Releasing Apache HBase</a></span></dt><dt><span class="section"><a href="#documentation">16.5. Generating the HBase Reference Guide</a></span></dt><dt><span class="section"><a href="#hbase.org">16.6. Updating hbase.apache.org</a></span></dt><dt><span class="section"><a href="#hbase.tests">16.7. Tests</a></span></dt><dt><span class="section"><a href="#maven.build.commands">16.8. Maven Build Commands</a></span></dt><dt><span class="section"><a href="#getting.involved">16.9. Getting Involved</a></span></dt><dt><span class="section"><a href="#developing">16.10. Developing</a></span></dt><dt><span class="section"><a href="#submitting.patches">16.11. Submitting Patches</a></span></dt></dl></dd><dt><span class="chapter"><a href="#zookeeper">17. ZooKeeper</a></span></dt><dd><dl><dt><span class="section"><a href="#d366e13785">17.1. Using existing ZooKeeper ensemble</a></span></dt><dt><span class="section"><a href="#zk.sasl.auth">17.2. SASL Authentication with ZooKeeper</a></span></dt></dl></dd><dt><span class="chapter"><a href="#cp">18. Apache HBase Coprocessors</a></span></dt><dt><span class="chapter"><a href="#community">19. Community</a></span></dt><dd><dl><dt><span class="section"><a href="#decisions">19.1. Decisions</a></span></dt><dt><span class="section"><a href="#community.roles">19.2. Community Roles</a></span></dt><dt><span class="section"><a href="#hbase.commit.msg.format">19.3. Commit Message format</a></span></dt></dl></dd><dt><span class="appendix"><a href="#faq">A. FAQ</a></span></dt><dt><span class="appendix"><a href="#hbck.in.depth">B. hbck In Depth</a></span></dt><dd><dl><dt><span class="section"><a href="#d366e14473">B.1. Running hbck to identify inconsistencies</a></span></dt><dt><span class="section"><a href="#d366e14489">B.2. Inconsistencies</a></span></dt><dt><span class="section"><a href="#d366e14508">B.3. Localized repairs</a></span></dt><dt><span class="section"><a href="#d366e14546">B.4. Region Overlap Repairs</a></span></dt></dl></dd><dt><span class="appendix"><a href="#compression">C. Compression In HBase</a></span></dt><dd><dl><dt><span class="section"><a href="#compression.test">C.1. CompressionTest Tool</a></span></dt><dt><span class="section"><a href="#hbase.regionserver.codecs">C.2. 
    <code class="varname">
    hbase.regionserver.codecs
    </code>
    </a></span></dt><dt><span class="section"><a href="#lzo.compression">C.3. 
    LZO
    </a></span></dt><dt><span class="section"><a href="#gzip.compression">C.4. 
    GZIP
    </a></span></dt><dt><span class="section"><a href="#snappy.compression">C.5. 
    SNAPPY
    </a></span></dt><dt><span class="section"><a href="#changing.compression">C.6. Changing Compression Schemes</a></span></dt></dl></dd><dt><span class="appendix"><a href="#d366e14765">D. YCSB: The Yahoo! Cloud Serving Benchmark and HBase</a></span></dt><dt><span class="appendix"><a href="#hfilev2">E. HFile format version 2</a></span></dt><dd><dl><dt><span class="section"><a href="#d366e14785">E.1. Motivation </a></span></dt><dt><span class="section"><a href="#d366e14798">E.2. HFile format version 1 overview </a></span></dt><dt><span class="section"><a href="#d366e14844">E.3. 
      HBase file format with inline blocks (version 2)
      </a></span></dt></dl></dd><dt><span class="appendix"><a href="#other.info">F. Other Information About HBase</a></span></dt><dd><dl><dt><span class="section"><a href="#other.info.videos">F.1. HBase Videos</a></span></dt><dt><span class="section"><a href="#other.info.pres">F.2. HBase Presentations (Slides)</a></span></dt><dt><span class="section"><a href="#other.info.papers">F.3. HBase Papers</a></span></dt><dt><span class="section"><a href="#other.info.sites">F.4. HBase Sites</a></span></dt><dt><span class="section"><a href="#other.info.books">F.5. HBase Books</a></span></dt><dt><span class="section"><a href="#other.info.books.hadoop">F.6. Hadoop Books</a></span></dt></dl></dd><dt><span class="appendix"><a href="#hbase.history">G. HBase History</a></span></dt><dt><span class="appendix"><a href="#asf">H. HBase and the Apache Software Foundation</a></span></dt><dd><dl><dt><span class="section"><a href="#asf.devprocess">H.1. ASF Development Process</a></span></dt><dt><span class="section"><a href="#asf.reporting">H.2. ASF Board Reporting</a></span></dt></dl></dd><dt><span class="appendix"><a href="#tracing">I. Enabling Dapper-like Tracing in HBase</a></span></dt><dd><dl><dt><span class="section"><a href="#tracing.spanreceivers">I.1. SpanReceivers</a></span></dt><dt><span class="section"><a href="#tracing.client.modifications">I.2. Client Modifications</a></span></dt><dt><span class="section"><a href="#tracing.client.shell">I.3. Tracing from HBase Shell</a></span></dt></dl></dd><dt><span class="appendix"><a href="#hbase.rpc">J. 0.95 RPC Specification</a></span></dt><dd><dl><dt><span class="section"><a href="#d366e15573">J.1. Goals</a></span></dt><dt><span class="section"><a href="#d366e15586">J.2. TODO</a></span></dt><dt><span class="section"><a href="#d366e15602">J.3. RPC</a></span></dt><dt><span class="section"><a href="#d366e15723">J.4. Notes</a></span></dt></dl></dd><dt><span class="index"><a href="#book_index">Index</a></span></dt></dl></div><div class="list-of-tables"><p><b>List of Tables</b></p><dl><dt>2.1. <a href="#d366e523">Hadoop version support matrix</a></dt><dt>5.1. <a href="#d366e3753">Table <code class="varname">webtable</code></a></dt><dt>5.2. <a href="#d366e3837">ColumnFamily <code class="varname">anchor</code></a></dt><dt>5.3. <a href="#d366e3876">ColumnFamily <code class="varname">contents</code></a></dt><dt>8.1. <a href="#d366e5847">Operation To Permission Mapping</a></dt><dt>9.1. <a href="#d366e7739"></a></dt><dt>15.1. <a href="#d366e10747"></a></dt></dl></div><div class="preface" title="Preface"><div class="titlepage"><div><div><h2 class="title"><a name="preface"></a>Preface</h2></div></div></div><p>This is the official reference guide for the <a class="link" href="http://hbase.apache.org/" target="_top">HBase</a> version it ships with.
  Herein you will find either the definitive documentation on an HBase topic
  as of its standing when the referenced HBase version shipped, or it
  will point to the location in <a class="link" href="http://hbase.apache.org/apidocs/index.html" target="_top">javadoc</a>,
  <a class="link" href="https://issues.apache.org/jira/browse/HBASE" target="_top">JIRA</a>
  or <a class="link" href="http://wiki.apache.org/hadoop/Hbase" target="_top">wiki</a> where
  the pertinent information can be found.</p><p>This reference guide is a work in progress.  The source for this guide can
      be found at <code class="filename">src/main/docbkx</code> in a checkout of the hbase
      project.  This reference guide is marked up using
      <a class="link" href="http://www.docbook.com/" target="_top">DocBook</a> from which the
      the finished guide is generated as part of the 'site' build target.  Run
      </p><pre class="programlisting">mvn site</pre><p> to generate this documentation.
      Amendments and improvements to the documentation are welcomed.  Add a
      patch to an issue up in the HBase <a class="link" href="https://issues.apache.org/jira/browse/HBASE" target="_top">JIRA</a>.</p><div class="note" title="Heads-up if this is your first foray into the world of distributed computing..." style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a name="headsup"></a>Heads-up if this is your first foray into the world of distributed computing...</h3><p>
          If this is your first foray into the wonderful world of
          Distributed Computing, then you are in for
          some interesting times.  First off, distributed systems are
          hard; making a distributed system hum requires a disparate
          skillset that spans systems (hardware and software) and
          networking.  Your cluster' operation can hiccup because of any
          of a myriad set of reasons from bugs in HBase itself through misconfigurations
          -- misconfiguration of HBase but also operating system misconfigurations --
          through to hardware problems whether it be a bug in your network card
          drivers or an underprovisioned RAM bus (to mention two recent
          examples of hardware issues that manifested as "HBase is slow").
          You will also need to do a recalibration if up to this your
          computing has been bound to a single box.  Here is one good
          starting point:
          <a class="link" href="http://en.wikipedia.org/wiki/Fallacies_of_Distributed_Computing" target="_top">Fallacies of Distributed Computing</a>.
          That said, you are welcome.  Its a fun place to be.  Yours, the HBase Community.
      </p></div></div><div class="chapter" title="Chapter&nbsp;1.&nbsp;Getting Started"><div class="titlepage"><div><div><h2 class="title"><a name="getting_started"></a>Chapter&nbsp;1.&nbsp;Getting Started</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#d366e83">1.1. Introduction</a></span></dt><dt><span class="section"><a href="#quickstart">1.2. Quick Start</a></span></dt><dd><dl><dt><span class="section"><a href="#d366e131">1.2.1. Download and unpack the latest stable release.</a></span></dt><dt><span class="section"><a href="#start_hbase">1.2.2. Start HBase</a></span></dt><dt><span class="section"><a href="#shell_exercises">1.2.3. Shell Exercises</a></span></dt><dt><span class="section"><a href="#stopping">1.2.4. Stopping HBase</a></span></dt><dt><span class="section"><a href="#d366e291">1.2.5. Where to go next</a></span></dt></dl></dd></dl></div><div class="section" title="1.1.&nbsp;Introduction"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="d366e83"></a>1.1.&nbsp;Introduction</h2></div></div></div><p><a class="xref" href="#quickstart" title="1.2.&nbsp;Quick Start">Section&nbsp;1.2, &#8220;Quick Start&#8221;</a> will get you up and
    running on a single-node, standalone instance of HBase.
    </p></div><div class="section" title="1.2.&nbsp;Quick Start"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="quickstart"></a>1.2.&nbsp;Quick Start</h2></div></div></div><p>This guide describes setup of a standalone HBase instance. It will
        run against the local filesystem.  In later sections we will take you through
        how to run HBase on Apache Hadoop's HDFS, a distributed filesystem.  This section
        shows you how to create a table in HBase, inserting
    rows into your new HBase table via the HBase <span class="command"><strong>shell</strong></span>, and then cleaning
    up and shutting down your standalone, local filesystem-based  HBase instance. The below exercise
    should take no more than ten minutes (not including download time).
    </p><div class="note" title="Local Filesystem and Durability" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a name="local.fs.durability"></a>Local Filesystem and Durability</h3><p>Using HBase with a LocalFileSystem does not currently guarantee durability.
        The HDFS local filesystem implementation will lose edits if files are not properly
        closed -- which is very likely to happen when experimenting with a new download.
            You need to run HBase on HDFS to ensure all writes are preserved.  Running
            against the local filesystem though will get you off the ground quickly and get you
            familiar with how the general system works so lets run with it for now. See
            <a class="link" href="https://issues.apache.org/jira/browse/HBASE-3696" target="_top">https://issues.apache.org/jira/browse/HBASE-3696</a> and its associated issues for more details.</p></div><div class="note" title="Loopback IP" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a name="loopback.ip.getting.started"></a>Loopback IP</h3><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p><span class="emphasis"><em>The below advice is for hbase-0.94.x and older versions only. We believe this fixed in hbase-0.96.0 and beyond
(let us know if we have it wrong).</em></span>  There should be no need of the below modification to <code class="filename">/etc/hosts</code> in
later versions of HBase.</p></div><p>HBase expects the loopback IP address to be 127.0.0.1.  Ubuntu and some other distributions,
            for example, will default to 127.0.1.1 and this will cause problems for you
            <sup>[<a name="d366e117" href="#ftn.d366e117" class="footnote">1</a>]</sup>.
        </p><p><code class="filename">/etc/hosts</code> should look something like this:
</p><pre class="programlisting">
            127.0.0.1 localhost
            127.0.0.1 ubuntu.ubuntu-domain ubuntu
</pre><p>
        </p></div><div class="section" title="1.2.1.&nbsp;Download and unpack the latest stable release."><div class="titlepage"><div><div><h3 class="title"><a name="d366e131"></a>1.2.1.&nbsp;Download and unpack the latest stable release.</h3></div></div></div><p>Choose a download site from this list of <a class="link" href="http://www.apache.org/dyn/closer.cgi/hbase/" target="_top">Apache Download
      Mirrors</a>. Click on the suggested top link. This will take you to a
      mirror of <span class="emphasis"><em>HBase Releases</em></span>. Click on the folder named
      <code class="filename">stable</code> and then download the file that ends in
      <code class="filename">.tar.gz</code> to your local filesystem; e.g.
      <code class="filename">hbase-0.94.2.tar.gz</code>.</p><p>Decompress and untar your download and then change into the
      unpacked directory.</p><pre class="programlisting">$ tar xfz hbase-0.98.1-hadoop2.tar.gz
$ cd hbase-0.98.1-hadoop2
</pre><p>At this point, you are ready to start HBase. But before starting
      it, edit <code class="filename">conf/hbase-site.xml</code>, the file you write
      your site-specific configurations into. Set
      <code class="varname">hbase.rootdir</code>, the directory HBase writes data to,
      and <code class="varname">hbase.zookeeper.property.dataDir</code>, the directory
      ZooKeeper writes its data too:
</p><pre class="programlisting">&lt;?xml version="1.0"?&gt;
&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;
&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.rootdir&lt;/name&gt;
    &lt;value&gt;file:///DIRECTORY/hbase&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;
    &lt;value&gt;/DIRECTORY/zookeeper&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;</pre><p> Replace <code class="varname">DIRECTORY</code> in the above with the
      path to the directory you would have HBase and ZooKeeper write their data. By default,
      <code class="varname">hbase.rootdir</code> is set to <code class="filename">/tmp/hbase-${user.name}</code>
      and similarly so for the default ZooKeeper data location which means you'll lose all
      your data whenever your server reboots unless you change it (Most operating systems clear
      <code class="filename">/tmp</code> on restart).</p></div><div class="section" title="1.2.2.&nbsp;Start HBase"><div class="titlepage"><div><div><h3 class="title"><a name="start_hbase"></a>1.2.2.&nbsp;Start HBase</h3></div></div></div><p>Now start HBase:</p><pre class="programlisting">$ ./bin/start-hbase.sh
starting Master, logging to logs/hbase-user-master-example.org.out</pre><p>You should now have a running standalone HBase instance. In
      standalone mode, HBase runs all daemons in the the one JVM; i.e. both
      the HBase and ZooKeeper daemons. HBase logs can be found in the
      <code class="filename">logs</code> subdirectory. Check them out especially if
      it seems HBase had trouble starting.</p><div class="note" title="Is java installed?" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Is <span class="application">java</span> installed?</h3><p>All of the above presumes a 1.6 version of Oracle
        <span class="application">java</span> is installed on your machine and
        available on your path (See <a class="xref" href="#java" title="2.1.1.&nbsp;Java">Section&nbsp;2.1.1, &#8220;Java&#8221;</a>); i.e. when you type
        <span class="application">java</span>, you see output that describes the
        options the java program takes (HBase requires java 6). If this is not
        the case, HBase will not start. Install java, edit
        <code class="filename">conf/hbase-env.sh</code>, uncommenting the
        <code class="envar">JAVA_HOME</code> line pointing it to your java install, then,
        retry the steps above.</p></div></div><div class="section" title="1.2.3.&nbsp;Shell Exercises"><div class="titlepage"><div><div><h3 class="title"><a name="shell_exercises"></a>1.2.3.&nbsp;Shell Exercises</h3></div></div></div><p>Connect to your running HBase via the <span class="command"><strong>shell</strong></span>.</p><pre class="programlisting">$ ./bin/hbase shell
HBase Shell; enter 'help&lt;RETURN&gt;' for list of supported commands.
Type "exit&lt;RETURN&gt;" to leave the HBase Shell
Version: 0.90.0, r1001068, Fri Sep 24 13:55:42 PDT 2010

hbase(main):001:0&gt; </pre><p>Type <span class="command"><strong>help</strong></span> and then
      <span class="command"><strong>&lt;RETURN&gt;</strong></span> to see a listing of shell commands and
      options. Browse at least the paragraphs at the end of the help emission
      for the gist of how variables and command arguments are entered into the
      HBase shell; in particular note how table names, rows, and columns,
      etc., must be quoted.</p><p>Create a table named <code class="varname">test</code> with a single column family named <code class="varname">cf</code>.
      Verify its creation by listing all tables and then insert some
      values.</p><pre class="programlisting">hbase(main):003:0&gt; create 'test', 'cf'
0 row(s) in 1.2200 seconds
hbase(main):003:0&gt; list 'test'
..
1 row(s) in 0.0550 seconds
hbase(main):004:0&gt; put 'test', 'row1', 'cf:a', 'value1'
0 row(s) in 0.0560 seconds
hbase(main):005:0&gt; put 'test', 'row2', 'cf:b', 'value2'
0 row(s) in 0.0370 seconds
hbase(main):006:0&gt; put 'test', 'row3', 'cf:c', 'value3'
0 row(s) in 0.0450 seconds</pre><p>Above we inserted 3 values, one at a time. The first insert is at
      <code class="varname">row1</code>, column <code class="varname">cf:a</code> with a value of
      <code class="varname">value1</code>. Columns in HBase are comprised of a column family prefix --
      <code class="varname">cf</code> in this example -- followed by a colon and then a
      column qualifier suffix (<code class="varname">a</code> in this case).</p><p>Verify the data insert by running a scan of the table as follows</p><pre class="programlisting">hbase(main):007:0&gt; scan 'test'
ROW        COLUMN+CELL
row1       column=cf:a, timestamp=1288380727188, value=value1
row2       column=cf:b, timestamp=1288380738440, value=value2
row3       column=cf:c, timestamp=1288380747365, value=value3
3 row(s) in 0.0590 seconds</pre><p>Get a single row</p><pre class="programlisting">hbase(main):008:0&gt; get 'test', 'row1'
COLUMN      CELL
cf:a        timestamp=1288380727188, value=value1
1 row(s) in 0.0400 seconds</pre><p>Now, disable and drop your table. This will clean up all done
      above.</p><pre class="programlisting">hbase(main):012:0&gt; disable 'test'
0 row(s) in 1.0930 seconds
hbase(main):013:0&gt; drop 'test'
0 row(s) in 0.0770 seconds </pre><p>Exit the shell by typing exit.</p><pre class="programlisting">hbase(main):014:0&gt; exit</pre></div><div class="section" title="1.2.4.&nbsp;Stopping HBase"><div class="titlepage"><div><div><h3 class="title"><a name="stopping"></a>1.2.4.&nbsp;Stopping HBase</h3></div></div></div><p>Stop your hbase instance by running the stop script.</p><pre class="programlisting">$ ./bin/stop-hbase.sh
stopping hbase...............</pre></div><div class="section" title="1.2.5.&nbsp;Where to go next"><div class="titlepage"><div><div><h3 class="title"><a name="d366e291"></a>1.2.5.&nbsp;Where to go next</h3></div></div></div><p>The above described standalone setup is good for testing and
          experiments only. In the next chapter, <a class="xref" href="#configuration" title="Chapter&nbsp;2.&nbsp;Apache HBase Configuration">Chapter&nbsp;2, <i>Apache HBase Configuration</i></a>,
      we'll go into depth on the different HBase run modes, system requirements
      running HBase, and critical configurations setting up a distributed HBase deploy.</p></div></div><div class="footnotes"><br><hr width="100" align="left"><div class="footnote"><p><sup>[<a id="ftn.d366e117" href="#d366e117" class="para">1</a>] </sup>See <a class="link" href="http://blog.devving.com/why-does-hbase-care-about-etchosts/" target="_top">Why does HBase care about /etc/hosts?</a> for detail.</p></div></div></div><div class="chapter" title="Chapter&nbsp;2.&nbsp;Apache HBase Configuration"><div class="titlepage"><div><div><h2 class="title"><a name="configuration"></a>Chapter&nbsp;2.&nbsp;Apache HBase Configuration</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#basic.prerequisites">2.1. Basic Prerequisites</a></span></dt><dd><dl><dt><span class="section"><a href="#java">2.1.1. Java</a></span></dt><dt><span class="section"><a href="#os">2.1.2. Operating System</a></span></dt><dt><span class="section"><a href="#hadoop">2.1.3. Hadoop</a></span></dt></dl></dd><dt><span class="section"><a href="#standalone_dist">2.2. HBase run modes: Standalone and Distributed</a></span></dt><dd><dl><dt><span class="section"><a href="#standalone">2.2.1. Standalone HBase</a></span></dt><dt><span class="section"><a href="#distributed">2.2.2. Distributed</a></span></dt><dt><span class="section"><a href="#confirm">2.2.3. Running and Confirming Your Installation</a></span></dt></dl></dd><dt><span class="section"><a href="#config.files">2.3. Configuration Files</a></span></dt><dd><dl><dt><span class="section"><a href="#hbase.site">2.3.1. <code class="filename">hbase-site.xml</code> and <code class="filename">hbase-default.xml</code></a></span></dt><dt><span class="section"><a href="#hbase.env.sh">2.3.2. <code class="filename">hbase-env.sh</code></a></span></dt><dt><span class="section"><a href="#log4j">2.3.3. <code class="filename">log4j.properties</code></a></span></dt><dt><span class="section"><a href="#client_dependencies">2.3.4. Client configuration and dependencies connecting to an HBase cluster</a></span></dt></dl></dd><dt><span class="section"><a href="#example_config">2.4. Example Configurations</a></span></dt><dd><dl><dt><span class="section"><a href="#d366e2846">2.4.1. Basic Distributed HBase Install</a></span></dt></dl></dd><dt><span class="section"><a href="#important_configurations">2.5. The Important Configurations</a></span></dt><dd><dl><dt><span class="section"><a href="#required_configuration">2.5.1. Required Configurations</a></span></dt><dt><span class="section"><a href="#recommended_configurations">2.5.2. Recommended Configurations</a></span></dt><dt><span class="section"><a href="#other_configuration">2.5.3. Other Configurations</a></span></dt></dl></dd></dl></div><p>This chapter is the Not-So-Quick start guide to Apache HBase configuration.  It goes
    over system requirements, Hadoop setup, the different Apache HBase run modes, and the
    various configurations in HBase.  Please read this chapter carefully.  At a mimimum
    ensure that all <a class="xref" href="#basic.prerequisites" title="2.1.&nbsp;Basic Prerequisites">Section&nbsp;2.1, &#8220;Basic Prerequisites&#8221;</a> have
      been satisfied.  Failure to do so will cause you (and us) grief debugging strange errors
      and/or data loss.</p><p>
        Apache HBase uses the same configuration system as Apache Hadoop.
        To configure a deploy, edit a file of environment variables
        in <code class="filename">conf/hbase-env.sh</code> -- this configuration
        is used mostly by the launcher shell scripts getting the cluster
        off the ground -- and then add configuration to an XML file to
        do things like override HBase defaults, tell HBase what Filesystem to
        use, and the location of the ZooKeeper ensemble
        <sup>[<a name="d366e311" href="#ftn.d366e311" class="footnote">2</a>]</sup>
        .
    </p><p>When running in distributed mode, after you make
    an edit to an HBase configuration, make sure you copy the
    content of the <code class="filename">conf</code> directory to
    all nodes of the cluster.  HBase will not do this for you.
    Use <span class="command"><strong>rsync</strong></span>.  For most configuration, a restart is
needed for servers to pick up changes (caveat dynamic config. to be described later below).</p><div class="section" title="2.1.&nbsp;Basic Prerequisites"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="basic.prerequisites"></a>2.1.&nbsp;Basic Prerequisites</h2></div></div></div><p>This section lists required services and some required system configuration.
    </p><div class="section" title="2.1.1.&nbsp;Java"><div class="titlepage"><div><div><h3 class="title"><a name="java"></a>2.1.1.&nbsp;Java</h3></div></div></div><p>Just like Hadoop, HBase requires at least Java 6 from
        <a class="link" href="http://www.java.com/download/" target="_top">Oracle</a>.
        </p></div><div class="section" title="2.1.2.&nbsp;Operating System"><div class="titlepage"><div><div><h3 class="title"><a name="os"></a>2.1.2.&nbsp;Operating System</h3></div></div></div><div class="section" title="2.1.2.1.&nbsp;ssh"><div class="titlepage"><div><div><h4 class="title"><a name="ssh"></a>2.1.2.1.&nbsp;ssh</h4></div></div></div><p><span class="command"><strong>ssh</strong></span> must be installed and
        <span class="command"><strong>sshd</strong></span> must be running to use Hadoop's scripts to
        manage remote Hadoop and HBase daemons. You must be able to ssh to all
        nodes, including your local node, using passwordless login (Google
        "ssh passwordless login").  If on mac osx, see the section,
        <a class="link" href="http://wiki.apache.org/hadoop/Running_Hadoop_On_OS_X_10.5_64-bit_%28Single-Node_Cluster%29" target="_top">SSH: Setting up Remote Desktop and Enabling Self-Login</a>
        on the hadoop wiki.</p></div><div class="section" title="2.1.2.2.&nbsp;DNS"><div class="titlepage"><div><div><h4 class="title"><a name="dns"></a>2.1.2.2.&nbsp;DNS</h4></div></div></div><p>HBase uses the local hostname to self-report its IP address.
        Both forward and reverse DNS resolving must work in versions of
        HBase previous to 0.92.0
        <sup>[<a name="d366e360" href="#ftn.d366e360" class="footnote">3</a>]</sup>.</p><p>If your machine has multiple interfaces, HBase will use the
        interface that the primary hostname resolves to.</p><p>If this is insufficient, you can set
        <code class="varname">hbase.regionserver.dns.interface</code> to indicate the
        primary interface. This only works if your cluster configuration is
        consistent and every host has the same network interface
        configuration.</p><p>Another alternative is setting
        <code class="varname">hbase.regionserver.dns.nameserver</code> to choose a
        different nameserver than the system wide default.</p></div><div class="section" title="2.1.2.3.&nbsp;Loopback IP"><div class="titlepage"><div><div><h4 class="title"><a name="loopback.ip"></a>2.1.2.3.&nbsp;Loopback IP</h4></div></div></div><p>Previous to hbase-0.96.0, HBase expects the loopback IP address to be 127.0.0.1.  See <a class="xref" href="#loopback.ip" title="2.1.2.3.&nbsp;Loopback IP">Section&nbsp;2.1.2.3, &#8220;Loopback IP&#8221;</a></p></div><div class="section" title="2.1.2.4.&nbsp;NTP"><div class="titlepage"><div><div><h4 class="title"><a name="ntp"></a>2.1.2.4.&nbsp;NTP</h4></div></div></div><p>The clocks on cluster members should be in basic alignments.
        Some skew is tolerable but wild skew could generate odd behaviors. Run
        <a class="link" href="http://en.wikipedia.org/wiki/Network_Time_Protocol" target="_top">NTP</a>
        on your cluster, or an equivalent.</p><p>If you are having problems querying data, or "weird" cluster
        operations, check system time!</p></div><div class="section" title="2.1.2.5.&nbsp; ulimit and nproc"><div class="titlepage"><div><div><h4 class="title"><a name="ulimit"></a>2.1.2.5.&nbsp;
          <code class="varname">ulimit</code><a class="indexterm" name="d366e400"></a>
            and
          <code class="varname">nproc</code><a class="indexterm" name="d366e406"></a>
        </h4></div></div></div><p>Apache HBase is a database.  It uses a lot of files all at the same time.
        The default ulimit -n -- i.e. user file limit -- of 1024 on most *nix systems
        is insufficient (On mac os x its 256). Any significant amount of loading will
        lead you to <a class="xref" href="#trouble.rs.runtime.filehandles" title="13.9.2.2.&nbsp;java.io.IOException...(Too many open files)">Section&nbsp;13.9.2.2, &#8220;java.io.IOException...(Too many open files)&#8221;</a>.
        You may also notice errors such as... </p><pre class="programlisting">
      2010-04-06 03:04:37,542 INFO org.apache.hadoop.hdfs.DFSClient: Exception increateBlockOutputStream java.io.EOFException
      2010-04-06 03:04:37,542 INFO org.apache.hadoop.hdfs.DFSClient: Abandoning block blk_-6935524980745310745_1391901
      </pre><p> Do yourself a favor and change the upper bound on the
        number of file descriptors. Set it to north of 10k.  The math runs roughly as follows:  per ColumnFamily
        there is at least one StoreFile and possibly up to 5 or 6 if the region is under load.  Multiply the
        average number of StoreFiles per ColumnFamily times the number of regions per RegionServer.  For example, assuming
        that a schema had 3 ColumnFamilies per region with an average of 3 StoreFiles per ColumnFamily,
        and there are 100 regions per RegionServer, the JVM will open 3 * 3 * 100 = 900 file descriptors
        (not counting open jar files, config files, etc.)
        </p><p>You should also up the hbase users'
        <code class="varname">nproc</code> setting; under load, a low-nproc
        setting could manifest as <code class="classname">OutOfMemoryError</code>
        <sup>[<a name="d366e425" href="#ftn.d366e425" class="footnote">4</a>]</sup>
        <sup>[<a name="d366e432" href="#ftn.d366e432" class="footnote">5</a>]</sup>.
       </p><p>To be clear, upping the file descriptors and nproc for the user who is
        running the HBase process is an operating system configuration, not an
        HBase configuration. Also, a common mistake is that administrators
        will up the file descriptors for a particular user but for whatever
        reason, HBase will be running as some one else. HBase prints in its
        logs as the first line the ulimit its seeing. Ensure its correct.
        <sup>[<a name="d366e444" href="#ftn.d366e444" class="footnote">6</a>]</sup></p><div class="section" title="2.1.2.5.1.&nbsp;ulimit on Ubuntu"><div class="titlepage"><div><div><h5 class="title"><a name="ulimit_ubuntu"></a>2.1.2.5.1.&nbsp;<code class="varname">ulimit</code> on Ubuntu</h5></div></div></div><p>If you are on Ubuntu you will need to make the following
          changes:</p><p>In the file <code class="filename">/etc/security/limits.conf</code> add
          a line like: </p><pre class="programlisting">hadoop  -       nofile  32768</pre><p>
          Replace <code class="varname">hadoop</code> with whatever user is running
          Hadoop and HBase. If you have separate users, you will need 2
          entries, one for each user.  In the same file set nproc hard and soft
          limits.  For example: </p><pre class="programlisting">hadoop soft/hard nproc 32000</pre><p>.</p><p>In the file <code class="filename">/etc/pam.d/common-session</code> add
          as the last line in the file: </p><pre class="programlisting">session required  pam_limits.so</pre><p>
          Otherwise the changes in <code class="filename">/etc/security/limits.conf</code> won't be
          applied.</p><p>Don't forget to log out and back in again for the changes to
          take effect!</p></div></div><div class="section" title="2.1.2.6.&nbsp;Windows"><div class="titlepage"><div><div><h4 class="title"><a name="windows"></a>2.1.2.6.&nbsp;Windows</h4></div></div></div><p>Previous to hbase-0.96.0, Apache HBase was little tested running on Windows. Running a
        production install of HBase on top of Windows is not recommended.</p><p>If you are running HBase on Windows pre-hbase-0.96.0, you must install <a class="link" href="http://cygwin.com/" target="_top">Cygwin</a> to have a *nix-like
        environment for the shell scripts. The full details are explained in
        the <a class="link" href="http://hbase.apache.org/cygwin.html" target="_top">Windows
        Installation</a> guide. Also
        <a class="link" href="http://search-hadoop.com/?q=hbase+windows&amp;fc_project=HBase&amp;fc_type=mail+_hash_+dev" target="_top">search our user mailing list</a> to pick
        up latest fixes figured by Windows users.</p><p>Post-hbase-0.96.0, hbase runs natively on windows with supporting <span class="command"><strong>*.cmd</strong></span> scripts bundled.
    </p></div></div><div class="section" title="2.1.3.&nbsp;Hadoop"><div class="titlepage"><div><div><h3 class="title"><a name="hadoop"></a>2.1.3.&nbsp;<a class="link" href="http://hadoop.apache.org" target="_top">Hadoop</a><a class="indexterm" name="d366e509"></a></h3></div></div></div><p>The below table shows some information about what versions of Hadoop are supported by various HBase versions.
              Based on the version of HBase, you should select the most appropriate version of Hadoop.
              We are not in the Hadoop distro selection business.
              You can use Hadoop distributions from Apache, or learn about vendor distributions of
              Hadoop at <a class="link" href="http://wiki.apache.org/hadoop/Distributions%20and%20Commercial%20Support" target="_top">http://wiki.apache.org/hadoop/Distributions%20and%20Commercial%20Support</a></p><p>
             </p><div class="tip" title="Hadoop 2.x is better than Hadoop 1.x" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Hadoop 2.x is better than Hadoop 1.x</h3><p>Hadoop 2.x is faster, with more features such as short-circuit reads which will help improve your
                     HBase random read profile as well important bug fixes that will improve your overall HBase experience.
                     You should run Hadoop 2 rather than Hadoop 1. HBase 0.98 deprecates use of Hadoop1.
                     HBase 1.0 will not support Hadoop1.
             </p></div><p>
	     </p><div class="table"><a name="d366e523"></a><p class="title"><b>Table&nbsp;2.1.&nbsp;Hadoop version support matrix</b></p><div class="table-contents"><table summary="Hadoop version support matrix" border="1"><colgroup><col align="left" class="c1"><col align="center" class="c2"><col align="center" class="c3"><col align="center" class="c4"><col align="center" class="c5"><col align="center" class="c6"></colgroup><thead><tr><th align="left">               </th><th align="center">HBase-0.92.x</th><th align="center">HBase-0.94.x</th><th align="center">HBase-0.96.x</th><th align="center">HBase-0.98.x<sup>[<a name="d366e545" href="#ftn.d366e545" class="footnote">a</a>]</sup></th><th align="center">HBase-1.0.x<sup>[<a name="d366e550" href="#ftn.d366e550" class="footnote">b</a>]</sup></th></tr></thead><tbody><tr><td align="left">Hadoop-0.20.205</td><td align="center">S</td><td align="center">X</td><td align="center">X</td><td align="center">X</td><td align="center">X</td></tr><tr><td align="left">Hadoop-0.22.x  </td><td align="center">S</td><td align="center">X</td><td align="center">X</td><td align="center">X</td><td align="center">X</td></tr><tr><td align="left">Hadoop-1.0.0-1.0.2<sup>[<a name="d366e583" href="#ftn.d366e583" class="footnote">c</a>]</sup>   </td><td align="center">X</td><td align="center">X</td><td align="center">X</td><td align="center">X</td><td align="center">X</td></tr><tr><td align="left">Hadoop-1.0.3+</td><td align="center">S</td><td align="center">S</td><td align="center">S</td><td align="center">X</td><td align="center">X</td></tr><tr><td align="left">Hadoop-1.1.x   </td><td align="center">NT</td><td align="center">S</td><td align="center">S</td><td align="center">X</td><td align="center">X</td></tr><tr><td align="left">Hadoop-0.23.x  </td><td align="center">X</td><td align="center">S</td><td align="center">NT</td><td align="center">X</td><td align="center">X</td></tr><tr><td align="left">Hadoop-2.0.x-alpha     </td><td align="center">X</td><td align="center">NT</td><td align="center">X</td><td align="center">X</td><td align="center">X</td></tr><tr><td align="left">Hadoop-2.1.0-beta     </td><td align="center">X</td><td align="center">NT</td><td align="center">S</td><td align="center">X</td><td align="center">X</td></tr><tr><td align="left">Hadoop-2.2.0     </td><td align="center">X</td><td align="center">NT<sup>[<a name="d366e669" href="#ftn.d366e669" class="footnote">d</a>]</sup></td><td align="center">S</td><td align="center">S</td><td align="center">NT</td></tr><tr><td align="left">Hadoop-2.3.x</td><td align="center">X</td><td align="center">NT</td><td align="center">S</td><td align="center">S</td><td align="center">NT</td></tr><tr><td align="left">Hadoop-2.4.x</td><td align="center">X</td><td align="center">NT</td><td align="center">S</td><td align="center">S</td><td align="center">S</td></tr><tr><td align="left">Hadoop-2.5.x</td><td align="center">X</td><td align="center">NT</td><td align="center">S</td><td align="center">S</td><td align="center">S</td></tr></tbody><tbody class="footnotes"><tr><td colspan="6"><div class="footnote"><p><sup>[<a id="ftn.d366e545" href="#d366e545" class="para">a</a>] </sup>Support for Hadoop 1.x is deprecated.</p></div><div class="footnote"><p><sup>[<a id="ftn.d366e550" href="#d366e550" class="para">b</a>] </sup>Hadoop 1.x is NOT supported</p></div><div class="footnote"><p><sup>[<a id="ftn.d366e583" href="#d366e583" class="para">c</a>] </sup>HBase requires hadoop 1.0.3 at a minimum; there is an issue where we cannot find KerberosUtil compiling against earlier versions of Hadoop.</p></div><div class="footnote"><p><sup>[<a id="ftn.d366e669" href="#d366e669" class="para">d</a>] </sup>To get 0.94.x to run on hadoop 2.2.0,
                         you need to change the hadoop 2 and protobuf versions in the <code class="filename">pom.xml</code> and then
                         build against the hadoop 2 profile by running something like the following command:
                         </p><pre class="programlisting">$  mvn clean install assembly:single -Dhadoop.profile=2.0 -DskipTests</pre><p>
                         Here is a diff with pom.xml changes:
                         </p><pre class="programlisting">$ svn diff pom.xml
Index: pom.xml
===================================================================
--- pom.xml     (revision 1545157)
+++ pom.xml     (working copy)
@@ -1034,7 +1034,7 @@
     &lt;slf4j.version&gt;1.4.3&lt;/slf4j.version&gt;
     &lt;log4j.version&gt;1.2.16&lt;/log4j.version&gt;
     &lt;mockito-all.version&gt;1.8.5&lt;/mockito-all.version&gt;
-    &lt;protobuf.version&gt;2.4.0a&lt;/protobuf.version&gt;
+    &lt;protobuf.version&gt;2.5.0&lt;/protobuf.version&gt;
     &lt;stax-api.version&gt;1.0.1&lt;/stax-api.version&gt;
     &lt;thrift.version&gt;0.8.0&lt;/thrift.version&gt;
     &lt;zookeeper.version&gt;3.4.5&lt;/zookeeper.version&gt;
@@ -2241,7 +2241,7 @@
         &lt;/property&gt;
       &lt;/activation&gt;
       &lt;properties&gt;
-        &lt;hadoop.version&gt;2.0.0-alpha&lt;/hadoop.version&gt;
+        &lt;hadoop.version&gt;2.2.0&lt;/hadoop.version&gt;
         &lt;slf4j.version&gt;1.6.1&lt;/slf4j.version&gt;
       &lt;/properties&gt;
       &lt;dependencies&gt;</pre><p>
          </p></div></td></tr></tbody></table></div></div><p><br class="table-break">

        Where
		</p><table border="0" summary="Simple list" class="simplelist"><tr><td>S = supported and tested,</td></tr><tr><td>X = not supported,</td></tr><tr><td>NT = it should run, but not tested enough.</td></tr></table><p>
        </p><div class="note" title="Replace the Hadoop Bundled With HBase!" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a name="replace.hadoop"></a>Replace the Hadoop Bundled With HBase!</h3><p>
	Because HBase depends on Hadoop, it bundles an instance of the Hadoop jar under its <code class="filename">lib</code> directory. The bundled jar is ONLY for use in standalone mode. In distributed mode, it is <span class="emphasis"><em>critical</em></span> that the version of Hadoop that is out on your cluster match what is under HBase. Replace the hadoop jar found in the HBase lib directory with the hadoop jar you are running on your cluster to avoid version mismatch issues. Make sure you replace the jar in HBase everywhere on your cluster. Hadoop version mismatch issues have various manifestations but often all looks like its hung up.
    </p></div><div class="section" title="2.1.3.1.&nbsp;Apache HBase 0.92 and 0.94"><div class="titlepage"><div><div><h4 class="title"><a name="hadoop.hbase-0.94"></a>2.1.3.1.&nbsp;Apache HBase 0.92 and 0.94</h4></div></div></div><p>HBase 0.92 and 0.94 versions can work with Hadoop versions, 0.20.205, 0.22.x, 1.0.x, and 1.1.x. HBase-0.94 can additionally work with Hadoop-0.23.x and 2.x, but you may have to recompile the code using the specific maven profile (see top level pom.xml)</p></div><div class="section" title="2.1.3.2.&nbsp;Apache HBase 0.96"><div class="titlepage"><div><div><h4 class="title"><a name="hadoop.hbase-0.96"></a>2.1.3.2.&nbsp;Apache HBase 0.96</h4></div></div></div><p>
    As of Apache HBase 0.96.x, Apache Hadoop 1.0.x at least is required.  Hadoop 2 is strongly encouraged (faster but also has fixes that help MTTR).
    We will no longer run properly on older Hadoops such as 0.20.205 or branch-0.20-append. Do not move to Apache HBase 0.96.x if you cannot upgrade your Hadoop<sup>[<a name="d366e756" href="#ftn.d366e756" class="footnote">7</a>]</sup>.</p></div><div class="section" title="2.1.3.3.&nbsp;Hadoop versions 0.20.x - 1.x"><div class="titlepage"><div><div><h4 class="title"><a name="hadoop.older.versions"></a>2.1.3.3.&nbsp;Hadoop versions 0.20.x - 1.x</h4></div></div></div><p>
     HBase will lose data unless it is running on an HDFS that has a durable
        <code class="code">sync</code> implementation.  DO NOT use Hadoop 0.20.2, Hadoop 0.20.203.0, and Hadoop 0.20.204.0 which DO NOT have this attribute. Currently only Hadoop versions 0.20.205.x or any release in excess of this version -- this includes hadoop-1.0.0 -- have a working, durable sync
          <sup>[<a name="d366e770" href="#ftn.d366e770" class="footnote">8</a>]</sup>.  Sync has to be explicitly enabled by setting
        <code class="varname">dfs.support.append</code> equal
        to true on both the client side -- in <code class="filename">hbase-site.xml</code>
        -- and on the serverside in <code class="filename">hdfs-site.xml</code> (The sync
        facility HBase needs is a subset of the append code path).
        </p><pre class="programlisting">
  &lt;property&gt;
    &lt;name&gt;dfs.support.append&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;
        </pre><p>
        You will have to restart your cluster after making this edit.  Ignore the chicken-little
        comment you'll find in the <code class="filename">hdfs-default.xml</code> in the
        description for the <code class="varname">dfs.support.append</code> configuration.
     </p></div><div class="section" title="2.1.3.4.&nbsp;Apache HBase on Secure Hadoop"><div class="titlepage"><div><div><h4 class="title"><a name="hadoop.security"></a>2.1.3.4.&nbsp;Apache HBase on Secure Hadoop</h4></div></div></div><p>Apache HBase will run on any Hadoop 0.20.x that incorporates Hadoop
          security features as long as you do as
          suggested above and replace the Hadoop jar that ships with HBase
          with the secure version.  If you want to read more about how to setup
          Secure HBase, see <a class="xref" href="#hbase.secure.configuration" title="8.1.&nbsp;Secure Client Access to Apache HBase">Section&nbsp;8.1, &#8220;Secure Client Access to Apache HBase&#8221;</a>.</p></div><div class="section" title="2.1.3.5.&nbsp;dfs.datanode.max.xcievers"><div class="titlepage"><div><div><h4 class="title"><a name="dfs.datanode.max.xcievers"></a>2.1.3.5.&nbsp;<code class="varname">dfs.datanode.max.xcievers</code><a class="indexterm" name="d366e806"></a></h4></div></div></div><p>An Hadoop HDFS datanode has an upper bound on the number of
        files that it will serve at any one time. The upper bound parameter is
        called <code class="varname">xcievers</code> (yes, this is misspelled). Again,
        before doing any loading, make sure you have configured Hadoop's
        <code class="filename">conf/hdfs-site.xml</code> setting the
        <code class="varname">xceivers</code> value to at least the following:
        </p><pre class="programlisting">
      &lt;property&gt;
        &lt;name&gt;dfs.datanode.max.xcievers&lt;/name&gt;
        &lt;value&gt;4096&lt;/value&gt;
      &lt;/property&gt;
      </pre><p>Be sure to restart your HDFS after making the above
        configuration.</p><p>Not having this configuration in place makes for strange looking
        failures. Eventually you'll see a complain in the datanode logs
        complaining about the xcievers exceeded, but on the run up to this one
        manifestation is complaint about missing blocks. For example:
        <code class="code">10/12/08 20:10:31 INFO hdfs.DFSClient: Could not obtain block
        blk_XXXXXXXXXXXXXXXXXXXXXX_YYYYYYYY from any node:
        java.io.IOException: No live nodes contain current block. Will get new
        block locations from namenode and retry...</code>
        <sup>[<a name="d366e829" href="#ftn.d366e829" class="footnote">9</a>]</sup></p><p>See also <a class="xref" href="#casestudies.xceivers" title="14.3.4.&nbsp;Case Study #4 (xcievers Config)">Section&nbsp;14.3.4, &#8220;Case Study #4 (xcievers Config)&#8221;</a>
       </p></div></div></div><div class="section" title="2.2.&nbsp;HBase run modes: Standalone and Distributed"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="standalone_dist"></a>2.2.&nbsp;HBase run modes: Standalone and Distributed</h2></div></div></div><p>HBase has two run modes: <a class="xref" href="#standalone" title="2.2.1.&nbsp;Standalone HBase">Section&nbsp;2.2.1, &#8220;Standalone HBase&#8221;</a> and <a class="xref" href="#distributed" title="2.2.2.&nbsp;Distributed">Section&nbsp;2.2.2, &#8220;Distributed&#8221;</a>. Out of the box, HBase runs in
          standalone mode.  Whatever your mode, you will need to configure HBase by editing files in the HBase <code class="filename">conf</code>
      directory.  At a minimum, you must edit <code class="code">conf/hbase-env.sh</code> to tell HBase which
      <span class="command"><strong>java</strong></span> to use. In this file you set HBase environment
      variables such as the heapsize and other options for the
      <span class="application">JVM</span>, the preferred location for log files,
      etc. Set <code class="varname">JAVA_HOME</code> to point at the root of your
      <span class="command"><strong>java</strong></span> install.</p><div class="section" title="2.2.1.&nbsp;Standalone HBase"><div class="titlepage"><div><div><h3 class="title"><a name="standalone"></a>2.2.1.&nbsp;Standalone HBase</h3></div></div></div><p>This is the default mode. Standalone mode is what is described
            in the <a class="xref" href="#quickstart" title="1.2.&nbsp;Quick Start">Section&nbsp;1.2, &#8220;Quick Start&#8221;</a> section. In
        standalone mode, HBase does not use HDFS -- it uses the local
        filesystem instead -- and it runs all HBase daemons and a local
        ZooKeeper all up in the same JVM. Zookeeper binds to a well known port
        so clients may talk to HBase.</p></div><div class="section" title="2.2.2.&nbsp;Distributed"><div class="titlepage"><div><div><h3 class="title"><a name="distributed"></a>2.2.2.&nbsp;Distributed</h3></div></div></div><p>Distributed mode can be subdivided into distributed but all
        daemons run on a single node -- a.k.a
        <span class="emphasis"><em>pseudo-distributed</em></span>-- and
        <span class="emphasis"><em>fully-distributed</em></span> where the daemons are spread
        across all nodes in the cluster <sup>[<a name="d366e885" href="#ftn.d366e885" class="footnote">10</a>]</sup>.</p><p>Pseudo-distributed mode can run against the local filesystem or
              it can run against an instance of the <span class="emphasis"><em>Hadoop
                  Distributed File System</em></span> (HDFS). Fully-distributed mode can
              ONLY run on HDFS. See the Hadoop <a class="link" href="http://hadoop.apache.org/common/docs/r1.1.1/api/overview-summary.html#overview_description" target="_top">
        requirements and instructions</a> for how to set up HDFS.</p><p>Below we describe the different distributed setups. Starting,
        verification and exploration of your install, whether a
        <span class="emphasis"><em>pseudo-distributed</em></span> or
        <span class="emphasis"><em>fully-distributed</em></span> configuration is described in a
        section that follows, <a class="xref" href="#confirm" title="2.2.3.&nbsp;Running and Confirming Your Installation">Section&nbsp;2.2.3, &#8220;Running and Confirming Your Installation&#8221;</a>. The same verification script applies to both
        deploy types.</p><div class="section" title="2.2.2.1.&nbsp;Pseudo-distributed"><div class="titlepage"><div><div><h4 class="title"><a name="pseudo"></a>2.2.2.1.&nbsp;Pseudo-distributed</h4></div></div></div><p>A pseudo-distributed mode is simply a fully-distributed mode run on
          a single host. Use this configuration testing and prototyping on
          HBase. Do not use this configuration for production nor for
          evaluating HBase performance.</p><p>First, if you want to run on HDFS rather than on the local filesystem,
          setup your HDFS.  You can set up HDFS also in pseudo-distributed mode
          (TODO: Add pointer to HOWTO doc; the hadoop site doesn't have any any more).
          Ensure you have a working HDFS before proceeding.
   	      </p><p>Next, configure HBase.  Edit <code class="filename">conf/hbase-site.xml</code>.
              This is the file into which you add local customizations and overrides.
          At a minimum, you must tell HBase to run in (pseudo-)distributed mode rather than
          in default standalone mode.  To do this, set the <code class="varname">hbase.cluster.distributed</code>
          property to true (Its default is <code class="varname">false</code>).  The absolute bare-minimum
          <code class="filename">hbase-site.xml</code> is therefore as follows:
</p><pre class="programlisting">
&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;
</pre><p>
With this configuration, HBase will start up an HBase Master process, a ZooKeeper server,
and a RegionServer process running against the
local filesystem writing to wherever your operating system stores temporary files into a directory
named <code class="filename">hbase-YOUR_USER_NAME</code>.</p><p>Such a setup, using the local filesystem and
writing to the operating systems's temporary directory is an ephemeral setup; the Hadoop
local filesystem -- which is what HBase uses when it is writing the local filesytem does not
support <span class="command"><strong>sync</strong></span> so unless the system is shutdown properly, the data will be lost.  Writing to
the operating system's temporary directory can also make for data loss when the machine
is restarted as this directory is usually cleared on reboot.  For a more permanent
setup, see the next example where we make use of an instance of HDFS; HBase data will
be written to the Hadoop distributed filesystem rather than to the local filesystem's
tmp directory.</p><p>In this <code class="filename">conf/hbase-site.xml</code> example, the
<code class="varname">hbase.rootdir</code> property points to the local HDFS instance
homed on the node <code class="varname">h-24-30.example.com</code>.
          </p><div class="note" title="Let HBase create ${hbase.rootdir}" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Let HBase create <code class="filename">${hbase.rootdir}</code></h3><p>Let HBase create the <code class="varname">hbase.rootdir</code>
            directory. If you don't, you'll get warning saying HBase needs a
            migration run because the directory is missing files expected by
            HBase (it'll create them if you let it).</p></div><p>
</p><pre class="programlisting">
&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.rootdir&lt;/name&gt;
    &lt;value&gt;hdfs://h-24-30.sfo.stumble.net:8020/hbase&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;
</pre><p>
</p><p>Now skip to <a class="xref" href="#confirm" title="2.2.3.&nbsp;Running and Confirming Your Installation">Section&nbsp;2.2.3, &#8220;Running and Confirming Your Installation&#8221;</a> for how to start and verify your
          pseudo-distributed install. <sup>[<a name="d366e968" href="#ftn.d366e968" class="footnote">11</a>]</sup></p><div class="section" title="2.2.2.1.1.&nbsp;Pseudo-distributed Extras"><div class="titlepage"><div><div><h5 class="title"><a name="pseudo.extras"></a>2.2.2.1.1.&nbsp;Pseudo-distributed Extras</h5></div></div></div><div class="section" title="2.2.2.1.1.1.&nbsp;Startup"><div class="titlepage"><div><div><h6 class="title"><a name="pseudo.extras.start"></a>2.2.2.1.1.1.&nbsp;Startup</h6></div></div></div><p>To start up the initial HBase cluster...
                   </p><pre class="programlisting">% bin/start-hbase.sh</pre><p>
                </p><p>To start up an extra backup master(s) on the same server run...
                       </p><pre class="programlisting">% bin/local-master-backup.sh start 1</pre><p>
                       ... the '1' means use ports 16001 &amp; 16011, and this backup master's
		       logfile will be at 
		       <code class="filename">logs/hbase-${USER}-1-master-${HOSTNAME}.log</code>.
                </p><p>To startup multiple backup masters run... </p><pre class="programlisting">% bin/local-master-backup.sh start 2 3</pre><p> You can start up to 9 backup masters (10 total).
 				</p><p>To start up more regionservers...
     			  </p><pre class="programlisting">% bin/local-regionservers.sh start 1</pre><p>
			... where '1' means use ports 16201 &amp; 16301 and its logfile will be at 
			`<code class="filename">logs/hbase-${USER}-1-regionserver-${HOSTNAME}.log</code>.
     			</p><p>To add 4 more regionservers in addition to the one you just started by running... </p><pre class="programlisting">% bin/local-regionservers.sh start 2 3 4 5</pre><p>
     			This supports up to 99 extra regionservers (100 total).
				</p></div><div class="section" title="2.2.2.1.1.2.&nbsp;Stop"><div class="titlepage"><div><div><h6 class="title"><a name="pseudo.options.stop"></a>2.2.2.1.1.2.&nbsp;Stop</h6></div></div></div><p>Assuming you want to stop master backup # 1, run...
            	</p><pre class="programlisting">% cat /tmp/hbase-${USER}-1-master.pid |xargs kill -9</pre><p>
            	Note that bin/local-master-backup.sh stop 1 will try to stop the cluster along with the master.
            	</p><p>To stop an individual regionserver, run...
                	</p><pre class="programlisting">% bin/local-regionservers.sh stop 1
	                </pre><p>
				</p></div></div></div><div class="section" title="2.2.2.2.&nbsp;Fully-distributed"><div class="titlepage"><div><div><h4 class="title"><a name="fully_dist"></a>2.2.2.2.&nbsp;Fully-distributed</h4></div></div></div><p>For running a fully-distributed operation on more than one
          host, make the following configurations. In
          <code class="filename">hbase-site.xml</code>, add the property
          <code class="varname">hbase.cluster.distributed</code> and set it to
          <code class="varname">true</code> and point the HBase
          <code class="varname">hbase.rootdir</code> at the appropriate HDFS NameNode
          and location in HDFS where you would like HBase to write data. For
          example, if you namenode were running at namenode.example.org on
          port 8020 and you wanted to home your HBase in HDFS at
          <code class="filename">/hbase</code>, make the following
          configuration.</p><pre class="programlisting">
&lt;configuration&gt;
  ...
  &lt;property&gt;
    &lt;name&gt;hbase.rootdir&lt;/name&gt;
    &lt;value&gt;hdfs://namenode.example.org:8020/hbase&lt;/value&gt;
    &lt;description&gt;The directory shared by RegionServers.
    &lt;/description&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
    &lt;description&gt;The mode the cluster will be in. Possible values are
      false: standalone and pseudo-distributed setups with managed Zookeeper
      true: fully-distributed with unmanaged Zookeeper Quorum (see hbase-env.sh)
    &lt;/description&gt;
  &lt;/property&gt;
  ...
&lt;/configuration&gt;
</pre><div class="section" title="2.2.2.2.1.&nbsp;regionservers"><div class="titlepage"><div><div><h5 class="title"><a name="regionserver"></a>2.2.2.2.1.&nbsp;<code class="filename">regionservers</code></h5></div></div></div><p>In addition, a fully-distributed mode requires that you
            modify <code class="filename">conf/regionservers</code>. The
            <a class="xref" href="#regionservers" title="2.4.1.2.&nbsp;regionservers">Section&nbsp;2.4.1.2, &#8220;<code class="filename">regionservers</code>&#8221;</a> file
            lists all hosts that you would have running
            <span class="application">HRegionServer</span>s, one host per line (This
            file in HBase is like the Hadoop <code class="filename">slaves</code>
            file). All servers listed in this file will be started and stopped
            when HBase cluster start or stop is run.</p></div><div class="section" title="2.2.2.2.2.&nbsp;ZooKeeper and HBase"><div class="titlepage"><div><div><h5 class="title"><a name="hbase.zookeeper"></a>2.2.2.2.2.&nbsp;ZooKeeper and HBase</h5></div></div></div><p>See section <a class="xref" href="#zookeeper" title="Chapter&nbsp;17.&nbsp;ZooKeeper">Chapter&nbsp;17, <i>ZooKeeper</i></a> for ZooKeeper setup for HBase.</p></div><div class="section" title="2.2.2.2.3.&nbsp;HDFS Client Configuration"><div class="titlepage"><div><div><h5 class="title"><a name="hdfs_client_conf"></a>2.2.2.2.3.&nbsp;HDFS Client Configuration</h5></div></div></div><p>Of note, if you have made <span class="emphasis"><em>HDFS client
            configuration</em></span> on your Hadoop cluster -- i.e.
            configuration you want HDFS clients to use as opposed to
            server-side configurations -- HBase will not see this
            configuration unless you do one of the following:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p>Add a pointer to your <code class="varname">HADOOP_CONF_DIR</code>
                to the <code class="varname">HBASE_CLASSPATH</code> environment variable
                in <code class="filename">hbase-env.sh</code>.</p></li><li class="listitem"><p>Add a copy of <code class="filename">hdfs-site.xml</code> (or
                <code class="filename">hadoop-site.xml</code>) or, better, symlinks,
                under <code class="filename">${HBASE_HOME}/conf</code>, or</p></li><li class="listitem"><p>if only a small set of HDFS client configurations, add
                them to <code class="filename">hbase-site.xml</code>.</p></li></ul></div><p>An example of such an HDFS client configuration is
            <code class="varname">dfs.replication</code>. If for example, you want to
            run with a replication factor of 5, hbase will create files with
            the default of 3 unless you do the above to make the configuration
            available to HBase.</p></div></div></div><div class="section" title="2.2.3.&nbsp;Running and Confirming Your Installation"><div class="titlepage"><div><div><h3 class="title"><a name="confirm"></a>2.2.3.&nbsp;Running and Confirming Your Installation</h3></div></div></div><p>Make sure HDFS is running first. Start and stop the Hadoop HDFS
        daemons by running <code class="filename">bin/start-hdfs.sh</code> over in the
        <code class="varname">HADOOP_HOME</code> directory. You can ensure it started
        properly by testing the <span class="command"><strong>put</strong></span> and
        <span class="command"><strong>get</strong></span> of files into the Hadoop filesystem. HBase does
        not normally use the mapreduce daemons. These do not need to be
        started.</p><p><span class="emphasis"><em>If</em></span> you are managing your own ZooKeeper,
        start it and confirm its running else, HBase will start up ZooKeeper
        for you as part of its start process.</p><p>Start HBase with the following command:</p><pre class="programlisting">bin/start-hbase.sh</pre>

         Run the above from the

        <code class="varname">HBASE_HOME</code>

         directory.

        <p>You should now have a running HBase instance. HBase logs can be
        found in the <code class="filename">logs</code> subdirectory. Check them out
        especially if HBase had trouble starting.</p><p>HBase also puts up a UI listing vital attributes. By default its
        deployed on the Master host at port 16010 (HBase RegionServers listen
        on port 16020 by default and put up an informational http server at
        16030). If the Master were running on a host named
        <code class="varname">master.example.org</code> on the default port, to see the
        Master's homepage you'd point your browser at
        <code class="filename">http://master.example.org:16010</code>.</p><p>Prior to HBase 0.98, the default ports the master ui was deployed
	on port 16010, and the HBase RegionServers would listen
        on port 16020 by default and put up an informational http server at
        16030.
	</p><p>Once HBase has started, see the <a class="xref" href="#shell_exercises" title="1.2.3.&nbsp;Shell Exercises">Section&nbsp;1.2.3, &#8220;Shell Exercises&#8221;</a> for how to
        create tables, add data, scan your insertions, and finally disable and
        drop your tables.</p><p>To stop HBase after exiting the HBase shell enter
        </p><pre class="programlisting">$ ./bin/stop-hbase.sh
stopping hbase...............</pre><p> Shutdown can take a moment to
        complete. It can take longer if your cluster is comprised of many
        machines. If you are running a distributed operation, be sure to wait
        until HBase has shut down completely before stopping the Hadoop
        daemons.</p></div></div><div class="section" title="2.3.&nbsp;Configuration Files"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="config.files"></a>2.3.&nbsp;Configuration Files</h2></div></div></div><div class="section" title="2.3.1.&nbsp;hbase-site.xml and hbase-default.xml"><div class="titlepage"><div><div><h3 class="title"><a name="hbase.site"></a>2.3.1.&nbsp;<code class="filename">hbase-site.xml</code> and <code class="filename">hbase-default.xml</code></h3></div></div></div><p>Just as in Hadoop where you add site-specific HDFS configuration
    to the <code class="filename">hdfs-site.xml</code> file,
    for HBase, site specific customizations go into
    the file <code class="filename">conf/hbase-site.xml</code>.
    For the list of configurable properties, see
    <a class="xref" href="#hbase_default_configurations" title="2.3.1.1.&nbsp;HBase Default Configuration">Section&nbsp;2.3.1.1, &#8220;HBase Default Configuration&#8221;</a>
    below or view the raw <code class="filename">hbase-default.xml</code>
    source file in the HBase source code at
    <code class="filename">src/main/resources</code>.
    </p><p>
    Not all configuration options make it out to
    <code class="filename">hbase-default.xml</code>.  Configuration
    that it is thought rare anyone would change can exist only
    in code; the only way to turn up such configurations is
    via a reading of the source code itself.
    </p><p>
      Currently, changes here will require a cluster restart for HBase to notice the change.
      </p><div class="section" title="2.3.1.1.&nbsp;HBase Default Configuration"><div class="titlepage"><div><div><h4 class="title"><a name="hbase_default_configurations"></a>2.3.1.1.&nbsp;HBase Default Configuration</h4></div></div></div><p></p><div class="glossary" title="HBase Default Configuration"><div class="titlepage"><div><div><h5 class="title"><a name="hbase.default.configuration"></a>HBase Default Configuration</h5></div></div></div><p>
The documentation below is generated using the default hbase configuration file,
<code class="filename">hbase-default.xml</code>, as source.
</p><dl><dt><a name="hbase.tmp.dir"></a><code class="varname">hbase.tmp.dir</code></dt><dd><p>Temporary directory on the local filesystem.
    Change this setting to point to a location more permanent
    than '/tmp', the usual resolve for java.io.tmpdir, as the
    '/tmp' directory is cleared on machine restart.</p><p>Default: <code class="varname">${java.io.tmpdir}/hbase-${user.name}</code></p></dd><dt><a name="hbase.rootdir"></a><code class="varname">hbase.rootdir</code></dt><dd><p>The directory shared by region servers and into
    which HBase persists.  The URL should be 'fully-qualified'
    to include the filesystem scheme.  For example, to specify the
    HDFS directory '/hbase' where the HDFS instance's namenode is
    running at namenode.example.org on port 9000, set this value to:
    hdfs://namenode.example.org:9000/hbase.  By default, we write
    to whatever ${hbase.tmp.dir} is set too -- usually /tmp --
    so change this configuration or else all data will be lost on
    machine restart.</p><p>Default: <code class="varname">${hbase.tmp.dir}/hbase</code></p></dd><dt><a name="hbase.cluster.distributed"></a><code class="varname">hbase.cluster.distributed</code></dt><dd><p>The mode the cluster will be in. Possible values are
      false for standalone mode and true for distributed mode.  If
      false, startup will run all HBase and ZooKeeper daemons together
      in the one JVM.</p><p>Default: <code class="varname">false</code></p></dd><dt><a name="hbase.zookeeper.quorum"></a><code class="varname">hbase.zookeeper.quorum</code></dt><dd><p>Comma separated list of servers in the ZooKeeper ensemble
    (This config. should have been named hbase.zookeeper.ensemble).
    For example, "host1.mydomain.com,host2.mydomain.com,host3.mydomain.com".
    By default this is set to localhost for local and pseudo-distributed modes
    of operation. For a fully-distributed setup, this should be set to a full
    list of ZooKeeper ensemble servers. If HBASE_MANAGES_ZK is set in hbase-env.sh
    this is the list of servers which hbase will start/stop ZooKeeper on as
    part of cluster start/stop.  Client-side, we will take this list of
    ensemble members and put it together with the hbase.zookeeper.clientPort
    config. and pass it into zookeeper constructor as the connectString
    parameter.</p><p>Default: <code class="varname">localhost</code></p></dd><dt><a name="hbase.local.dir"></a><code class="varname">hbase.local.dir</code></dt><dd><p>Directory on the local filesystem to be used
    as a local storage.</p><p>Default: <code class="varname">${hbase.tmp.dir}/local/</code></p></dd><dt><a name="hbase.master.port"></a><code class="varname">hbase.master.port</code></dt><dd><p>The port the HBase Master should bind to.</p><p>Default: <code class="varname">60000</code></p></dd><dt><a name="hbase.master.info.port"></a><code class="varname">hbase.master.info.port</code></dt><dd><p>The port for the HBase Master web UI.
    Set to -1 if you do not want a UI instance run.</p><p>Default: <code class="varname">60010</code></p></dd><dt><a name="hbase.master.info.bindAddress"></a><code class="varname">hbase.master.info.bindAddress</code></dt><dd><p>The bind address for the HBase Master web UI
    </p><p>Default: <code class="varname">0.0.0.0</code></p></dd><dt><a name="hbase.master.logcleaner.plugins"></a><code class="varname">hbase.master.logcleaner.plugins</code></dt><dd><p>A comma-separated list of LogCleanerDelegate invoked by
    the LogsCleaner service. These WAL/HLog cleaners are called in order,
    so put the HLog cleaner that prunes the most HLog files in front. To
    implement your own LogCleanerDelegate, just put it in HBase's classpath
    and add the fully qualified class name here. Always add the above
    default log cleaners in the list.</p><p>Default: <code class="varname">org.apache.hadoop.hbase.master.cleaner.TimeToLiveLogCleaner</code></p></dd><dt><a name="hbase.master.logcleaner.ttl"></a><code class="varname">hbase.master.logcleaner.ttl</code></dt><dd><p>Maximum time a HLog can stay in the .oldlogdir directory,
    after which it will be cleaned by a Master thread.</p><p>Default: <code class="varname">600000</code></p></dd><dt><a name="hbase.master.hfilecleaner.plugins"></a><code class="varname">hbase.master.hfilecleaner.plugins</code></dt><dd><p>A comma-separated list of HFileCleanerDelegate invoked by
    the HFileCleaner service. These HFiles cleaners are called in order,
    so put the cleaner that prunes the most files in front. To
    implement your own HFileCleanerDelegate, just put it in HBase's classpath
    and add the fully qualified class name here. Always add the above
    default log cleaners in the list as they will be overwritten in
    hbase-site.xml.</p><p>Default: <code class="varname">org.apache.hadoop.hbase.master.cleaner.TimeToLiveHFileCleaner</code></p></dd><dt><a name="hbase.master.catalog.timeout"></a><code class="varname">hbase.master.catalog.timeout</code></dt><dd><p>Timeout value for the Catalog Janitor from the master to
    META.</p><p>Default: <code class="varname">600000</code></p></dd><dt><a name="fail.fast.expired.active.master"></a><code class="varname">fail.fast.expired.active.master</code></dt><dd><p>If abort immediately for the expired master without trying
      to recover its zk session.</p><p>Default: <code class="varname">false</code></p></dd><dt><a name="hbase.master.dns.interface"></a><code class="varname">hbase.master.dns.interface</code></dt><dd><p>The name of the Network Interface from which a master
      should report its IP address.</p><p>Default: <code class="varname">default</code></p></dd><dt><a name="hbase.master.dns.nameserver"></a><code class="varname">hbase.master.dns.nameserver</code></dt><dd><p>The host name or IP address of the name server (DNS)
      which a master should use to determine the host name used
      for communication and display purposes.</p><p>Default: <code class="varname">default</code></p></dd><dt><a name="hbase.regionserver.port"></a><code class="varname">hbase.regionserver.port</code></dt><dd><p>The port the HBase RegionServer binds to.</p><p>Default: <code class="varname">60020</code></p></dd><dt><a name="hbase.regionserver.info.port"></a><code class="varname">hbase.regionserver.info.port</code></dt><dd><p>The port for the HBase RegionServer web UI
    Set to -1 if you do not want the RegionServer UI to run.</p><p>Default: <code class="varname">60030</code></p></dd><dt><a name="hbase.regionserver.info.bindAddress"></a><code class="varname">hbase.regionserver.info.bindAddress</code></dt><dd><p>The address for the HBase RegionServer web UI</p><p>Default: <code class="varname">0.0.0.0</code></p></dd><dt><a name="hbase.regionserver.info.port.auto"></a><code class="varname">hbase.regionserver.info.port.auto</code></dt><dd><p>Whether or not the Master or RegionServer
    UI should search for a port to bind to. Enables automatic port
    search if hbase.regionserver.info.port is already in use.
    Useful for testing, turned off by default.</p><p>Default: <code class="varname">false</code></p></dd><dt><a name="hbase.regionserver.handler.count"></a><code class="varname">hbase.regionserver.handler.count</code></dt><dd><p>Count of RPC Listener instances spun up on RegionServers.
    Same property is used by the Master for count of master handlers.</p><p>Default: <code class="varname">30</code></p></dd><dt><a name="hbase.regionserver.msginterval"></a><code class="varname">hbase.regionserver.msginterval</code></dt><dd><p>Interval between messages from the RegionServer to Master
    in milliseconds.</p><p>Default: <code class="varname">3000</code></p></dd><dt><a name="hbase.regionserver.regionSplitLimit"></a><code class="varname">hbase.regionserver.regionSplitLimit</code></dt><dd><p>Limit for the number of regions after which no more region
    splitting should take place. This is not a hard limit for the number of
    regions but acts as a guideline for the regionserver to stop splitting after
    a certain limit. Default is MAX_INT; i.e. do not block splitting.</p><p>Default: <code class="varname">2147483647</code></p></dd><dt><a name="hbase.regionserver.logroll.period"></a><code class="varname">hbase.regionserver.logroll.period</code></dt><dd><p>Period at which we will roll the commit log regardless
    of how many edits it has.</p><p>Default: <code class="varname">3600000</code></p></dd><dt><a name="hbase.regionserver.logroll.errors.tolerated"></a><code class="varname">hbase.regionserver.logroll.errors.tolerated</code></dt><dd><p>The number of consecutive WAL close errors we will allow
    before triggering a server abort.  A setting of 0 will cause the
    region server to abort if closing the current WAL writer fails during
    log rolling.  Even a small value (2 or 3) will allow a region server
    to ride over transient HDFS errors.</p><p>Default: <code class="varname">2</code></p></dd><dt><a name="hbase.regionserver.hlog.reader.impl"></a><code class="varname">hbase.regionserver.hlog.reader.impl</code></dt><dd><p>The HLog file reader implementation.</p><p>Default: <code class="varname">org.apache.hadoop.hbase.regionserver.wal.ProtobufLogReader</code></p></dd><dt><a name="hbase.regionserver.hlog.writer.impl"></a><code class="varname">hbase.regionserver.hlog.writer.impl</code></dt><dd><p>The HLog file writer implementation.</p><p>Default: <code class="varname">org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter</code></p></dd><dt><a name="hbase.regionserver.global.memstore.upperLimit"></a><code class="varname">hbase.regionserver.global.memstore.upperLimit</code></dt><dd><p>Maximum size of all memstores in a region server before new
      updates are blocked and flushes are forced. Defaults to 40% of heap.
      Updates are blocked and flushes are forced until size of all memstores
      in a region server hits hbase.regionserver.global.memstore.lowerLimit.</p><p>Default: <code class="varname">0.4</code></p></dd><dt><a name="hbase.regionserver.global.memstore.lowerLimit"></a><code class="varname">hbase.regionserver.global.memstore.lowerLimit</code></dt><dd><p>Maximum size of all memstores in a region server before
      flushes are forced. Defaults to 38% of heap.
      This value equal to hbase.regionserver.global.memstore.upperLimit causes
      the minimum possible flushing to occur when updates are blocked due to
      memstore limiting.</p><p>Default: <code class="varname">0.38</code></p></dd><dt><a name="hbase.regionserver.optionalcacheflushinterval"></a><code class="varname">hbase.regionserver.optionalcacheflushinterval</code></dt><dd><p>
    Maximum amount of time an edit lives in memory before being automatically flushed.
    Default 1 hour. Set it to 0 to disable automatic flushing.</p><p>Default: <code class="varname">3600000</code></p></dd><dt><a name="hbase.regionserver.catalog.timeout"></a><code class="varname">hbase.regionserver.catalog.timeout</code></dt><dd><p>Timeout value for the Catalog Janitor from the regionserver to META.</p><p>Default: <code class="varname">600000</code></p></dd><dt><a name="hbase.regionserver.dns.interface"></a><code class="varname">hbase.regionserver.dns.interface</code></dt><dd><p>The name of the Network Interface from which a region server
      should report its IP address.</p><p>Default: <code class="varname">default</code></p></dd><dt><a name="hbase.regionserver.dns.nameserver"></a><code class="varname">hbase.regionserver.dns.nameserver</code></dt><dd><p>The host name or IP address of the name server (DNS)
      which a region server should use to determine the host name used by the
      master for communication and display purposes.</p><p>Default: <code class="varname">default</code></p></dd><dt><a name="hbase.regionserver.region.split.policy"></a><code class="varname">hbase.regionserver.region.split.policy</code></dt><dd><p>
      A split policy determines when a region should be split. The various other split policies that
      are available currently are ConstantSizeRegionSplitPolicy, DisabledRegionSplitPolicy, 
      DelimitedKeyPrefixRegionSplitPolicy, KeyPrefixRegionSplitPolicy etc.  
    </p><p>Default: <code class="varname">org.apache.hadoop.hbase.regionserver.IncreasingToUpperBoundRegionSplitPolicy</code></p></dd><dt><a name="zookeeper.session.timeout"></a><code class="varname">zookeeper.session.timeout</code></dt><dd><p>ZooKeeper session timeout in milliseconds. It is used in two different ways.
      First, this value is used in the ZK client that HBase uses to connect to the ensemble.
      It is also used by HBase when it starts a ZK server and it is passed as the 'maxSessionTimeout'. See
      http://hadoop.apache.org/zookeeper/docs/current/zookeeperProgrammers.html#ch_zkSessions.
      For example, if a HBase region server connects to a ZK ensemble that's also managed by HBase, then the
      session timeout will be the one specified by this configuration. But, a region server that connects
      to an ensemble managed with a different configuration will be subjected that ensemble's maxSessionTimeout. So,
      even though HBase might propose using 90 seconds, the ensemble can have a max timeout lower than this and
      it will take precedence. The current default that ZK ships with is 40 seconds, which is lower than HBase's.
    </p><p>Default: <code class="varname">90000</code></p></dd><dt><a name="zookeeper.znode.parent"></a><code class="varname">zookeeper.znode.parent</code></dt><dd><p>Root ZNode for HBase in ZooKeeper. All of HBase's ZooKeeper
      files that are configured with a relative path will go under this node.
      By default, all of HBase's ZooKeeper file path are configured with a
      relative path, so they will all go under this directory unless changed.</p><p>Default: <code class="varname">/hbase</code></p></dd><dt><a name="zookeeper.znode.rootserver"></a><code class="varname">zookeeper.znode.rootserver</code></dt><dd><p>Path to ZNode holding root region location. This is written by
      the master and read by clients and region servers. If a relative path is
      given, the parent folder will be ${zookeeper.znode.parent}. By default,
      this means the root location is stored at /hbase/root-region-server.</p><p>Default: <code class="varname">root-region-server</code></p></dd><dt><a name="zookeeper.znode.acl.parent"></a><code class="varname">zookeeper.znode.acl.parent</code></dt><dd><p>Root ZNode for access control lists.</p><p>Default: <code class="varname">acl</code></p></dd><dt><a name="hbase.zookeeper.dns.interface"></a><code class="varname">hbase.zookeeper.dns.interface</code></dt><dd><p>The name of the Network Interface from which a ZooKeeper server
      should report its IP address.</p><p>Default: <code class="varname">default</code></p></dd><dt><a name="hbase.zookeeper.dns.nameserver"></a><code class="varname">hbase.zookeeper.dns.nameserver</code></dt><dd><p>The host name or IP address of the name server (DNS)
      which a ZooKeeper server should use to determine the host name used by the
      master for communication and display purposes.</p><p>Default: <code class="varname">default</code></p></dd><dt><a name="hbase.zookeeper.peerport"></a><code class="varname">hbase.zookeeper.peerport</code></dt><dd><p>Port used by ZooKeeper peers to talk to each other.
    Seehttp://hadoop.apache.org/zookeeper/docs/r3.1.1/zookeeperStarted.html#sc_RunningReplicatedZooKeeper
    for more information.</p><p>Default: <code class="varname">2888</code></p></dd><dt><a name="hbase.zookeeper.leaderport"></a><code class="varname">hbase.zookeeper.leaderport</code></dt><dd><p>Port used by ZooKeeper for leader election.
    See http://hadoop.apache.org/zookeeper/docs/r3.1.1/zookeeperStarted.html#sc_RunningReplicatedZooKeeper
    for more information.</p><p>Default: <code class="varname">3888</code></p></dd><dt><a name="hbase.zookeeper.useMulti"></a><code class="varname">hbase.zookeeper.useMulti</code></dt><dd><p>Instructs HBase to make use of ZooKeeper's multi-update functionality.
    This allows certain ZooKeeper operations to complete more quickly and prevents some issues
    with rare Replication failure scenarios (see the release note of HBASE-2611 for an example).
    IMPORTANT: only set this to true if all ZooKeeper servers in the cluster are on version 3.4+
    and will not be downgraded.  ZooKeeper versions before 3.4 do not support multi-update and
    will not fail gracefully if multi-update is invoked (see ZOOKEEPER-1495).</p><p>Default: <code class="varname">false</code></p></dd><dt><a name="hbase.config.read.zookeeper.config"></a><code class="varname">hbase.config.read.zookeeper.config</code></dt><dd><p>
        Set to true to allow HBaseConfiguration to read the
        zoo.cfg file for ZooKeeper properties. Switching this to true
        is not recommended, since the functionality of reading ZK
        properties from a zoo.cfg file has been deprecated.</p><p>Default: <code class="varname">false</code></p></dd><dt><a name="hbase.zookeeper.property.initLimit"></a><code class="varname">hbase.zookeeper.property.initLimit</code></dt><dd><p>Property from ZooKeeper's config zoo.cfg.
    The number of ticks that the initial synchronization phase can take.</p><p>Default: <code class="varname">10</code></p></dd><dt><a name="hbase.zookeeper.property.syncLimit"></a><code class="varname">hbase.zookeeper.property.syncLimit</code></dt><dd><p>Property from ZooKeeper's config zoo.cfg.
    The number of ticks that can pass between sending a request and getting an
    acknowledgment.</p><p>Default: <code class="varname">5</code></p></dd><dt><a name="hbase.zookeeper.property.dataDir"></a><code class="varname">hbase.zookeeper.property.dataDir</code></dt><dd><p>Property from ZooKeeper's config zoo.cfg.
    The directory where the snapshot is stored.</p><p>Default: <code class="varname">${hbase.tmp.dir}/zookeeper</code></p></dd><dt><a name="hbase.zookeeper.property.clientPort"></a><code class="varname">hbase.zookeeper.property.clientPort</code></dt><dd><p>Property from ZooKeeper's config zoo.cfg.
    The port at which the clients will connect.</p><p>Default: <code class="varname">2181</code></p></dd><dt><a name="hbase.zookeeper.property.maxClientCnxns"></a><code class="varname">hbase.zookeeper.property.maxClientCnxns</code></dt><dd><p>Property from ZooKeeper's config zoo.cfg.
    Limit on number of concurrent connections (at the socket level) that a
    single client, identified by IP address, may make to a single member of
    the ZooKeeper ensemble. Set high to avoid zk connection issues running
    standalone and pseudo-distributed.</p><p>Default: <code class="varname">300</code></p></dd><dt><a name="hbase.client.write.buffer"></a><code class="varname">hbase.client.write.buffer</code></dt><dd><p>Default size of the HTable client write buffer in bytes.
    A bigger buffer takes more memory -- on both the client and server
    side since server instantiates the passed write buffer to process
    it -- but a larger buffer size reduces the number of RPCs made.
    For an estimate of server-side memory-used, evaluate
    hbase.client.write.buffer * hbase.regionserver.handler.count</p><p>Default: <code class="varname">2097152</code></p></dd><dt><a name="hbase.client.pause"></a><code class="varname">hbase.client.pause</code></dt><dd><p>General client pause value.  Used mostly as value to wait
    before running a retry of a failed get, region lookup, etc.
    See hbase.client.retries.number for description of how we backoff from
    this initial pause amount and how this pause works w/ retries.</p><p>Default: <code class="varname">100</code></p></dd><dt><a name="hbase.client.retries.number"></a><code class="varname">hbase.client.retries.number</code></dt><dd><p>Maximum retries.  Used as maximum for all retryable
    operations such as the getting of a cell's value, starting a row update,
    etc.  Retry interval is a rough function based on hbase.client.pause.  At
    first we retry at this interval but then with backoff, we pretty quickly reach
    retrying every ten seconds.  See HConstants#RETRY_BACKOFF for how the backup
    ramps up.  Change this setting and hbase.client.pause to suit your workload.</p><p>Default: <code class="varname">35</code></p></dd><dt><a name="hbase.client.max.total.tasks"></a><code class="varname">hbase.client.max.total.tasks</code></dt><dd><p>The maximum number of concurrent tasks a single HTable instance will
    send to the cluster.</p><p>Default: <code class="varname">100</code></p></dd><dt><a name="hbase.client.max.perserver.tasks"></a><code class="varname">hbase.client.max.perserver.tasks</code></dt><dd><p>The maximum number of concurrent tasks a single HTable instance will
    send to a single region server.</p><p>Default: <code class="varname">5</code></p></dd><dt><a name="hbase.client.max.perregion.tasks"></a><code class="varname">hbase.client.max.perregion.tasks</code></dt><dd><p>The maximum number of concurrent connections the client will
    maintain to a single Region. That is, if there is already
    hbase.client.max.perregion.tasks writes in progress for this region, new puts
    won't be sent to this region until some writes finishes.</p><p>Default: <code class="varname">1</code></p></dd><dt><a name="hbase.client.scanner.caching"></a><code class="varname">hbase.client.scanner.caching</code></dt><dd><p>Number of rows that will be fetched when calling next
    on a scanner if it is not served from (local, client) memory. Higher
    caching values will enable faster scanners but will eat up more memory
    and some calls of next may take longer and longer times when the cache is empty.
    Do not set this value such that the time between invocations is greater
    than the scanner timeout; i.e. hbase.client.scanner.timeout.period</p><p>Default: <code class="varname">100</code></p></dd><dt><a name="hbase.client.keyvalue.maxsize"></a><code class="varname">hbase.client.keyvalue.maxsize</code></dt><dd><p>Specifies the combined maximum allowed size of a KeyValue
    instance. This is to set an upper boundary for a single entry saved in a
    storage file. Since they cannot be split it helps avoiding that a region
    cannot be split any further because the data is too large. It seems wise
    to set this to a fraction of the maximum region size. Setting it to zero
    or less disables the check.</p><p>Default: <code class="varname">10485760</code></p></dd><dt><a name="hbase.client.scanner.timeout.period"></a><code class="varname">hbase.client.scanner.timeout.period</code></dt><dd><p>Client scanner lease period in milliseconds.</p><p>Default: <code class="varname">60000</code></p></dd><dt><a name="hbase.client.localityCheck.threadPoolSize"></a><code class="varname">hbase.client.localityCheck.threadPoolSize</code></dt><dd><p></p><p>Default: <code class="varname">2</code></p></dd><dt><a name="hbase.bulkload.retries.number"></a><code class="varname">hbase.bulkload.retries.number</code></dt><dd><p>Maximum retries.  This is maximum number of iterations
    to atomic bulk loads are attempted in the face of splitting operations
    0 means never give up.</p><p>Default: <code class="varname">0</code></p></dd><dt><a name="hbase.balancer.period%0A    "></a><code class="varname">hbase.balancer.period
    </code></dt><dd><p>Period at which the region balancer runs in the Master.</p><p>Default: <code class="varname">300000</code></p></dd><dt><a name="hbase.regions.slop"></a><code class="varname">hbase.regions.slop</code></dt><dd><p>Rebalance if any regionserver has average + (average * slop) regions.</p><p>Default: <code class="varname">0.2</code></p></dd><dt><a name="hbase.server.thread.wakefrequency"></a><code class="varname">hbase.server.thread.wakefrequency</code></dt><dd><p>Time to sleep in between searches for work (in milliseconds).
    Used as sleep interval by service threads such as log roller.</p><p>Default: <code class="varname">10000</code></p></dd><dt><a name="hbase.server.versionfile.writeattempts"></a><code class="varname">hbase.server.versionfile.writeattempts</code></dt><dd><p>
    How many time to retry attempting to write a version file
    before just aborting. Each attempt is seperated by the
    hbase.server.thread.wakefrequency milliseconds.</p><p>Default: <code class="varname">3</code></p></dd><dt><a name="hbase.hregion.memstore.flush.size"></a><code class="varname">hbase.hregion.memstore.flush.size</code></dt><dd><p>
    Memstore will be flushed to disk if size of the memstore
    exceeds this number of bytes.  Value is checked by a thread that runs
    every hbase.server.thread.wakefrequency.</p><p>Default: <code class="varname">134217728</code></p></dd><dt><a name="hbase.hregion.preclose.flush.size"></a><code class="varname">hbase.hregion.preclose.flush.size</code></dt><dd><p>
      If the memstores in a region are this size or larger when we go
      to close, run a "pre-flush" to clear out memstores before we put up
      the region closed flag and take the region offline.  On close,
      a flush is run under the close flag to empty memory.  During
      this time the region is offline and we are not taking on any writes.
      If the memstore content is large, this flush could take a long time to
      complete.  The preflush is meant to clean out the bulk of the memstore
      before putting up the close flag and taking the region offline so the
      flush that runs under the close flag has little to do.</p><p>Default: <code class="varname">5242880</code></p></dd><dt><a name="hbase.hregion.memstore.block.multiplier"></a><code class="varname">hbase.hregion.memstore.block.multiplier</code></dt><dd><p>
    Block updates if memstore has hbase.hregion.memstore.block.multiplier
    times hbase.hregion.memstore.flush.size bytes.  Useful preventing
    runaway memstore during spikes in update traffic.  Without an
    upper-bound, memstore fills such that when it flushes the
    resultant flush files take a long time to compact or split, or
    worse, we OOME.</p><p>Default: <code class="varname">2</code></p></dd><dt><a name="hbase.hregion.memstore.mslab.enabled"></a><code class="varname">hbase.hregion.memstore.mslab.enabled</code></dt><dd><p>
      Enables the MemStore-Local Allocation Buffer,
      a feature which works to prevent heap fragmentation under
      heavy write loads. This can reduce the frequency of stop-the-world
      GC pauses on large heaps.</p><p>Default: <code class="varname">true</code></p></dd><dt><a name="hbase.hregion.max.filesize"></a><code class="varname">hbase.hregion.max.filesize</code></dt><dd><p>
    Maximum HStoreFile size. If any one of a column families' HStoreFiles has
    grown to exceed this value, the hosting HRegion is split in two.</p><p>Default: <code class="varname">10737418240</code></p></dd><dt><a name="hbase.hregion.majorcompaction"></a><code class="varname">hbase.hregion.majorcompaction</code></dt><dd><p>The time (in miliseconds) between 'major' compactions of all
    HStoreFiles in a region.  Default: Set to 7 days.  Major compactions tend to
    happen exactly when you need them least so enable them such that they run at
    off-peak for your deploy; or, since this setting is on a periodicity that is
    unlikely to match your loading, run the compactions via an external
    invocation out of a cron job or some such.</p><p>Default: <code class="varname">604800000</code></p></dd><dt><a name="hbase.hregion.majorcompaction.jitter"></a><code class="varname">hbase.hregion.majorcompaction.jitter</code></dt><dd><p>Jitter outer bound for major compactions.
    On each regionserver, we multiply the hbase.region.majorcompaction
    interval by some random fraction that is inside the bounds of this
    maximum.  We then add this + or - product to when the next
    major compaction is to run.  The idea is that major compaction
    does happen on every regionserver at exactly the same time.  The
    smaller this number, the closer the compactions come together.</p><p>Default: <code class="varname">0.50</code></p></dd><dt><a name="hbase.hstore.compactionThreshold"></a><code class="varname">hbase.hstore.compactionThreshold</code></dt><dd><p>
    If more than this number of HStoreFiles in any one HStore
    (one HStoreFile is written per flush of memstore) then a compaction
    is run to rewrite all HStoreFiles files as one.  Larger numbers
    put off compaction but when it runs, it takes longer to complete.</p><p>Default: <code class="varname">3</code></p></dd><dt><a name="hbase.hstore.blockingStoreFiles"></a><code class="varname">hbase.hstore.blockingStoreFiles</code></dt><dd><p>
    If more than this number of StoreFiles in any one Store
    (one StoreFile is written per flush of MemStore) then updates are
    blocked for this HRegion until a compaction is completed, or
    until hbase.hstore.blockingWaitTime has been exceeded.</p><p>Default: <code class="varname">10</code></p></dd><dt><a name="hbase.hstore.blockingWaitTime"></a><code class="varname">hbase.hstore.blockingWaitTime</code></dt><dd><p>
    The time an HRegion will block updates for after hitting the StoreFile
    limit defined by hbase.hstore.blockingStoreFiles.
    After this time has elapsed, the HRegion will stop blocking updates even
    if a compaction has not been completed.</p><p>Default: <code class="varname">90000</code></p></dd><dt><a name="hbase.hstore.compaction.max"></a><code class="varname">hbase.hstore.compaction.max</code></dt><dd><p>Max number of HStoreFiles to compact per 'minor' compaction.</p><p>Default: <code class="varname">10</code></p></dd><dt><a name="hbase.hstore.compaction.kv.max"></a><code class="varname">hbase.hstore.compaction.kv.max</code></dt><dd><p>How many KeyValues to read and then write in a batch when flushing
        or compacting.  Do less if big KeyValues and problems with OOME.
        Do more if wide, small rows.</p><p>Default: <code class="varname">10</code></p></dd><dt><a name="hbase.storescanner.parallel.seek.enable"></a><code class="varname">hbase.storescanner.parallel.seek.enable</code></dt><dd><p>
      Enables StoreFileScanner parallel-seeking in StoreScanner,
      a feature which can reduce response latency under special conditions.</p><p>Default: <code class="varname">false</code></p></dd><dt><a name="hbase.storescanner.parallel.seek.threads"></a><code class="varname">hbase.storescanner.parallel.seek.threads</code></dt><dd><p>
      The default thread pool size if parallel-seeking feature enabled.</p><p>Default: <code class="varname">10</code></p></dd><dt><a name="hfile.block.cache.size"></a><code class="varname">hfile.block.cache.size</code></dt><dd><p>Percentage of maximum heap (-Xmx setting) to allocate to block cache
        used by HFile/StoreFile. Default of 0.4 means allocate 40%.
        Set to 0 to disable but it's not recommended; you need at least
        enough cache to hold the storefile indices.</p><p>Default: <code class="varname">0.4</code></p></dd><dt><a name="hfile.block.index.cacheonwrite"></a><code class="varname">hfile.block.index.cacheonwrite</code></dt><dd><p>This allows to put non-root multi-level index blocks into the block
          cache at the time the index is being written.</p><p>Default: <code class="varname">false</code></p></dd><dt><a name="hfile.index.block.max.size"></a><code class="varname">hfile.index.block.max.size</code></dt><dd><p>When the size of a leaf-level, intermediate-level, or root-level
          index block in a multi-level block index grows to this size, the
          block is written out and a new block is started.</p><p>Default: <code class="varname">131072</code></p></dd><dt><a name="hfile.format.version"></a><code class="varname">hfile.format.version</code></dt><dd><p>The HFile format version to use for new files. Set this to 1 to test
          backwards-compatibility. The default value of this option should be
          consistent with FixedFileTrailer.MAX_VERSION.</p><p>Default: <code class="varname">2</code></p></dd><dt><a name="hfile.block.bloom.cacheonwrite"></a><code class="varname">hfile.block.bloom.cacheonwrite</code></dt><dd><p>Enables cache-on-write for inline blocks of a compound Bloom filter.</p><p>Default: <code class="varname">false</code></p></dd><dt><a name="io.storefile.bloom.block.size"></a><code class="varname">io.storefile.bloom.block.size</code></dt><dd><p>The size in bytes of a single block ("chunk") of a compound Bloom
          filter. This size is approximate, because Bloom blocks can only be
          inserted at data block boundaries, and the number of keys per data
          block varies.</p><p>Default: <code class="varname">131072</code></p></dd><dt><a name="hbase.rs.cacheblocksonwrite"></a><code class="varname">hbase.rs.cacheblocksonwrite</code></dt><dd><p>Whether an HFile block should be added to the block cache when the
          block is finished.</p><p>Default: <code class="varname">false</code></p></dd><dt><a name="hbase.rpc.server.engine"></a><code class="varname">hbase.rpc.server.engine</code></dt><dd><p>Implementation of org.apache.hadoop.hbase.ipc.RpcServerEngine to be
    used for server RPC call marshalling.</p><p>Default: <code class="varname">org.apache.hadoop.hbase.ipc.ProtobufRpcServerEngine</code></p></dd><dt><a name="hbase.rpc.timeout"></a><code class="varname">hbase.rpc.timeout</code></dt><dd><p>This is for the RPC layer to define how long HBase client applications
        take for a remote call to time out. It uses pings to check connections
        but will eventually throw a TimeoutException.</p><p>Default: <code class="varname">60000</code></p></dd><dt><a name="hbase.rpc.shortoperation.timeout"></a><code class="varname">hbase.rpc.shortoperation.timeout</code></dt><dd><p>This is another version of "hbase.rpc.timeout". For those RPC operation
        within cluster, we rely on this configuration to set a short timeout limitation
        for short operation. For example, short rpc timeout for region server's trying
        to report to active master can benefit quicker master failover process.</p><p>Default: <code class="varname">10000</code></p></dd><dt><a name="hbase.ipc.client.tcpnodelay"></a><code class="varname">hbase.ipc.client.tcpnodelay</code></dt><dd><p>Set no delay on rpc socket connections.  See
    http://docs.oracle.com/javase/1.5.0/docs/api/java/net/Socket.html#getTcpNoDelay()</p><p>Default: <code class="varname">true</code></p></dd><dt><a name="hbase.master.keytab.file"></a><code class="varname">hbase.master.keytab.file</code></dt><dd><p>Full path to the kerberos keytab file to use for logging in
    the configured HMaster server principal.</p><p>Default: <code class="varname"></code></p></dd><dt><a name="hbase.master.kerberos.principal"></a><code class="varname">hbase.master.kerberos.principal</code></dt><dd><p>Ex. "hbase/_HOST@EXAMPLE.COM".  The kerberos principal name
    that should be used to run the HMaster process.  The principal name should
    be in the form: user/hostname@DOMAIN.  If "_HOST" is used as the hostname
    portion, it will be replaced with the actual hostname of the running
    instance.</p><p>Default: <code class="varname"></code></p></dd><dt><a name="hbase.regionserver.keytab.file"></a><code class="varname">hbase.regionserver.keytab.file</code></dt><dd><p>Full path to the kerberos keytab file to use for logging in
    the configured HRegionServer server principal.</p><p>Default: <code class="varname"></code></p></dd><dt><a name="hbase.regionserver.kerberos.principal"></a><code class="varname">hbase.regionserver.kerberos.principal</code></dt><dd><p>Ex. "hbase/_HOST@EXAMPLE.COM".  The kerberos principal name
    that should be used to run the HRegionServer process.  The principal name
    should be in the form: user/hostname@DOMAIN.  If "_HOST" is used as the
    hostname portion, it will be replaced with the actual hostname of the
    running instance.  An entry for this principal must exist in the file
    specified in hbase.regionserver.keytab.file</p><p>Default: <code class="varname"></code></p></dd><dt><a name="hadoop.policy.file"></a><code class="varname">hadoop.policy.file</code></dt><dd><p>The policy configuration file used by RPC servers to make
      authorization decisions on client requests.  Only used when HBase
      security is enabled.</p><p>Default: <code class="varname">hbase-policy.xml</code></p></dd><dt><a name="hbase.superuser"></a><code class="varname">hbase.superuser</code></dt><dd><p>List of users or groups (comma-separated), who are allowed
    full privileges, regardless of stored ACLs, across the cluster.
    Only used when HBase security is enabled.</p><p>Default: <code class="varname"></code></p></dd><dt><a name="hbase.auth.key.update.interval"></a><code class="varname">hbase.auth.key.update.interval</code></dt><dd><p>The update interval for master key for authentication tokens
    in servers in milliseconds.  Only used when HBase security is enabled.</p><p>Default: <code class="varname">86400000</code></p></dd><dt><a name="hbase.auth.token.max.lifetime"></a><code class="varname">hbase.auth.token.max.lifetime</code></dt><dd><p>The maximum lifetime in milliseconds after which an
    authentication token expires.  Only used when HBase security is enabled.</p><p>Default: <code class="varname">604800000</code></p></dd><dt><a name="hbase.ipc.client.fallback-to-simple-auth-allowed"></a><code class="varname">hbase.ipc.client.fallback-to-simple-auth-allowed</code></dt><dd><p>When a client is configured to attempt a secure connection, but attempts to
      connect to an insecure server, that server may instruct the client to
      switch to SASL SIMPLE (unsecure) authentication. This setting controls
      whether or not the client will accept this instruction from the server.
      When false (the default), the client will not allow the fallback to SIMPLE
      authentication, and will abort the connection.</p><p>Default: <code class="varname">false</code></p></dd><dt><a name="hbase.coprocessor.region.classes"></a><code class="varname">hbase.coprocessor.region.classes</code></dt><dd><p>A comma-separated list of Coprocessors that are loaded by
    default on all tables. For any override coprocessor method, these classes
    will be called in order. After implementing your own Coprocessor, just put
    it in HBase's classpath and add the fully qualified class name here.
    A coprocessor can also be loaded on demand by setting HTableDescriptor.</p><p>Default: <code class="varname"></code></p></dd><dt><a name="hbase.rest.port"></a><code class="varname">hbase.rest.port</code></dt><dd><p>The port for the HBase REST server.</p><p>Default: <code class="varname">8080</code></p></dd><dt><a name="hbase.rest.readonly"></a><code class="varname">hbase.rest.readonly</code></dt><dd><p>Defines the mode the REST server will be started in. Possible values are:
    false: All HTTP methods are permitted - GET/PUT/POST/DELETE.
    true: Only the GET method is permitted.</p><p>Default: <code class="varname">false</code></p></dd><dt><a name="hbase.rest.threads.max"></a><code class="varname">hbase.rest.threads.max</code></dt><dd><p>The maximum number of threads of the REST server thread pool.
        Threads in the pool are reused to process REST requests. This
        controls the maximum number of requests processed concurrently.
        It may help to control the memory used by the REST server to
        avoid OOM issues. If the thread pool is full, incoming requests
        will be queued up and wait for some free threads.</p><p>Default: <code class="varname">100</code></p></dd><dt><a name="hbase.rest.threads.min"></a><code class="varname">hbase.rest.threads.min</code></dt><dd><p>The minimum number of threads of the REST server thread pool.
        The thread pool always has at least these number of threads so
        the REST server is ready to serve incoming requests.</p><p>Default: <code class="varname">2</code></p></dd><dt><a name="hbase.rest.support.proxyuser"></a><code class="varname">hbase.rest.support.proxyuser</code></dt><dd><p>Enables running the REST server to support proxy-user mode.</p><p>Default: <code class="varname">false</code></p></dd><dt><a name="hbase.defaults.for.version.skip"></a><code class="varname">hbase.defaults.for.version.skip</code></dt><dd><p>Set to true to skip the 'hbase.defaults.for.version' check.
    Setting this to true can be useful in contexts other than
    the other side of a maven generation; i.e. running in an
    ide.  You'll want to set this boolean to true to avoid
    seeing the RuntimException complaint: "hbase-default.xml file
    seems to be for and old version of HBase (\${hbase.version}), this
    version is X.X.X-SNAPSHOT"</p><p>Default: <code class="varname">false</code></p></dd><dt><a name="hbase.coprocessor.master.classes"></a><code class="varname">hbase.coprocessor.master.classes</code></dt><dd><p>A comma-separated list of
    org.apache.hadoop.hbase.coprocessor.MasterObserver coprocessors that are
    loaded by default on the active HMaster process. For any implemented
    coprocessor methods, the listed classes will be called in order. After
    implementing your own MasterObserver, just put it in HBase's classpath
    and add the fully qualified class name here.</p><p>Default: <code class="varname"></code></p></dd><dt><a name="hbase.coprocessor.abortonerror"></a><code class="varname">hbase.coprocessor.abortonerror</code></dt><dd><p>Set to true to cause the hosting server (master or regionserver)
      to abort if a coprocessor fails to load, fails to initialize, or throws an
      unexpected Throwable object. Setting this to false will allow the server to
      continue execution but the system wide state of the coprocessor in question
      will become inconsistent as it will be properly executing in only a subset
      of servers, so this is most useful for debugging only.</p><p>Default: <code class="varname">true</code></p></dd><dt><a name="hbase.online.schema.update.enable"></a><code class="varname">hbase.online.schema.update.enable</code></dt><dd><p>Set true to enable online schema changes.</p><p>Default: <code class="varname">true</code></p></dd><dt><a name="hbase.table.lock.enable"></a><code class="varname">hbase.table.lock.enable</code></dt><dd><p>Set to true to enable locking the table in zookeeper for schema change operations.
    Table locking from master prevents concurrent schema modifications to corrupt table
    state.</p><p>Default: <code class="varname">true</code></p></dd><dt><a name="hbase.thrift.minWorkerThreads"></a><code class="varname">hbase.thrift.minWorkerThreads</code></dt><dd><p>The "core size" of the thread pool. New threads are created on every
    connection until this many threads are created.</p><p>Default: <code class="varname">16</code></p></dd><dt><a name="hbase.thrift.maxWorkerThreads"></a><code class="varname">hbase.thrift.maxWorkerThreads</code></dt><dd><p>The maximum size of the thread pool. When the pending request queue
    overflows, new threads are created until their number reaches this number.
    After that, the server starts dropping connections.</p><p>Default: <code class="varname">1000</code></p></dd><dt><a name="hbase.thrift.maxQueuedRequests"></a><code class="varname">hbase.thrift.maxQueuedRequests</code></dt><dd><p>The maximum number of pending Thrift connections waiting in the queue. If
     there are no idle threads in the pool, the server queues requests. Only
     when the queue overflows, new threads are added, up to
     hbase.thrift.maxQueuedRequests threads.</p><p>Default: <code class="varname">1000</code></p></dd><dt><a name="hbase.thrift.htablepool.size.max"></a><code class="varname">hbase.thrift.htablepool.size.max</code></dt><dd><p>The upper bound for the table pool used in the Thrift gateways server.
      Since this is per table name, we assume a single table and so with 1000 default
      worker threads max this is set to a matching number. For other workloads this number
      can be adjusted as needed.
    </p><p>Default: <code class="varname">1000</code></p></dd><dt><a name="hbase.offheapcache.percentage"></a><code class="varname">hbase.offheapcache.percentage</code></dt><dd><p>The amount of off heap space to be allocated towards the experimental
     off heap cache. If you desire the cache to be disabled, simply set this
     value to 0.</p><p>Default: <code class="varname">0</code></p></dd><dt><a name="hbase.data.umask.enable"></a><code class="varname">hbase.data.umask.enable</code></dt><dd><p>Enable, if true, that file permissions should be assigned
      to the files written by the regionserver</p><p>Default: <code class="varname">false</code></p></dd><dt><a name="hbase.data.umask"></a><code class="varname">hbase.data.umask</code></dt><dd><p>File permissions that should be used to write data
      files when hbase.data.umask.enable is true</p><p>Default: <code class="varname">000</code></p></dd><dt><a name="hbase.metrics.showTableName"></a><code class="varname">hbase.metrics.showTableName</code></dt><dd><p>Whether to include the prefix "tbl.tablename" in per-column family metrics.
	If true, for each metric M, per-cf metrics will be reported for tbl.T.cf.CF.M, if false,
	per-cf metrics will be aggregated by column-family across tables, and reported for cf.CF.M.
	In both cases, the aggregated metric M across tables and cfs will be reported.</p><p>Default: <code class="varname">true</code></p></dd><dt><a name="hbase.metrics.exposeOperationTimes"></a><code class="varname">hbase.metrics.exposeOperationTimes</code></dt><dd><p>Whether to report metrics about time taken performing an
      operation on the region server.  Get, Put, Delete, Increment, and Append can all
      have their times exposed through Hadoop metrics per CF and per region.</p><p>Default: <code class="varname">true</code></p></dd><dt><a name="hbase.snapshot.enabled"></a><code class="varname">hbase.snapshot.enabled</code></dt><dd><p>Set to true to allow snapshots to be taken / restored / cloned.</p><p>Default: <code class="varname">true</code></p></dd><dt><a name="hbase.snapshot.restore.take.failsafe.snapshot"></a><code class="varname">hbase.snapshot.restore.take.failsafe.snapshot</code></dt><dd><p>Set to true to take a snapshot before the restore operation.
      The snapshot taken will be used in case of failure, to restore the previous state.
      At the end of the restore operation this snapshot will be deleted</p><p>Default: <code class="varname">true</code></p></dd><dt><a name="hbase.snapshot.restore.failsafe.name"></a><code class="varname">hbase.snapshot.restore.failsafe.name</code></dt><dd><p>Name of the failsafe snapshot taken by the restore operation.
      You can use the {snapshot.name}, {table.name} and {restore.timestamp} variables
      to create a name based on what you are restoring.</p><p>Default: <code class="varname">hbase-failsafe-{snapshot.name}-{restore.timestamp}</code></p></dd><dt><a name="hbase.server.compactchecker.interval.multiplier"></a><code class="varname">hbase.server.compactchecker.interval.multiplier</code></dt><dd><p>The number that determines how often we scan to see if compaction is necessary.
        Normally, compactions are done after some events (such as memstore flush), but if
        region didn't receive a lot of writes for some time, or due to different compaction
        policies, it may be necessary to check it periodically. The interval between checks is
        hbase.server.compactchecker.interval.multiplier multiplied by
        hbase.server.thread.wakefrequency.</p><p>Default: <code class="varname">1000</code></p></dd><dt><a name="hbase.lease.recovery.timeout"></a><code class="varname">hbase.lease.recovery.timeout</code></dt><dd><p>How long we wait on dfs lease recovery in total before giving up.</p><p>Default: <code class="varname">900000</code></p></dd><dt><a name="hbase.lease.recovery.dfs.timeout"></a><code class="varname">hbase.lease.recovery.dfs.timeout</code></dt><dd><p>How long between dfs recover lease invocations. Should be larger than the sum of
        the time it takes for the namenode to issue a block recovery command as part of
        datanode; dfs.heartbeat.interval and the time it takes for the primary
        datanode, performing block recovery to timeout on a dead datanode; usually
        dfs.socket.timeout. See the end of HBASE-8389 for more.</p><p>Default: <code class="varname">64000</code></p></dd><dt><a name="hbase.dfs.client.read.shortcircuit.buffer.size"></a><code class="varname">hbase.dfs.client.read.shortcircuit.buffer.size</code></dt><dd><p>If the DFSClient configuration
    dfs.client.read.shortcircuit.buffer.size is unset, we will
    use what is configured here as the short circuit read default
    direct byte buffer size. DFSClient native default is 1MB; HBase
    keeps its HDFS files open so number of file blocks * 1MB soon
    starts to add up and threaten OOME because of a shortage of
    direct memory.  So, we set it down from the default.  Make
    it &gt; the default hbase block size set in the HColumnDescriptor
    which is usually 64k.
    </p><p>Default: <code class="varname">131072</code></p></dd><dt><a name="hbase.regionserver.checksum.verify"></a><code class="varname">hbase.regionserver.checksum.verify</code></dt><dd><p>
        If set to true, HBase will read data and then verify checksums for
        hfile blocks. Checksum verification inside HDFS will be switched off.
        If the hbase-checksum verification fails, then it will switch back to
        using HDFS checksums.
    </p><p>Default: <code class="varname">true</code></p></dd><dt><a name="hbase.hstore.bytes.per.checksum"></a><code class="varname">hbase.hstore.bytes.per.checksum</code></dt><dd><p>
        Number of bytes in a newly created checksum chunk for HBase-level
        checksums in hfile blocks.
    </p><p>Default: <code class="varname">16384</code></p></dd><dt><a name="hbase.hstore.checksum.algorithm"></a><code class="varname">hbase.hstore.checksum.algorithm</code></dt><dd><p>
      Name of an algorithm that is used to compute checksums. Possible values
      are NULL, CRC32, CRC32C.
    </p><p>Default: <code class="varname">CRC32</code></p></dd><dt><a name="hbase.status.published"></a><code class="varname">hbase.status.published</code></dt><dd><p>
      This setting activates the publication by the master of the status of the region server.
      When a region server dies and its recovery starts, the master will push this information
      to the client application, to let them cut the connection immediately instead of waiting
      for a timeout.
    </p><p>Default: <code class="varname">false</code></p></dd><dt><a name="hbase.status.publisher.class"></a><code class="varname">hbase.status.publisher.class</code></dt><dd><p>
      Implementation of the status publication with a multicast message.
    </p><p>Default: <code class="varname">org.apache.hadoop.hbase.master.ClusterStatusPublisher$MulticastPublisher</code></p></dd><dt><a name="hbase.status.listener.class"></a><code class="varname">hbase.status.listener.class</code></dt><dd><p>
      Implementation of the status listener with a multicast message.
    </p><p>Default: <code class="varname">org.apache.hadoop.hbase.client.ClusterStatusListener$MulticastListener</code></p></dd><dt><a name="hbase.status.multicast.address.ip"></a><code class="varname">hbase.status.multicast.address.ip</code></dt><dd><p>
      Multicast address to use for the status publication by multicast.
    </p><p>Default: <code class="varname">226.1.1.3</code></p></dd><dt><a name="hbase.status.multicast.address.port"></a><code class="varname">hbase.status.multicast.address.port</code></dt><dd><p>
      Multicast port to use for the status publication by multicast.
    </p><p>Default: <code class="varname">60100</code></p></dd><dt><a name="hbase.dynamic.jars.dir"></a><code class="varname">hbase.dynamic.jars.dir</code></dt><dd><p>
      The directory from which the custom filter/co-processor jars can be loaded
      dynamically by the region server without the need to restart. However,
      an already loaded filter/co-processor class would not be un-loaded. See
      HBASE-1936 for more details.
    </p><p>Default: <code class="varname">${hbase.rootdir}/lib</code></p></dd><dt><a name="hbase.security.authentication"></a><code class="varname">hbase.security.authentication</code></dt><dd><p>
      Controls whether or not secure authentication is enabled for HBase.
      Possible values are 'simple' (no authentication), and 'kerberos'.
    </p><p>Default: <code class="varname">simple</code></p></dd><dt><a name="hbase.rest.filter.classes"></a><code class="varname">hbase.rest.filter.classes</code></dt><dd><p>
      Servlet filters for REST service.
    </p><p>Default: <code class="varname">org.apache.hadoop.hbase.rest.filter.GzipFilter</code></p></dd><dt><a name="hbase.rest.filter.classes"></a><code class="varname">hbase.rest.filter.classes</code></dt><dd><p>
      Servlet filters for REST service.
    </p><p>Default: <code class="varname">org.apache.hadoop.hbase.rest.filter.GzipFilter</code></p></dd><dt><a name="hbase.master.loadbalancer.class"></a><code class="varname">hbase.master.loadbalancer.class</code></dt><dd><p>
      Class used to execute the regions balancing when the period occurs.
      See the class comment for more on how it works
      http://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.html
      It replaces the DefaultLoadBalancer as the default (since renamed
      as the SimpleLoadBalancer).
    </p><p>Default: <code class="varname">org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer</code></p></dd><dt><a name="hbase.security.exec.permission.checks"></a><code class="varname">hbase.security.exec.permission.checks</code></dt><dd><p>
      If this setting is enabled and ACL based access control is active (the
      AccessController coprocessor is installed either as a system coprocessor
      or on a table as a table coprocessor) then you must grant all relevant
      users EXEC privilege if they require the ability to execute coprocessor
      endpoint calls. EXEC privilege, like any other permission, can be
      granted globally to a user, or to a user on a per table or per namespace
      basis. For more information on coprocessor endpoints, see the coprocessor
      section of the HBase online manual. For more information on granting or
      revoking permissions using the AccessController, see the security
      section of the HBase online manual.
    </p><p>Default: <code class="varname">false</code></p></dd><dt><a name="hbase.procedure.regionserver.classes"></a><code class="varname">hbase.procedure.regionserver.classes</code></dt><dd><p>A comma-separated list of 
    org.apache.hadoop.hbase.procedure.RegionServerProcedureManager procedure managers that are 
    loaded by default on the active HRegionServer process. The lifecycle methods (init/start/stop) 
    will be called by the active HRegionServer process to perform the specific globally barriered 
    procedure. After implementing your own RegionServerProcedureManager, just put it in 
    HBase's classpath and add the fully qualified class name here.
    </p><p>Default: <code class="varname"></code></p></dd><dt><a name="hbase.procedure.master.classes"></a><code class="varname">hbase.procedure.master.classes</code></dt><dd><p>A comma-separated list of
    org.apache.hadoop.hbase.procedure.MasterProcedureManager procedure managers that are
    loaded by default on the active HMaster process. A procedure is identified by its signature and
    users can use the signature and an instant name to trigger an execution of a globally barriered
    procedure. After implementing your own MasterProcedureManager, just put it in HBase's classpath
    and add the fully qualified class name here.</p><p>Default: <code class="varname"></code></p></dd></dl></div></div></div><div class="section" title="2.3.2.&nbsp;hbase-env.sh"><div class="titlepage"><div><div><h3 class="title"><a name="hbase.env.sh"></a>2.3.2.&nbsp;<code class="filename">hbase-env.sh</code></h3></div></div></div><p>Set HBase environment variables in this file.
      Examples include options to pass the JVM on start of
      an HBase daemon such as heap size and garbarge collector configs.
      You can also set configurations for HBase configuration, log directories,
      niceness, ssh options, where to locate process pid files,
      etc. Open the file at
      <code class="filename">conf/hbase-env.sh</code> and peruse its content.
      Each option is fairly well documented.  Add your own environment
      variables here if you want them read by HBase daemons on startup.</p><p>
      Changes here will require a cluster restart for HBase to notice the change.
      </p></div><div class="section" title="2.3.3.&nbsp;log4j.properties"><div class="titlepage"><div><div><h3 class="title"><a name="log4j"></a>2.3.3.&nbsp;<code class="filename">log4j.properties</code></h3></div></div></div><p>Edit this file to change rate at which HBase files
      are rolled and to change the level at which HBase logs messages.
      </p><p>
      Changes here will require a cluster restart for HBase to notice the change
      though log levels can be changed for particular daemons via the HBase UI.
      </p></div><div class="section" title="2.3.4.&nbsp;Client configuration and dependencies connecting to an HBase cluster"><div class="titlepage"><div><div><h3 class="title"><a name="client_dependencies"></a>2.3.4.&nbsp;Client configuration and dependencies connecting to an HBase cluster</h3></div></div></div><p>If you are running HBase in standalone mode, you don't need to configure anything for your client to work
             provided that they are all on the same machine.</p><p>
          Since the HBase Master may move around, clients bootstrap by looking to ZooKeeper for
          current critical locations.  ZooKeeper is where all these values are kept.  Thus clients
          require the location of the ZooKeeper ensemble information before they can do anything else.
          Usually this the ensemble location is kept out in the <code class="filename">hbase-site.xml</code> and
          is picked up by the client from the <code class="varname">CLASSPATH</code>.</p><p>If you are configuring an IDE to run a HBase client, you should
            include the <code class="filename">conf/</code> directory on your classpath so
            <code class="filename">hbase-site.xml</code> settings can be found (or
            add <code class="filename">src/test/resources</code> to pick up the hbase-site.xml
            used by tests).
      </p><p>
          Minimally, a client of HBase needs several libraries in its <code class="varname">CLASSPATH</code> when connecting to a cluster, including:
          </p><pre class="programlisting">
commons-configuration (commons-configuration-1.6.jar)
commons-lang (commons-lang-2.5.jar)
commons-logging (commons-logging-1.1.1.jar)
hadoop-core (hadoop-core-1.0.0.jar)
hbase (hbase-0.92.0.jar)
log4j (log4j-1.2.16.jar)
slf4j-api (slf4j-api-1.5.8.jar)
slf4j-log4j (slf4j-log4j12-1.5.8.jar)
zookeeper (zookeeper-3.4.2.jar)</pre><p>
      </p><p>
          An example basic <code class="filename">hbase-site.xml</code> for client only
          might look as follows:
          </p><pre class="programlisting">
&lt;?xml version="1.0"?&gt;
&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;
&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;
    &lt;value&gt;example1,example2,example3&lt;/value&gt;
    &lt;description&gt;The directory shared by region servers.
    &lt;/description&gt;
  &lt;/property&gt;
&lt;/configuration&gt;
</pre><p>
        </p><div class="section" title="2.3.4.1.&nbsp;Java client configuration"><div class="titlepage"><div><div><h4 class="title"><a name="java.client.config"></a>2.3.4.1.&nbsp;Java client configuration</h4></div></div></div><p>The configuration used by a Java client is kept
        in an <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HBaseConfiguration" target="_top">HBaseConfiguration</a> instance.
        The factory method on HBaseConfiguration, <code class="code">HBaseConfiguration.create();</code>,
        on invocation, will read in the content of the first <code class="filename">hbase-site.xml</code> found on
        the client's <code class="varname">CLASSPATH</code>, if one is present
        (Invocation will also factor in any <code class="filename">hbase-default.xml</code> found;
        an hbase-default.xml ships inside the <code class="filename">hbase.X.X.X.jar</code>).
        It is also possible to specify configuration directly without having to read from a
        <code class="filename">hbase-site.xml</code>.  For example, to set the ZooKeeper
        ensemble for the cluster programmatically do as follows:
        </p><pre class="programlisting">Configuration config = HBaseConfiguration.create();
config.set("hbase.zookeeper.quorum", "localhost");  // Here we are running zookeeper locally</pre><p>
        If multiple ZooKeeper instances make up your ZooKeeper ensemble,
        they may be specified in a comma-separated list (just as in the <code class="filename">hbase-site.xml</code> file).
        This populated <code class="classname">Configuration</code> instance can then be passed to an
        <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html" target="_top">HTable</a>,
        and so on.
        </p></div></div></div><div class="section" title="2.4.&nbsp;Example Configurations"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="example_config"></a>2.4.&nbsp;Example Configurations</h2></div></div></div><div class="section" title="2.4.1.&nbsp;Basic Distributed HBase Install"><div class="titlepage"><div><div><h3 class="title"><a name="d366e2846"></a>2.4.1.&nbsp;Basic Distributed HBase Install</h3></div></div></div><p>Here is an example basic configuration for a distributed ten
        node cluster. The nodes are named <code class="varname">example0</code>,
        <code class="varname">example1</code>, etc., through node
        <code class="varname">example9</code> in this example. The HBase Master and the
        HDFS namenode are running on the node <code class="varname">example0</code>.
        RegionServers run on nodes
        <code class="varname">example1</code>-<code class="varname">example9</code>. A 3-node
        ZooKeeper ensemble runs on <code class="varname">example1</code>,
        <code class="varname">example2</code>, and <code class="varname">example3</code> on the
        default ports. ZooKeeper data is persisted to the directory
        <code class="filename">/export/zookeeper</code>. Below we show what the main
        configuration files -- <code class="filename">hbase-site.xml</code>,
        <code class="filename">regionservers</code>, and
        <code class="filename">hbase-env.sh</code> -- found in the HBase
        <code class="filename">conf</code> directory might look like.</p><div class="section" title="2.4.1.1.&nbsp;hbase-site.xml"><div class="titlepage"><div><div><h4 class="title"><a name="hbase_site"></a>2.4.1.1.&nbsp;<code class="filename">hbase-site.xml</code></h4></div></div></div><pre class="programlisting">

&lt;?xml version="1.0"?&gt;
&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;
&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;
    &lt;value&gt;example1,example2,example3&lt;/value&gt;
    &lt;description&gt;The directory shared by RegionServers.
    &lt;/description&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;
    &lt;value&gt;/export/zookeeper&lt;/value&gt;
    &lt;description&gt;Property from ZooKeeper's config zoo.cfg.
    The directory where the snapshot is stored.
    &lt;/description&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.rootdir&lt;/name&gt;
    &lt;value&gt;hdfs://example0:8020/hbase&lt;/value&gt;
    &lt;description&gt;The directory shared by RegionServers.
    &lt;/description&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
    &lt;description&gt;The mode the cluster will be in. Possible values are
      false: standalone and pseudo-distributed setups with managed Zookeeper
      true: fully-distributed with unmanaged Zookeeper Quorum (see hbase-env.sh)
    &lt;/description&gt;
  &lt;/property&gt;
&lt;/configuration&gt;

    </pre></div><div class="section" title="2.4.1.2.&nbsp;regionservers"><div class="titlepage"><div><div><h4 class="title"><a name="regionservers"></a>2.4.1.2.&nbsp;<code class="filename">regionservers</code></h4></div></div></div><p>In this file you list the nodes that will run RegionServers.
          In our case, these nodes are <code class="varname">example1</code>-<code class="varname">example9</code>.
          </p><pre class="programlisting">
    example1
    example2
    example3
    example4
    example5
    example6
    example7
    example8
    example9
    </pre></div><div class="section" title="2.4.1.3.&nbsp;hbase-env.sh"><div class="titlepage"><div><div><h4 class="title"><a name="hbase_env"></a>2.4.1.3.&nbsp;<code class="filename">hbase-env.sh</code></h4></div></div></div><p>Below we use a <span class="command"><strong>diff</strong></span> to show the differences
          from default in the <code class="filename">hbase-env.sh</code> file. Here we
          are setting the HBase heap to be 4G instead of the default
          1G.</p><pre class="programlisting">

$ git diff hbase-env.sh
diff --git a/conf/hbase-env.sh b/conf/hbase-env.sh
index e70ebc6..96f8c27 100644
--- a/conf/hbase-env.sh
+++ b/conf/hbase-env.sh
@@ -31,7 +31,7 @@ export JAVA_HOME=/usr/lib//jvm/java-6-sun/
 # export HBASE_CLASSPATH=

 # The maximum amount of heap to use, in MB. Default is 1000.
-# export HBASE_HEAPSIZE=1000
+export HBASE_HEAPSIZE=4096

 # Extra Java runtime options.
 # Below are what we set by default.  May only work with SUN JVM.

    </pre><p>Use <span class="command"><strong>rsync</strong></span> to copy the content of the
          <code class="filename">conf</code> directory to all nodes of the
          cluster.</p></div></div></div><div class="section" title="2.5.&nbsp;The Important Configurations"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="important_configurations"></a>2.5.&nbsp;The Important Configurations</h2></div></div></div><p>Below we list what the <span class="emphasis"><em>important</em></span>
      Configurations.  We've divided this section into
      required configuration and worth-a-look recommended configs.
      </p><div class="section" title="2.5.1.&nbsp;Required Configurations"><div class="titlepage"><div><div><h3 class="title"><a name="required_configuration"></a>2.5.1.&nbsp;Required Configurations</h3></div></div></div><p>Review the <a class="xref" href="#os" title="2.1.2.&nbsp;Operating System">Section&nbsp;2.1.2, &#8220;Operating System&#8221;</a> and <a class="xref" href="#hadoop" title="2.1.3.&nbsp;Hadoop">Section&nbsp;2.1.3, &#8220;Hadoop&#8221;</a> sections.
      </p><div class="section" title="2.5.1.1.&nbsp;Big Cluster Configurations"><div class="titlepage"><div><div><h4 class="title"><a name="big.cluster.config"></a>2.5.1.1.&nbsp;Big Cluster Configurations</h4></div></div></div><p>If a cluster with a lot of regions, it is possible if an eager beaver
            regionserver checks in soon after master start while all the rest in the
            cluster are laggardly, this first server to checkin will be assigned all
            regions.  If lots of regions, this first server could buckle under the
            load.  To prevent the above scenario happening up the
            <code class="varname">hbase.master.wait.on.regionservers.mintostart</code> from its
            default value of 1.  See
            <a class="link" href="https://issues.apache.org/jira/browse/HBASE-6389" target="_top">HBASE-6389 Modify the conditions to ensure that Master waits for sufficient number of Region Servers before starting region assignments</a>
            for more detail.
        </p></div><div class="section" title="2.5.1.2.&nbsp;If a backup Master, making primary Master fail fast"><div class="titlepage"><div><div><h4 class="title"><a name="backup.master.fail.fast"></a>2.5.1.2.&nbsp;If a backup Master, making primary Master fail fast</h4></div></div></div><p>If the primary Master loses its connection with ZooKeeper, it will fall into a loop where it
              keeps trying to reconnect.  Disable this functionality if you are running more than one Master:
              i.e. a backup Master.  Failing to do so, the dying Master may continue to receive RPCs though
              another Master has assumed the role of primary.
              See the configuration <a class="xref" href="#fail.fast.expired.active.master" title="fail.fast.expired.active.master"><code class="varname">fail.fast.expired.active.master</code></a>.

        </p></div></div><div class="section" title="2.5.2.&nbsp;Recommended Configurations"><div class="titlepage"><div><div><h3 class="title"><a name="recommended_configurations"></a>2.5.2.&nbsp;Recommended Configurations</h3></div></div></div><div class="section" title="2.5.2.1.&nbsp;ZooKeeper Configuration"><div class="titlepage"><div><div><h4 class="title"><a name="recommended_configurations.zk"></a>2.5.2.1.&nbsp;ZooKeeper Configuration</h4></div></div></div><div class="section" title="2.5.2.1.1.&nbsp;zookeeper.session.timeout"><div class="titlepage"><div><div><h5 class="title"><a name="zookeeper.session.timeout"></a>2.5.2.1.1.&nbsp;<code class="varname">zookeeper.session.timeout</code></h5></div></div></div><p>The default timeout is three minutes (specified in milliseconds). This means
              that if a server crashes, it will be three minutes before the Master notices
              the crash and starts recovery. You might like to tune the timeout down to
              a minute or even less so the Master notices failures the sooner.
              Before changing this value, be sure you have your JVM garbage collection
              configuration under control otherwise, a long garbage collection that lasts
              beyond the ZooKeeper session timeout will take out
              your RegionServer (You might be fine with this -- you probably want recovery to start
          on the server if a RegionServer has been in GC for a long period of time).</p><p>To change this configuration, edit <code class="filename">hbase-site.xml</code>,
          copy the changed file around the cluster and restart.</p><p>We set this value high to save our having to field noob questions up on the mailing lists asking
              why a RegionServer went down during a massive import.  The usual cause is that their JVM is untuned and
              they are running into long GC pauses.  Our thinking is that
              while users are  getting familiar with HBase, we'd save them having to know all of its
              intricacies.  Later when they've built some confidence, then they can play
              with configuration such as this.
          </p></div><div class="section" title="2.5.2.1.2.&nbsp;Number of ZooKeeper Instances"><div class="titlepage"><div><div><h5 class="title"><a name="zookeeper.instances"></a>2.5.2.1.2.&nbsp;Number of ZooKeeper Instances</h5></div></div></div><p>See <a class="xref" href="#zookeeper" title="Chapter&nbsp;17.&nbsp;ZooKeeper">Chapter&nbsp;17, <i>ZooKeeper</i></a>.
          </p></div></div><div class="section" title="2.5.2.2.&nbsp;HDFS Configurations"><div class="titlepage"><div><div><h4 class="title"><a name="recommended.configurations.hdfs"></a>2.5.2.2.&nbsp;HDFS Configurations</h4></div></div></div><div class="section" title="2.5.2.2.1.&nbsp;dfs.datanode.failed.volumes.tolerated"><div class="titlepage"><div><div><h5 class="title"><a name="dfs.datanode.failed.volumes.tolerated"></a>2.5.2.2.1.&nbsp;dfs.datanode.failed.volumes.tolerated</h5></div></div></div><p>This is the "...number of volumes that are allowed to fail before a datanode stops offering service. By default
                  any volume failure will cause a datanode to shutdown" from the <code class="filename">hdfs-default.xml</code>
                  description.  If you have &gt; three or four disks, you might want to set this to 1 or if you have many disks,
                  two or more.
              </p></div></div><div class="section" title="2.5.2.3.&nbsp;hbase.regionserver.handler.count"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.handler.count"></a>2.5.2.3.&nbsp;<code class="varname">hbase.regionserver.handler.count</code></h4></div></div></div><p>
          This setting defines the number of threads that are kept open to answer
          incoming requests to user tables. The rule of thumb is to keep this
          number low when the payload per request approaches the MB (big puts, scans using
          a large cache) and high when the payload is small (gets, small puts, ICVs, deletes).
          The total size of the queries in progress is limited  by the setting
          "ipc.server.max.callqueue.size".
          </p><p>
          It is safe to set that number to the
          maximum number of incoming clients if their payload is small, the typical example
          being a cluster that serves a website since puts aren't typically buffered
          and most of the operations are gets.
          </p><p>
          The reason why it is dangerous to keep this setting high is that the aggregate
          size of all the puts that are currently happening in a region server may impose
          too much pressure on its memory, or even trigger an OutOfMemoryError. A region server
          running on low memory will trigger its JVM's garbage collector to run more frequently
          up to a point where GC pauses become noticeable (the reason being that all the memory
          used to keep all the requests' payloads cannot be trashed, no matter how hard the
          garbage collector tries). After some time, the overall cluster
          throughput is affected since every request that hits that region server will take longer,
          which exacerbates the problem even more.
          </p><p>You can get a sense of whether you have too little or too many handlers by
            <a class="xref" href="#rpc.logging" title="13.2.2.1.&nbsp;Enabling RPC-level logging">Section&nbsp;13.2.2.1, &#8220;Enabling RPC-level logging&#8221;</a>
            on an individual RegionServer then tailing its logs (Queued requests
            consume memory).
            </p></div><div class="section" title="2.5.2.4.&nbsp;Configuration for large memory machines"><div class="titlepage"><div><div><h4 class="title"><a name="big_memory"></a>2.5.2.4.&nbsp;Configuration for large memory machines</h4></div></div></div><p>
          HBase ships with a reasonable, conservative configuration that will
          work on nearly all
          machine types that people might want to test with. If you have larger
          machines -- HBase has 8G and larger heap -- you might the following configuration options helpful.
          TODO.
        </p></div><div class="section" title="2.5.2.5.&nbsp;Compression"><div class="titlepage"><div><div><h4 class="title"><a name="config.compression"></a>2.5.2.5.&nbsp;Compression</h4></div></div></div><p>You should consider enabling ColumnFamily compression.  There are several options that are near-frictionless and in most all cases boost
      performance by reducing the size of StoreFiles and thus reducing I/O.
      </p><p>See <a class="xref" href="#compression" title="Appendix&nbsp;C.&nbsp;Compression In HBase">Appendix&nbsp;C, <i>Compression In HBase</i></a> for more information.</p></div><div class="section" title="2.5.2.6.&nbsp;Configuring the size and number of WAL files"><div class="titlepage"><div><div><h4 class="title"><a name="config.wals"></a>2.5.2.6.&nbsp;Configuring the size and number of WAL files</h4></div></div></div><p>HBase uses <a class="xref" href="#wal" title="9.6.5.&nbsp;Write Ahead Log (WAL)">Section&nbsp;9.6.5, &#8220;Write Ahead Log (WAL)&#8221;</a> to recover the memstore data that has not been flushed to disk in case of an RS failure. These WAL files should be configured to be slightly smaller than HDFS block (by default, HDFS block is 64Mb and WAL file is ~60Mb).</p><p>HBase also has a limit on number of WAL files, designed to ensure there's never too much data that needs to be replayed during recovery. This limit needs to be set according to memstore configuration, so that all the necessary data would fit. It is recommended to allocated enough WAL files to store at least that much data (when all memstores are close to full).
      For example, with 16Gb RS heap, default memstore settings (0.4), and default WAL file size (~60Mb), 16Gb*0.4/60, the starting point for WAL file count is ~109.
      However, as all memstores are not expected to be full all the time, less WAL files can be allocated.</p></div><div class="section" title="2.5.2.7.&nbsp;Managed Splitting"><div class="titlepage"><div><div><h4 class="title"><a name="disable.splitting"></a>2.5.2.7.&nbsp;Managed Splitting</h4></div></div></div><p>
      Rather than let HBase auto-split your Regions, manage the splitting manually
      <sup>[<a name="d366e3050" href="#ftn.d366e3050" class="footnote">12</a>]</sup>.
 With growing amounts of data, splits will continually be needed. Since
 you always know exactly what regions you have, long-term debugging and
 profiling is much easier with manual splits. It is hard to trace the logs to
 understand region level problems if it keeps splitting and getting renamed.
 Data offlining bugs + unknown number of split regions == oh crap! If an
 <code class="classname">HLog</code> or <code class="classname">StoreFile</code>
 was mistakenly unprocessed by HBase due to a weird bug and
 you notice it a day or so later, you can be assured that the regions
 specified in these files are the same as the current regions and you have
 less headaches trying to restore/replay your data.
 You can finely tune your compaction algorithm. With roughly uniform data
 growth, it's easy to cause split / compaction storms as the regions all
 roughly hit the same data size at the same time. With manual splits, you can
 let staggered, time-based major compactions spread out your network IO load.
      </p><p>
 How do I turn off automatic splitting? Automatic splitting is determined by the configuration value
 <code class="code">hbase.hregion.max.filesize</code>. It is not recommended that you set this
 to <code class="varname">Long.MAX_VALUE</code> in case you forget about manual splits. A suggested setting
 is 100GB, which would result in &gt; 1hr major compactions if reached.
 </p><p>What's the optimal number of pre-split regions to create?
 Mileage will vary depending upon your application.
 You could start low with 10 pre-split regions / server and watch as data grows
 over time. It's better to err on the side of too little regions and rolling split later.
 A more complicated answer is that this depends upon the largest storefile
 in your region. With a growing data size, this will get larger over time. You
 want the largest region to be just big enough that the <code class="classname">Store</code> compact
 selection algorithm only compacts it due to a timed major. If you don't, your
 cluster can be prone to compaction storms as the algorithm decides to run
 major compactions on a large series of regions all at once. Note that
 compaction storms are due to the uniform data growth, not the manual split
 decision.
 </p><p> If you pre-split your regions too thin, you can increase the major compaction
interval by configuring <code class="varname">HConstants.MAJOR_COMPACTION_PERIOD</code>. If your data size
grows too large, use the (post-0.90.0 HBase) <code class="classname">org.apache.hadoop.hbase.util.RegionSplitter</code>
script to perform a network IO safe rolling split
of all regions.
</p></div><div class="section" title="2.5.2.8.&nbsp;Managed Compactions"><div class="titlepage"><div><div><h4 class="title"><a name="managed.compactions"></a>2.5.2.8.&nbsp;Managed Compactions</h4></div></div></div><p>A common administrative technique is to manage major compactions manually, rather than letting
      HBase do it.  By default, <code class="varname">HConstants.MAJOR_COMPACTION_PERIOD</code> is one day and major compactions
      may kick in when you least desire it - especially on a busy system.  To turn off automatic major compactions set
      the value to <code class="varname">0</code>.
      </p><p>It is important to stress that major compactions are absolutely necessary for StoreFile cleanup, the only variant is when
      they occur.  They can be administered through the HBase shell, or via
      <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HBaseAdmin.html#majorCompact%28java.lang.String%29" target="_top">HBaseAdmin</a>.
      </p><p>For more information about compactions and the compaction file selection process, see <a class="xref" href="#compaction" title="9.7.6.5.&nbsp;Compaction">Section&nbsp;9.7.6.5, &#8220;Compaction&#8221;</a></p></div><div class="section" title="2.5.2.9.&nbsp;Speculative Execution"><div class="titlepage"><div><div><h4 class="title"><a name="spec.ex"></a>2.5.2.9.&nbsp;Speculative Execution</h4></div></div></div><p>Speculative Execution of MapReduce tasks is on by default, and for HBase clusters it is generally advised to turn off
        Speculative Execution at a system-level unless you need it for a specific case, where it can be configured per-job.
        Set the properties <code class="varname">mapred.map.tasks.speculative.execution</code> and
        <code class="varname">mapred.reduce.tasks.speculative.execution</code> to false.
        </p></div></div><div class="section" title="2.5.3.&nbsp;Other Configurations"><div class="titlepage"><div><div><h3 class="title"><a name="other_configuration"></a>2.5.3.&nbsp;Other Configurations</h3></div></div></div><div class="section" title="2.5.3.1.&nbsp;Balancer"><div class="titlepage"><div><div><h4 class="title"><a name="balancer_config"></a>2.5.3.1.&nbsp;Balancer</h4></div></div></div><p>The balancer is a periodic operation which is run on the master to redistribute regions on the cluster.  It is configured via
           <code class="varname">hbase.balancer.period</code> and defaults to 300000 (5 minutes). </p><p>See <a class="xref" href="#master.processes.loadbalancer" title="9.5.4.1.&nbsp;LoadBalancer">Section&nbsp;9.5.4.1, &#8220;LoadBalancer&#8221;</a> for more information on the LoadBalancer.
           </p></div><div class="section" title="2.5.3.2.&nbsp;Disabling Blockcache"><div class="titlepage"><div><div><h4 class="title"><a name="disabling.blockcache"></a>2.5.3.2.&nbsp;Disabling Blockcache</h4></div></div></div><p>Do not turn off block cache (You'd do it by setting <code class="varname">hbase.block.cache.size</code> to zero).
           Currently we do not do well if you do this because the regionserver will spend all its time loading hfile
           indices over and over again.  If your working set it such that block cache does you no good, at least
           size the block cache such that hfile indices will stay up in the cache (you can get a rough idea
           on the size you need by surveying regionserver UIs; you'll see index block size accounted near the
           top of the webpage).</p></div><div class="section" title="2.5.3.3.&nbsp;Nagle's or the small package problem"><div class="titlepage"><div><div><h4 class="title"><a name="nagles"></a>2.5.3.3.&nbsp;<a class="link" href="http://en.wikipedia.org/wiki/Nagle's_algorithm" target="_top">Nagle's</a> or the small package problem</h4></div></div></div><p>If a big 40ms or so occasional delay is seen in operations against HBase,
      try the Nagles' setting.  For example, see the user mailing list thread,
      <a class="link" href="http://search-hadoop.com/m/pduLg2fydtE/Inconsistent+scan+performance+with+caching+set+&amp;subj=Re+Inconsistent+scan+performance+with+caching+set+to+1" target="_top">Inconsistent scan performance with caching set to 1</a>
      and the issue cited therein where setting notcpdelay improved scan speeds.  You might also
      see the graphs on the tail of <a class="link" href="https://issues.apache.org/jira/browse/HBASE-7008" target="_top">HBASE-7008 Set scanner caching to a better default</a>
      where our Lars Hofhansl tries various data sizes w/ Nagle's on and off measuring the effect.</p></div><div class="section" title="2.5.3.4.&nbsp;Better Mean Time to Recover (MTTR)"><div class="titlepage"><div><div><h4 class="title"><a name="mttr"></a>2.5.3.4.&nbsp;Better Mean Time to Recover (MTTR)</h4></div></div></div><p>This section is about configurations that will make servers come back faster after a fail.
          See the Deveraj Das an Nicolas Liochon blog post
          <a class="link" href="http://hortonworks.com/blog/introduction-to-hbase-mean-time-to-recover-mttr/" target="_top">Introduction to HBase Mean Time to Recover (MTTR)</a>
          for a brief introduction.</p><p>The issue <a class="link" href="https://issues.apache.org/jira/browse/HBASE-8389" target="_top">HBASE-8354 forces Namenode into loop with lease recovery requests</a>
          is messy but has a bunch of good discussion toward the end on low timeouts and how to effect faster recovery including citation of fixes
          added to HDFS.  Read the Varun Sharma comments.  The below suggested configurations are Varun's suggestions distilled and tested.  Make sure you are
          running on a late-version HDFS so you have the fixes he refers too and himself adds to HDFS that help HBase MTTR
          (e.g. HDFS-3703, HDFS-3712, and HDFS-4791 -- hadoop 2 for sure has them and late hadoop 1 has some).
          Set the following in the RegionServer.
&lt;property&gt;
    &lt;name&gt;hbase.lease.recovery.dfs.timeout&lt;/name&gt;
    &lt;value&gt;23000&lt;/value&gt;
    &lt;description&gt;How much time we allow elapse between calls to recover lease.
    Should be larger than the dfs timeout.&lt;/description&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;dfs.client.socket-timeout&lt;/name&gt;
    &lt;value&gt;10000&lt;/value&gt;
    &lt;description&gt;Down the DFS timeout from 60 to 10 seconds.&lt;/description&gt;
&lt;/property&gt;
And on the namenode/datanode side, set the following to enable 'staleness' introduced in HDFS-3703, HDFS-3912.
&lt;property&gt;
    &lt;name&gt;dfs.client.socket-timeout&lt;/name&gt;
    &lt;value&gt;10000&lt;/value&gt;
    &lt;description&gt;Down the DFS timeout from 60 to 10 seconds.&lt;/description&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;dfs.datanode.socket.write.timeout&lt;/name&gt;
    &lt;value&gt;10000&lt;/value&gt;
    &lt;description&gt;Down the DFS timeout from 8 * 60 to 10 seconds.&lt;/description&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;ipc.client.connect.timeout&lt;/name&gt;
    &lt;value&gt;3000&lt;/value&gt;
    &lt;description&gt;Down from 60 seconds to 3.&lt;/description&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;ipc.client.connect.max.retries.on.timeouts&lt;/name&gt;
    &lt;value&gt;2&lt;/value&gt;
    &lt;description&gt;Down from 45 seconds to 3 (2 == 3 retries).&lt;/description&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;dfs.namenode.avoid.read.stale.datanode&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
    &lt;description&gt;Enable stale state in hdfs&lt;/description&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;dfs.namenode.stale.datanode.interval&lt;/name&gt;
    &lt;value&gt;20000&lt;/value&gt;
    &lt;description&gt;Down from default 30 seconds&lt;/description&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;dfs.namenode.avoid.write.stale.datanode&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
    &lt;description&gt;Enable stale state in hdfs&lt;/description&gt;
&lt;/property&gt;

      </p></div></div></div><div class="footnotes"><br><hr width="100" align="left"><div class="footnote"><p><sup>[<a id="ftn.d366e311" href="#d366e311" class="para">2</a>] </sup>
Be careful editing XML.  Make sure you close all elements.
Run your file through <span class="command"><strong>xmllint</strong></span> or similar
to ensure well-formedness of your document after an edit session.
</p></div><div class="footnote"><p><sup>[<a id="ftn.d366e360" href="#d366e360" class="para">3</a>] </sup>The <a class="link" href="https://github.com/sujee/hadoop-dns-checker" target="_top">hadoop-dns-checker</a> tool can be used to verify
        DNS is working correctly on the cluster.  The project README file provides detailed instructions on usage.
</p></div><div class="footnote"><p><sup>[<a id="ftn.d366e425" href="#d366e425" class="para">4</a>] </sup>See Jack Levin's <a class="link" href="" target="_top">major hdfs issues</a>
                note up on the user list.</p></div><div class="footnote"><p><sup>[<a id="ftn.d366e432" href="#d366e432" class="para">5</a>] </sup>The requirement that a database requires upping of system limits
        is not peculiar to Apache HBase.  See for example the section
        <span class="emphasis"><em>Setting Shell Limits for the Oracle User</em></span> in
        <a class="link" href="http://www.akadia.com/services/ora_linux_install_10g.html" target="_top">
        Short Guide to install Oracle 10 on Linux</a>.</p></div><div class="footnote"><p><sup>[<a id="ftn.d366e444" href="#d366e444" class="para">6</a>] </sup>A useful read setting config on you hadoop cluster is Aaron
            Kimballs' Configuration
            Parameters: What can you just ignore?</p></div><div class="footnote"><p><sup>[<a id="ftn.d366e756" href="#d366e756" class="para">7</a>] </sup>See <a class="link" href="http://search-hadoop.com/m/7vFVx4EsUb2" target="_top">HBase, mail # dev - DISCUSS: Have hbase require at least hadoop 1.0.0 in hbase 0.96.0?</a></p></div><div class="footnote"><p><sup>[<a id="ftn.d366e770" href="#d366e770" class="para">8</a>] </sup>The Cloudera blog post <a class="link" href="http://www.cloudera.com/blog/2012/01/an-update-on-apache-hadoop-1-0/" target="_top">An update on Apache Hadoop 1.0</a>
          by Charles Zedlweski has a nice exposition on how all the Hadoop versions relate.
          Its worth checking out if you are having trouble making sense of the
          Hadoop version morass.
          </p></div><div class="footnote"><p><sup>[<a id="ftn.d366e829" href="#d366e829" class="para">9</a>] </sup>See <a class="link" href="http://ccgtech.blogspot.com/2010/02/hadoop-hdfs-deceived-by-xciever.html" target="_top">Hadoop HDFS: Deceived by Xciever</a> for an informative rant on xceivering.</p></div><div class="footnote"><p><sup>[<a id="ftn.d366e885" href="#d366e885" class="para">10</a>] </sup>The pseudo-distributed vs fully-distributed nomenclature
            comes from Hadoop.</p></div><div class="footnote"><p><sup>[<a id="ftn.d366e968" href="#d366e968" class="para">11</a>] </sup>See <a class="xref" href="#pseudo.extras" title="2.2.2.1.1.&nbsp;Pseudo-distributed Extras">Section&nbsp;2.2.2.1.1, &#8220;Pseudo-distributed Extras&#8221;</a> for notes on how to start extra Masters and
              RegionServers when running pseudo-distributed.</p></div><div class="footnote"><p><sup>[<a id="ftn.d366e3050" href="#d366e3050" class="para">12</a>] </sup>What follows is taken from the javadoc at the head of
      the <code class="classname">org.apache.hadoop.hbase.util.RegionSplitter</code> tool
      added to HBase post-0.90.0 release.
      </p></div></div></div><div class="chapter" title="Chapter&nbsp;3.&nbsp;Upgrading"><div class="titlepage"><div><div><h2 class="title"><a name="upgrading"></a>Chapter&nbsp;3.&nbsp;Upgrading</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#hbase.versioning">3.1. HBase version numbers</a></span></dt><dd><dl><dt><span class="section"><a href="#hbase.development.series">3.1.1. Odd/Even Versioning or "Development"" Series Releases</a></span></dt><dt><span class="section"><a href="#hbase.binary.compatibility">3.1.2. Binary Compatibility</a></span></dt><dt><span class="section"><a href="#hbase.rolling.restart">3.1.3. Rolling Upgrade between versions/Binary compatibility</a></span></dt></dl></dd><dt><span class="section"><a href="#upgrade0.98">3.2. Upgrading from 0.96.x to 0.98.x</a></span></dt><dt><span class="section"><a href="#upgrade0.96">3.3. Upgrading from 0.94.x to 0.96.x</a></span></dt><dd><dl><dt><span class="section"><a href="#d366e3223">3.3.1. Executing the 0.96 Upgrade</a></span></dt></dl></dd><dt><span class="section"><a href="#upgrade0.94">3.4. Upgrading from 0.92.x to 0.94.x</a></span></dt><dt><span class="section"><a href="#upgrade0.92">3.5. Upgrading from 0.90.x to 0.92.x</a></span></dt><dd><dl><dt><span class="section"><a href="#d366e3371">3.5.1. You can&#8217;t go back!
</a></span></dt><dt><span class="section"><a href="#d366e3385">3.5.2. MSLAB is ON by default
</a></span></dt><dt><span class="section"><a href="#d366e3410">3.5.3. Distributed splitting is on by default
</a></span></dt><dt><span class="section"><a href="#d366e3415">3.5.4. Memory accounting is different now
</a></span></dt><dt><span class="section"><a href="#d366e3424">3.5.5. On the Hadoop version to use
</a></span></dt><dt><span class="section"><a href="#d366e3436">3.5.6. HBase 0.92.0 ships with ZooKeeper 3.4.2
</a></span></dt><dt><span class="section"><a href="#d366e3441">3.5.7. Online alter is off by default
</a></span></dt><dt><span class="section"><a href="#d366e3448">3.5.8. WebUI
</a></span></dt><dt><span class="section"><a href="#d366e3453">3.5.9. Security tarball
</a></span></dt><dt><span class="section"><a href="#slabcache">3.5.10. Experimental off-heap cache: SlabCache</a></span></dt><dt><span class="section"><a href="#d366e3480">3.5.11. Changes in HBase replication
</a></span></dt><dt><span class="section"><a href="#d366e3485">3.5.12. RegionServer now aborts if OOME
</a></span></dt><dt><span class="section"><a href="#d366e3490">3.5.13. HFile V2 and the &#8220;Bigger, Fewer&#8221; Tendency
</a></span></dt></dl></dd><dt><span class="section"><a href="#upgrade0.90">3.6. Upgrading to HBase 0.90.x from 0.20.x or 0.89.x</a></span></dt></dl></div><p>You cannot skip major verisons upgrading.  If you are upgrading from
    version 0.90.x to 0.94.x, you must first go from 0.90.x to 0.92.x and then go
    from 0.92.x to 0.94.x.</p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>It may be possible to skip across versions -- for example go from
    0.92.2 straight to 0.98.0 just following the 0.96.x upgrade instructions --
    but we have not tried it so cannot say whether it works or not.</p></div><p>
        Review <a class="xref" href="#configuration" title="Chapter&nbsp;2.&nbsp;Apache HBase Configuration">Chapter&nbsp;2, <i>Apache HBase Configuration</i></a>, in particular the section on Hadoop version.
    </p><div class="section" title="3.1.&nbsp;HBase version numbers"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="hbase.versioning"></a>3.1.&nbsp;HBase version numbers</h2></div></div></div><p>HBase has not walked a straight line where version numbers are concerned.
            Since we came up out of hadoop itself, we originally tracked hadoop versioning.
            Later we left hadoop versioning behind because we were moving at a different rate
            to that of our parent.  If you are into the arcane, checkout our old wiki page
            on <a class="link" href="http://wiki.apache.org/hadoop/Hbase/HBaseVersions" target="_top">HBase Versioning</a>
            which tries to connect the HBase version dots.</p><div class="section" title="3.1.1.&nbsp;Odd/Even Versioning or &#34;Development&#34;&#34; Series Releases"><div class="titlepage"><div><div><h3 class="title"><a name="hbase.development.series"></a>3.1.1.&nbsp;Odd/Even Versioning or "Development"" Series Releases</h3></div></div></div><p>Ahead of big releases, we have been putting up preview versions to start the
                feedback cycle turning-over  earlier.  These "Development" Series releases,
                always odd-numbered, come with no guarantees, not even regards being able
                to upgrade between two sequential releases (we reserve the right to break compatibility across
                "Development" Series releases).  Needless to say, these releases are not for
                production deploys.  They are a preview of what is coming in the hope that
                interested parties will take the release for a test drive and flag us early if we
                there are issues we've missed ahead of our rolling a production-worthy release.
            </p><p>Our first "Development" Series was the 0.89 set that came out ahead of
                HBase 0.90.0.  HBase 0.95 is another "Development" Series that portends
                HBase 0.96.0.
            </p></div><div class="section" title="3.1.2.&nbsp;Binary Compatibility"><div class="titlepage"><div><div><h3 class="title"><a name="hbase.binary.compatibility"></a>3.1.2.&nbsp;Binary Compatibility</h3></div></div></div><p>When we say two HBase versions are compatible, we mean that the versions
                are wire and binary compatible.  Compatible HBase versions means that
                clients can talk to compatible but differently versioned servers.
                It means too that you can just swap out the jars of one version and replace
                them with the jars of another, compatible version and all will just work.
                Unless otherwise specified, HBase point versions are binary compatible.
                You can safely do rolling upgrades between binary compatible versions; i.e.
                across point versions: e.g. from 0.94.5 to 0.94.6<sup>[<a name="d366e3197" href="#ftn.d366e3197" class="footnote">13</a>]</sup>.
            </p></div><div class="section" title="3.1.3.&nbsp;Rolling Upgrade between versions/Binary compatibility"><div class="titlepage"><div><div><h3 class="title"><a name="hbase.rolling.restart"></a>3.1.3.&nbsp;Rolling Upgrade between versions/Binary compatibility</h3></div></div></div><p>Unless otherwise specified, HBase point versions are binary compatible.
                you can do a rolling upgrade between hbase point versions;
                for example, you can go to 0.94.6 from 0.94.5 by doing a rolling upgrade across the cluster
                replacing the 0.94.5 binary with a 0.94.6 binary.
            </p></div></div><div class="section" title="3.2.&nbsp;Upgrading from 0.96.x to 0.98.x"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="upgrade0.98"></a>3.2.&nbsp;Upgrading from 0.96.x to 0.98.x</h2></div></div></div><p>A rolling upgrade from 0.96.x to 0.98.x works.  The two versions are not binary compatible.
      TODO: List of changes.</p></div><div class="section" title="3.3.&nbsp;Upgrading from 0.94.x to 0.96.x"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="upgrade0.96"></a>3.3.&nbsp;Upgrading from 0.94.x to 0.96.x</h2></div><div><h3 class="subtitle">The Singularity</h3></div></div></div><p>You will have to stop your old 0.94.x cluster completely to upgrade.  If you are replicating
     between clusters, both clusters will have to go down to upgrade.  Make sure it is a clean shutdown.
     The less WAL files around, the faster the upgrade will run (the upgrade will split any log files it
     finds in the filesystem as part of the upgrade process).  All clients must be upgraded to 0.96 too.
 </p><p>The API has changed.  You will need to recompile your code against 0.96 and you may need to
     adjust applications to go against new APIs (TODO: List of changes).
 </p><div class="section" title="3.3.1.&nbsp;Executing the 0.96 Upgrade"><div class="titlepage"><div><div><h3 class="title"><a name="d366e3223"></a>3.3.1.&nbsp;Executing the 0.96 Upgrade</h3></div></div></div><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>HDFS and ZooKeeper should be up and running during the upgrade process.</p></div><p>hbase-0.96.0 comes with an upgrade script.  Run
     </p><pre class="programlisting">$ bin/hbase upgrade</pre><p> to see its usage.
     The script has two main modes: -check, and -execute.
 </p><div class="section" title="3.3.1.1.&nbsp;check"><div class="titlepage"><div><div><h4 class="title"><a name="d366e3234"></a>3.3.1.1.&nbsp;check</h4></div></div></div><p>The <span class="emphasis"><em>check</em></span> step is run against a running 0.94 cluster.
             Run it from a downloaded 0.96.x binary.  The <span class="emphasis"><em>check</em></span> step
             is looking for the presence of <code class="filename">HFileV1</code> files.  These are
             unsupported in hbase-0.96.0.  To purge them -- have them rewritten as HFileV2 --
             you must run a compaction.
         </p><p>The <span class="emphasis"><em>check</em></span> step prints stats at the end of its run
             (grep for &#8220;Result:&#8221; in the log) printing absolute path of the tables it scanned,
             any HFileV1 files found, the regions containing said files (the regions we
             need to  major compact to purge the HFileV1s), and any corrupted files if
             any found. A corrupt file is unreadable, and so is undefined (neither HFileV1 nor HFileV2).
         </p><p>To run the check step, run </p><pre class="programlisting">$ bin/hbase upgrade -check</pre><p>.
          Here is sample output:
</p><pre class="programlisting">
             Tables Processed:
             hdfs://localhost:41020/myHBase/.META.
             hdfs://localhost:41020/myHBase/usertable
             hdfs://localhost:41020/myHBase/TestTable
             hdfs://localhost:41020/myHBase/t

             Count of HFileV1: 2
             HFileV1:
             hdfs://localhost:41020/myHBase/usertable    /fa02dac1f38d03577bd0f7e666f12812/family/249450144068442524
             hdfs://localhost:41020/myHBase/usertable    /ecdd3eaee2d2fcf8184ac025555bb2af/family/249450144068442512

             Count of corrupted files: 1
             Corrupted Files:
             hdfs://localhost:41020/myHBase/usertable/fa02dac1f38d03577bd0f7e666f12812/family/1
             Count of Regions with HFileV1: 2
             Regions to Major Compact:
             hdfs://localhost:41020/myHBase/usertable/fa02dac1f38d03577bd0f7e666f12812
             hdfs://localhost:41020/myHBase/usertable/ecdd3eaee2d2fcf8184ac025555bb2af

             There are some HFileV1, or corrupt files (files with incorrect major version)
</pre><p>
             In the above sample output, there are two HFileV1 in two regions, and one corrupt file.
             Corrupt files should probably be removed.  The regions that have HFileV1s need to be major
             compacted.  To major compact, start up the hbase shell and review how to compact an individual
             region.  After the major compaction is done, rerun the check step and the HFileV1s shoudl be
             gone, replaced by HFileV2 instances.
         </p><p>By default, the check step scans the hbase root directory (defined as hbase.rootdir in the configuration).
             To scan a specific directory only, pass the <span class="emphasis"><em>-dir</em></span> option.
             </p><pre class="programlisting">$ bin/hbase upgrade -check -dir /myHBase/testTable</pre><p>
             The above command would detect HFileV1s in the /myHBase/testTable directory.
         </p><p>
             Once the check step reports all the HFileV1 files have been rewritten, it is safe to proceed with the
             upgrade.
          </p></div><div class="section" title="3.3.1.2.&nbsp;execute"><div class="titlepage"><div><div><h4 class="title"><a name="d366e3271"></a>3.3.1.2.&nbsp;execute</h4></div></div></div><p>After the check step shows the cluster is free of HFileV1, it is safe to proceed with the upgrade.
             Next is the <span class="emphasis"><em>execute</em></span> step.  You must <span class="emphasis"><em>SHUTDOWN YOUR 0.94.x CLUSTER</em></span>
             before you can run the <span class="emphasis"><em>execute</em></span> step.  The execute step will not run if it
             detects running HBase masters or regionservers.
         </p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>HDFS and ZooKeeper should be up and running during the upgrade process.
             If zookeeper is managed by HBase, then you can start zookeeper so it is available to the upgrade
             by running </p><pre class="programlisting">$ ./hbase/bin/hbase-daemon.sh  start zookeeper</pre><p>
         </p></div><p>
         </p><p>
             The <span class="emphasis"><em>execute</em></span> upgrade step is made of three substeps.

             </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p>Namespaces: HBase 0.96.0 has support for namespaces.  The upgrade needs to reorder directories in the filesystem for namespaces to work.</p></li><li class="listitem"><p>ZNodes: All znodes are purged so that new ones can be written in their place using a new protobuf'ed format and a few are migrated in place: e.g. replication and table state znodes</p></li><li class="listitem"><p>WAL Log Splitting: If the 0.94.x cluster shutdown was not clean, we'll split WAL logs as part of migration before
                     we startup on 0.96.0.  This WAL splitting runs slower than the native distributed WAL splitting because it is all inside the
                     single upgrade process (so try and get a clean shutdown of the 0.94.0 cluster  if you can).
             </p></li></ul></div><p>
         </p><p>
             To run the <span class="emphasis"><em>execute</em></span> step, make sure that first you have copied hbase-0.96.0
             binaries everywhere under servers and under clients.  Make sure the 0.94.0 cluster is down.
             Then do as follows:
         </p><pre class="programlisting">$ bin/hbase upgrade -execute</pre><p>
         Here is some sample output
         </p><pre class="programlisting">
             Starting Namespace upgrade
             Created version file at hdfs://localhost:41020/myHBase with version=7
             Migrating table testTable to hdfs://localhost:41020/myHBase/.data/default/testTable
             &#8230;..
             Created version file at hdfs://localhost:41020/myHBase with version=8
             Successfully completed NameSpace upgrade.
             Starting Znode upgrade
             &#8230;.
             Successfully completed Znode upgrade

             Starting Log splitting
             &#8230;
             Successfully completed Log splitting
         </pre><p>
         </p><p>
             If the output from the execute step looks good, stop the zookeeper instance you started
             to do the upgrade: </p><pre class="programlisting">$ ./hbase/bin/hbase-daemon.sh stop zookeeper</pre><p>
             Now start up hbase-0.96.0.
         </p></div><div class="section" title="3.3.1.3.&nbsp;Troubleshooting"><div class="titlepage"><div><div><h4 class="title"><a name="096.migration.troubleshooting"></a>3.3.1.3.&nbsp;Troubleshooting</h4></div></div></div><div class="section" title="3.3.1.3.1.&nbsp;Old Client connecting to 0.96 cluster"><div class="titlepage"><div><div><h5 class="title"><a name="096.migration.troubleshooting.old.client"></a>3.3.1.3.1.&nbsp;Old Client connecting to 0.96 cluster</h5></div></div></div><p>It will fail with an exception like the below.  Upgrade.
             </p><pre class="programlisting">17:22:15  Exception in thread "main" java.lang.IllegalArgumentException: Not a host:port pair: PBUF
17:22:15  *
17:22:15   api-compat-8.ent.cloudera.com &#65533;&#65533;  &#65533;&#65533;&#65533;(
17:22:15    at org.apache.hadoop.hbase.util.Addressing.parseHostname(Addressing.java:60)
17:22:15    at org.apache.hadoop.hbase.ServerName.&amp;init&gt;(ServerName.java:101)
17:22:15    at org.apache.hadoop.hbase.ServerName.parseVersionedServerName(ServerName.java:283)
17:22:15    at org.apache.hadoop.hbase.MasterAddressTracker.bytesToServerName(MasterAddressTracker.java:77)
17:22:15    at org.apache.hadoop.hbase.MasterAddressTracker.getMasterAddress(MasterAddressTracker.java:61)
17:22:15    at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getMaster(HConnectionManager.java:703)
17:22:15    at org.apache.hadoop.hbase.client.HBaseAdmin.&amp;init&gt;(HBaseAdmin.java:126)
17:22:15    at Client_4_3_0.setup(Client_4_3_0.java:716)
17:22:15    at Client_4_3_0.main(Client_4_3_0.java:63)</pre><p>
         </p></div></div></div></div><div class="section" title="3.4.&nbsp;Upgrading from 0.92.x to 0.94.x"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="upgrade0.94"></a>3.4.&nbsp;Upgrading from 0.92.x to 0.94.x</h2></div></div></div><p>We used to think that 0.92 and 0.94 were interface compatible and that you can do a
          rolling upgrade between these versions but then we figured that
          <a class="link" href="https://issues.apache.org/jira/browse/HBASE-5357" target="_top">HBASE-5357 Use builder pattern in HColumnDescriptor</a>
          changed method signatures so rather than return void they instead return HColumnDescriptor.  This
          will throw </p><pre class="programlisting">java.lang.NoSuchMethodError: org.apache.hadoop.hbase.HColumnDescriptor.setMaxVersions(I)V</pre><p>
          .... so 0.92 and 0.94 are NOT compatible.  You cannot do a rolling upgrade between them.
    </p></div><div class="section" title="3.5.&nbsp;Upgrading from 0.90.x to 0.92.x"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="upgrade0.92"></a>3.5.&nbsp;Upgrading from 0.90.x to 0.92.x</h2></div><div><h3 class="subtitle">Upgrade Guide</h3></div></div></div><p>You will find that 0.92.0 runs a little differently to 0.90.x releases.  Here are a few things to watch out for upgrading from 0.90.x to 0.92.0.
</p><div class="note" title="tl;dr" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">tl;dr</h3><p>
If you've not patience, here are the important things to know upgrading.
</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">Once you upgrade, you can&#8217;t go back.
</li><li class="listitem">
MSLAB is on by default. Watch that heap usage if you have a lot of regions.
</li><li class="listitem">
Distributed splitting is on by defaul.  It should make region server failover faster.
</li><li class="listitem">
There&#8217;s a separate tarball for security.
</li><li class="listitem">
If -XX:MaxDirectMemorySize is set in your hbase-env.sh, it&#8217;s going to enable the experimental off-heap cache (You may not want this).
</li></ol></div><p>
</p></div><p>
</p><div class="section" title="3.5.1.&nbsp;You can&#8217;t go back!"><div class="titlepage"><div><div><h3 class="title"><a name="d366e3371"></a>3.5.1.&nbsp;You can&#8217;t go back!
</h3></div></div></div><p>To move to 0.92.0, all you need to do is shutdown your cluster, replace your hbase 0.90.x with hbase 0.92.0 binaries (be sure you clear out all 0.90.x instances) and restart (You cannot do a rolling restart from 0.90.x to 0.92.x -- you must restart).
On startup, the <code class="varname">.META.</code> table content is rewritten removing the table schema from the <code class="varname">info:regioninfo</code> column.
Also, any flushes done post first startup will write out data in the new 0.92.0 file format, <a class="link" href="http://hbase.apache.org/book.html#hfilev2" target="_top">HFile V2</a>.
This means you cannot go back to 0.90.x once you&#8217;ve started HBase 0.92.0 over your HBase data directory.
</p></div><div class="section" title="3.5.2.&nbsp;MSLAB is ON by default"><div class="titlepage"><div><div><h3 class="title"><a name="d366e3385"></a>3.5.2.&nbsp;MSLAB is ON by default
</h3></div></div></div><p>In 0.92.0, the <a class="link" href="http://hbase.apache.org/book.html#hbase.hregion.memstore.mslab.enabled" target="_top">hbase.hregion.memstore.mslab.enabled</a> flag is set to true
(See <a class="xref" href="#mslab">Section&nbsp;12.3.1.1, &#8220;Long GC pauses&#8221;</a>).  In 0.90.x it was <code class="constant">false</code>.  When it is enabled, memstores will step allocate memory in MSLAB 2MB chunks even if the
memstore has zero or just a few small elements.  This is fine usually but if you had lots of regions per regionserver in a 0.90.x cluster (and MSLAB was off),
you may find yourself OOME'ing on upgrade because the <span class="mathphrase">thousands of regions * number of column families * 2MB MSLAB (at a minimum)</span>
puts your heap over the top.  Set <code class="varname">hbase.hregion.memstore.mslab.enabled</code> to
<code class="constant">false</code> or set the MSLAB size down from 2MB by setting <code class="varname">hbase.hregion.memstore.mslab.chunksize</code> to something less.
</p></div><div class="section" title="3.5.3.&nbsp;Distributed splitting is on by default"><div class="titlepage"><div><div><h3 class="title"><a name="d366e3410"></a>3.5.3.&nbsp;Distributed splitting is on by default
</h3></div></div></div><p>Previous, WAL logs on crash were split by the Master alone.  In 0.92.0, log splitting is done by the cluster (See See &#8220;HBASE-1364 [performance] Distributed splitting of regionserver commit logs&#8221;).  This should cut down significantly on the amount of time it takes splitting logs and getting regions back online again.
</p></div><div class="section" title="3.5.4.&nbsp;Memory accounting is different now"><div class="titlepage"><div><div><h3 class="title"><a name="d366e3415"></a>3.5.4.&nbsp;Memory accounting is different now
</h3></div></div></div><p>In 0.92.0, <a class="xref" href="#hfilev2" title="Appendix&nbsp;E.&nbsp;HFile format version 2">Appendix&nbsp;E, <i>HFile format version 2</i></a> indices and bloom filters take up residence in the same LRU used caching blocks that come from the filesystem.
In 0.90.x, the HFile v1 indices lived outside of the LRU so they took up space even if the index was on a &#8216;cold&#8217; file, one that wasn&#8217;t being actively used.  With the indices now in the LRU, you may find you
have less space for block caching.  Adjust your block cache accordingly.  See the <a class="xref" href="#block.cache" title="9.6.4.&nbsp;Block Cache">Section&nbsp;9.6.4, &#8220;Block Cache&#8221;</a> for more detail.
The block size default size has been changed in 0.92.0 from 0.2 (20 percent of heap) to 0.25.
</p></div><div class="section" title="3.5.5.&nbsp;On the Hadoop version to use"><div class="titlepage"><div><div><h3 class="title"><a name="d366e3424"></a>3.5.5.&nbsp;On the Hadoop version to use
</h3></div></div></div><p>Run 0.92.0 on Hadoop 1.0.x (or CDH3u3 when it ships).  The performance benefits are worth making the move.  Otherwise, our Hadoop prescription is as it has been; you need an Hadoop that supports a working sync.  See <a class="xref" href="#hadoop" title="2.1.3.&nbsp;Hadoop">Section&nbsp;2.1.3, &#8220;Hadoop&#8221;</a>.
</p><p>If running on Hadoop 1.0.x (or CDH3u3), enable local read.  See <a class="link" href="http://files.meetup.com/1350427/hug_ebay_jdcryans.pdf" target="_top">Practical Caching</a> presentation for ruminations on the performance benefits &#8216;going local&#8217; (and for how to enable local reads).
</p></div><div class="section" title="3.5.6.&nbsp;HBase 0.92.0 ships with ZooKeeper 3.4.2"><div class="titlepage"><div><div><h3 class="title"><a name="d366e3436"></a>3.5.6.&nbsp;HBase 0.92.0 ships with ZooKeeper 3.4.2
</h3></div></div></div><p>If you can, upgrade your zookeeper.  If you can&#8217;t, 3.4.2 clients should work against 3.3.X ensembles (HBase makes use of 3.4.2 API).
</p></div><div class="section" title="3.5.7.&nbsp;Online alter is off by default"><div class="titlepage"><div><div><h3 class="title"><a name="d366e3441"></a>3.5.7.&nbsp;Online alter is off by default
</h3></div></div></div><p>In 0.92.0, we&#8217;ve added an experimental online schema alter facility  (See <a class="xref" href="#hbase.online.schema.update.enable" title="hbase.online.schema.update.enable"><code class="varname">hbase.online.schema.update.enable</code></a>).  Its off by default.  Enable it at your own risk.  Online alter and splitting tables do not play well together so be sure your cluster quiescent using this feature (for now).
</p></div><div class="section" title="3.5.8.&nbsp;WebUI"><div class="titlepage"><div><div><h3 class="title"><a name="d366e3448"></a>3.5.8.&nbsp;WebUI
</h3></div></div></div><p>The webui has had a few additions made in 0.92.0.  It now shows a list of the regions currently transitioning, recent compactions/flushes, and a process list of running processes (usually empty if all is well and requests are being handled promptly).  Other additions including requests by region, a debugging servlet dump, etc.
</p></div><div class="section" title="3.5.9.&nbsp;Security tarball"><div class="titlepage"><div><div><h3 class="title"><a name="d366e3453"></a>3.5.9.&nbsp;Security tarball
</h3></div></div></div><p>We now ship with two tarballs; secure and insecure HBase.  Documentation on how to setup a secure HBase is on the way.
</p></div><div class="section" title="3.5.10.&nbsp;Experimental off-heap cache: SlabCache"><div class="titlepage"><div><div><h3 class="title"><a name="slabcache"></a>3.5.10.&nbsp;Experimental off-heap cache: SlabCache</h3></div></div></div><p>
A new cache was contributed to 0.92.0 to act as a solution between using the &#8220;on-heap&#8221; cache which is the current LRU cache the region servers have and the operating system cache which is out of our control.
To enable <span class="emphasis"><em>SlabCache</em></span>, as this feature is being called, set &#8220;-XX:MaxDirectMemorySize&#8221; in hbase-env.sh to the value for maximum direct memory size and specify
<span class="property">hbase.offheapcache.percentage</span> in <code class="filename">hbase-site.xml</code> with the percentage that you want to dedicate to off-heap cache. This should only be set for servers and not for clients. Use at your own risk.
See this blog post, <a class="link" href="http://www.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/" target="_top">Caching in Apache HBase: SlabCache</a>, for additional information on this new experimental feature.
</p><p>This feature has mostly been eclipsed in later HBases.  See <a class="link" href="https://issues.apache.org/jira/browse/HBASE-7404 " target="_top">HBASE-7404 Bucket Cache:A solution about CMS,Heap Fragment and Big Cache on HBASE</a>, etc.</p></div><div class="section" title="3.5.11.&nbsp;Changes in HBase replication"><div class="titlepage"><div><div><h3 class="title"><a name="d366e3480"></a>3.5.11.&nbsp;Changes in HBase replication
</h3></div></div></div><p>0.92.0 adds two new features: multi-slave and multi-master replication. The way to enable this is the same as adding a new peer, so in order to have multi-master you would just run add_peer for each cluster that acts as a master to the other slave clusters. Collisions are handled at the timestamp level which may or may not be what you want, this needs to be evaluated on a per use case basis. Replication is still experimental in 0.92 and is disabled by default, run it at your own risk.
</p></div><div class="section" title="3.5.12.&nbsp;RegionServer now aborts if OOME"><div class="titlepage"><div><div><h3 class="title"><a name="d366e3485"></a>3.5.12.&nbsp;RegionServer now aborts if OOME
</h3></div></div></div><p>If an OOME, we now have the JVM kill -9 the regionserver process so it goes down fast.  Previous, a RegionServer might stick around after incurring an OOME limping along in some wounded state.  To disable this facility, and recommend you leave it in place, you&#8217;d need to edit the bin/hbase file.  Look for the addition of the -XX:OnOutOfMemoryError="kill -9 %p" arguments (See [HBASE-4769] - &#8216;Abort RegionServer Immediately on OOME&#8217;)
</p></div><div class="section" title="3.5.13.&nbsp;HFile V2 and the &#8220;Bigger, Fewer&#8221; Tendency"><div class="titlepage"><div><div><h3 class="title"><a name="d366e3490"></a>3.5.13.&nbsp;HFile V2 and the &#8220;Bigger, Fewer&#8221; Tendency
</h3></div></div></div><p>0.92.0 stores data in a new format, <a class="xref" href="#hfilev2" title="Appendix&nbsp;E.&nbsp;HFile format version 2">Appendix&nbsp;E, <i>HFile format version 2</i></a>.   As HBase runs, it will move all your data from HFile v1 to HFile v2 format.  This auto-migration will run in the background as flushes and compactions run.
HFile V2 allows HBase run with larger regions/files.  In fact, we encourage that all HBasers going forward tend toward Facebook axiom #1, run with larger, fewer regions.
If you have lots of regions now -- more than 100s per host -- you should look into setting your region size up after you move to 0.92.0 (In 0.92.0, default size is now 1G, up from 256M), and then running online merge tool (See &#8220;HBASE-1621 merge tool should work on online cluster, but disabled table&#8221;).
</p></div></div><div class="section" title="3.6.&nbsp;Upgrading to HBase 0.90.x from 0.20.x or 0.89.x"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="upgrade0.90"></a>3.6.&nbsp;Upgrading to HBase 0.90.x from 0.20.x or 0.89.x</h2></div></div></div><p>This version of 0.90.x HBase can be started on data written by
              HBase 0.20.x or HBase 0.89.x.  There is no need of a migration step.
              HBase 0.89.x and 0.90.x does write out the name of region directories
              differently -- it names them with a md5 hash of the region name rather
              than a jenkins hash -- so this means that once started, there is no
              going back to HBase 0.20.x.
          </p><p>
             Be sure to remove the <code class="filename">hbase-default.xml</code> from
             your <code class="filename">conf</code>
             directory on upgrade.  A 0.20.x version of this file will have
             sub-optimal configurations for 0.90.x HBase.  The
             <code class="filename">hbase-default.xml</code> file is now bundled into the
             HBase jar and read from there.  If you would like to review
             the content of this file, see it in the src tree at
             <code class="filename">src/main/resources/hbase-default.xml</code> or
             see <a class="xref" href="#hbase_default_configurations" title="2.3.1.1.&nbsp;HBase Default Configuration">Section&nbsp;2.3.1.1, &#8220;HBase Default Configuration&#8221;</a>.
          </p><p>
            Finally, if upgrading from 0.20.x, check your
            <code class="varname">.META.</code> schema in the shell.  In the past we would
            recommend that users run with a 16kb
            <code class="varname">MEMSTORE_FLUSHSIZE</code>.
            Run <code class="code">hbase&gt; scan '-ROOT-'</code> in the shell. This will output
            the current <code class="varname">.META.</code> schema.  Check
            <code class="varname">MEMSTORE_FLUSHSIZE</code> size.  Is it 16kb (16384)?  If so, you will
            need to change this (The 'normal'/default value is 64MB (67108864)).
            Run the script <code class="filename">bin/set_meta_memstore_size.rb</code>.
            This will make the necessary edit to your <code class="varname">.META.</code> schema.
            Failure to run this change will make for a slow cluster <sup>[<a name="d366e3541" href="#ftn.d366e3541" class="footnote">14</a>]</sup>
            .

          </p></div><div class="footnotes"><br><hr width="100" align="left"><div class="footnote"><p><sup>[<a id="ftn.d366e3197" href="#d366e3197" class="para">13</a>] </sup>See
                        <a class="link" href="http://search-hadoop.com/m/bOOvwHGW981/Does+compatibility+between+versions+also+mean+binary+compatibility%253F&amp;subj=Re+Does+compatibility+between+versions+also+mean+binary+compatibility+" target="_top">Does compatibility between versions also mean binary compatibility?</a>
                        discussion on the hbaes dev mailing list.
                </p></div><div class="footnote"><p><sup>[<a id="ftn.d366e3541" href="#d366e3541" class="para">14</a>] </sup>
            See <a class="link" href="https://issues.apache.org/jira/browse/HBASE-3499" target="_top">HBASE-3499 Users upgrading to 0.90.0 need to have their .META. table updated with the right MEMSTORE_SIZE</a>
            </p></div></div></div><div class="chapter" title="Chapter&nbsp;4.&nbsp;The Apache HBase Shell"><div class="titlepage"><div><div><h2 class="title"><a name="shell"></a>Chapter&nbsp;4.&nbsp;The Apache HBase Shell</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#scripting">4.1. Scripting</a></span></dt><dt><span class="section"><a href="#shell_tricks">4.2. Shell Tricks</a></span></dt><dd><dl><dt><span class="section"><a href="#table_variables">4.2.1. Table variables</a></span></dt><dt><span class="section"><a href="#d366e3621">4.2.2. <code class="filename">irbrc</code></a></span></dt><dt><span class="section"><a href="#d366e3639">4.2.3. LOG data to timestamp</a></span></dt><dt><span class="section"><a href="#d366e3657">4.2.4. Debug</a></span></dt><dt><span class="section"><a href="#d366e3679">4.2.5. Commands</a></span></dt></dl></dd></dl></div><p>
        The Apache HBase Shell is <a class="link" href="http://jruby.org" target="_top">(J)Ruby</a>'s
        IRB with some HBase particular commands added.  Anything you can do in
        IRB, you should be able to do in the HBase Shell.</p><p>To run the HBase shell,
        do as follows:
        </p><pre class="programlisting">$ ./bin/hbase shell</pre><p>
        </p><p>Type <span class="command"><strong>help</strong></span> and then <span class="command"><strong>&lt;RETURN&gt;</strong></span>
            to see a listing of shell
            commands and options. Browse at least the paragraphs at the end of
            the help emission for the gist of how variables and command
            arguments are entered into the
            HBase shell; in particular note how table names, rows, and
            columns, etc., must be quoted.</p><p>See <a class="xref" href="#shell_exercises" title="1.2.3.&nbsp;Shell Exercises">Section&nbsp;1.2.3, &#8220;Shell Exercises&#8221;</a>
            for example basic shell operation.
        </p><p>Here is a nicely formatted listing of <a class="link" href="http://learnhbase.wordpress.com/2013/03/02/hbase-shell-commands/" target="_top">all shell commands</a> by Rajeshbabu Chintaguntla.
        </p><div class="section" title="4.1.&nbsp;Scripting"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="scripting"></a>4.1.&nbsp;Scripting</h2></div></div></div><p>For examples scripting Apache HBase, look in the
            HBase <code class="filename">bin</code> directory.  Look at the files
            that end in <code class="filename">*.rb</code>.  To run one of these
            files, do as follows:
            </p><pre class="programlisting">$ ./bin/hbase org.jruby.Main PATH_TO_SCRIPT</pre><p>
        </p></div><div class="section" title="4.2.&nbsp;Shell Tricks"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="shell_tricks"></a>4.2.&nbsp;Shell Tricks</h2></div></div></div><div class="section" title="4.2.1.&nbsp;Table variables"><div class="titlepage"><div><div><h3 class="title"><a name="table_variables"></a>4.2.1.&nbsp;Table variables</h3></div></div></div><p>
	  HBase 0.95 adds shell commands that provide a jruby-style
	  object-oriented references for tables.  Previously all of
	  the shell commands that act upon a table have a procedural
	  style that always took the name of the table as an
	  argument. HBase 0.95 introduces the ability to assign a
	  table to a jruby variable.  The table reference can be used
	  to perform data read write operations such as puts, scans,
	  and gets well as admin functionality such as disabling,
	  dropping, describing tables.
	</p><p>
	  For example, previously you would always specify a table name:
	  </p><pre class="programlisting">
hbase(main):000:0&gt; create &#8216;t&#8217;, &#8216;f&#8217;
0 row(s) in 1.0970 seconds
hbase(main):001:0&gt; put 't', 'rold', 'f', 'v'
0 row(s) in 0.0080 seconds

hbase(main):002:0&gt; scan 't' 
ROW                                COLUMN+CELL                                                                                      
 rold                              column=f:, timestamp=1378473207660, value=v                                                      
1 row(s) in 0.0130 seconds

hbase(main):003:0&gt; describe 't'
DESCRIPTION                                                                           ENABLED                                       
 't', {NAME =&gt; 'f', DATA_BLOCK_ENCODING =&gt; 'NONE', BLOOMFILTER =&gt; 'ROW', REPLICATION_ true                                          
 SCOPE =&gt; '0', VERSIONS =&gt; '1', COMPRESSION =&gt; 'NONE', MIN_VERSIONS =&gt; '0', TTL =&gt; '2                                               
 147483647', KEEP_DELETED_CELLS =&gt; 'false', BLOCKSIZE =&gt; '65536', IN_MEMORY =&gt; 'false                                               
 ', BLOCKCACHE =&gt; 'true'}                                                                                 
1 row(s) in 1.4430 seconds

hbase(main):004:0&gt; disable 't'
0 row(s) in 14.8700 seconds

hbase(main):005:0&gt; drop 't'
0 row(s) in 23.1670 seconds

hbase(main):006:0&gt; 
	  </pre><p>
	</p><p>
	  Now you can assign the table to a variable and use the results in jruby shell code.
	  </p><pre class="programlisting">
hbase(main):007 &gt; t = create 't', 'f'
0 row(s) in 1.0970 seconds

=&gt; Hbase::Table - t
hbase(main):008 &gt; t.put 'r', 'f', 'v'
0 row(s) in 0.0640 seconds
hbase(main):009 &gt; t.scan
ROW                           COLUMN+CELL                                                                        
 r                            column=f:, timestamp=1331865816290, value=v                                        
1 row(s) in 0.0110 seconds
hbase(main):010:0&gt; t.describe
DESCRIPTION                                                                           ENABLED                                       
 't', {NAME =&gt; 'f', DATA_BLOCK_ENCODING =&gt; 'NONE', BLOOMFILTER =&gt; 'ROW', REPLICATION_ true                                          
 SCOPE =&gt; '0', VERSIONS =&gt; '1', COMPRESSION =&gt; 'NONE', MIN_VERSIONS =&gt; '0', TTL =&gt; '2                                               
 147483647', KEEP_DELETED_CELLS =&gt; 'false', BLOCKSIZE =&gt; '65536', IN_MEMORY =&gt; 'false                                               
 ', BLOCKCACHE =&gt; 'true'}                                                                                 
1 row(s) in 0.0210 seconds
hbase(main):038:0&gt; t.disable
0 row(s) in 6.2350 seconds
hbase(main):039:0&gt; t.drop
0 row(s) in 0.2340 seconds
	  </pre><p>
	</p><p>
	  If the table has already been created, you can assign a
	  Table to a variable by using the get_table method:
	  </p><pre class="programlisting">
hbase(main):011 &gt; create 't','f'
0 row(s) in 1.2500 seconds

=&gt; Hbase::Table - t
hbase(main):012:0&gt; tab = get_table 't'
0 row(s) in 0.0010 seconds

=&gt; Hbase::Table - t
hbase(main):013:0&gt; tab.put &#8216;r1&#8217; ,&#8217;f&#8217;, &#8216;v&#8217; 
0 row(s) in 0.0100 seconds
hbase(main):014:0&gt; tab.scan
ROW                                COLUMN+CELL                                                                                      
 r1                                column=f:, timestamp=1378473876949, value=v                                                      
1 row(s) in 0.0240 seconds
hbase(main):015:0&gt; 
	  </pre><p>
	</p><p>
	  The list functionality has also been extended so that it
	  returns a list of table names as strings.  You can then use
	  jruby to script table operations based on these names.  The
	  list_snapshots command also acts similarly.
	  </p><pre class="programlisting">
hbase(main):016 &gt; tables = list(&#8216;t.*&#8217;)
TABLE                                                                                                                               
t                                                                                                                                   
1 row(s) in 0.1040 seconds

=&gt; #&lt;#&lt;Class:0x7677ce29&gt;:0x21d377a4&gt;
hbase(main):017:0&gt; tables.map { |t| disable t ; drop  t}
0 row(s) in 2.2510 seconds

=&gt; [nil]
hbase(main):018:0&gt; 
            </pre><p>
          </p></div><div class="section" title="4.2.2.&nbsp;irbrc"><div class="titlepage"><div><div><h3 class="title"><a name="d366e3621"></a>4.2.2.&nbsp;<code class="filename">irbrc</code></h3></div></div></div><p>Create an <code class="filename">.irbrc</code> file for yourself in your
                    home directory. Add customizations. A useful one is
                    command history so commands are save across Shell invocations:
                    </p><pre class="programlisting">
                        $ more .irbrc
                        require 'irb/ext/save-history'
                        IRB.conf[:SAVE_HISTORY] = 100
                        IRB.conf[:HISTORY_FILE] = "#{ENV['HOME']}/.irb-save-history"</pre><p>
                See the <span class="application">ruby</span> documentation of
                <code class="filename">.irbrc</code> to learn about other possible
                confiurations.
                </p></div><div class="section" title="4.2.3.&nbsp;LOG data to timestamp"><div class="titlepage"><div><div><h3 class="title"><a name="d366e3639"></a>4.2.3.&nbsp;LOG data to timestamp</h3></div></div></div><p>
                To convert the date '08/08/16 20:56:29' from an hbase log into a timestamp, do:
                </p><pre class="programlisting">
                    hbase(main):021:0&gt; import java.text.SimpleDateFormat
                    hbase(main):022:0&gt; import java.text.ParsePosition
                    hbase(main):023:0&gt; SimpleDateFormat.new("yy/MM/dd HH:mm:ss").parse("08/08/16 20:56:29", ParsePosition.new(0)).getTime() =&gt; 1218920189000</pre><p>
            </p><p>
                To go the other direction:
                </p><pre class="programlisting">
                    hbase(main):021:0&gt; import java.util.Date
                    hbase(main):022:0&gt; Date.new(1218920189000).toString() =&gt; "Sat Aug 16 20:56:29 UTC 2008"</pre><p>
            </p><p>
                To output in a format that is exactly like that of the HBase log format will take a little messing with
                <a class="link" href="http://download.oracle.com/javase/6/docs/api/java/text/SimpleDateFormat.html" target="_top">SimpleDateFormat</a>.
            </p></div><div class="section" title="4.2.4.&nbsp;Debug"><div class="titlepage"><div><div><h3 class="title"><a name="d366e3657"></a>4.2.4.&nbsp;Debug</h3></div></div></div><div class="section" title="4.2.4.1.&nbsp;Shell debug switch"><div class="titlepage"><div><div><h4 class="title"><a name="d366e3660"></a>4.2.4.1.&nbsp;Shell debug switch</h4></div></div></div><p>You can set a debug switch in the shell to see more output
                    -- e.g. more of the stack trace on exception --
                    when you run a command:
                    </p><pre class="programlisting">hbase&gt; debug &lt;RETURN&gt;</pre><p>
                 </p></div><div class="section" title="4.2.4.2.&nbsp;DEBUG log level"><div class="titlepage"><div><div><h4 class="title"><a name="d366e3668"></a>4.2.4.2.&nbsp;DEBUG log level</h4></div></div></div><p>To enable DEBUG level logging in the shell,
                    launch it with the <span class="command"><strong>-d</strong></span> option.
                    </p><pre class="programlisting">$ ./bin/hbase shell -d</pre><p>
               </p></div></div><div class="section" title="4.2.5.&nbsp;Commands"><div class="titlepage"><div><div><h3 class="title"><a name="d366e3679"></a>4.2.5.&nbsp;Commands</h3></div></div></div><div class="section" title="4.2.5.1.&nbsp;count"><div class="titlepage"><div><div><h4 class="title"><a name="d366e3682"></a>4.2.5.1.&nbsp;count</h4></div></div></div><p>Count command returns the number of rows in a table.
		    It's quite fast when configured with the right CACHE
            </p><pre class="programlisting">hbase&gt; count '&lt;tablename&gt;', CACHE =&gt; 1000</pre><p>
            The above count fetches 1000 rows at a time.  Set CACHE lower if your rows are big.
            Default is to fetch one row at a time.
                 </p></div></div></div></div><div class="chapter" title="Chapter&nbsp;5.&nbsp;Data Model"><div class="titlepage"><div><div><h2 class="title"><a name="datamodel"></a>Chapter&nbsp;5.&nbsp;Data Model</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#conceptual.view">5.1. Conceptual View</a></span></dt><dt><span class="section"><a href="#physical.view">5.2. Physical View</a></span></dt><dt><span class="section"><a href="#namespace">5.3. Namespace</a></span></dt><dd><dl><dt><span class="section"><a href="#namespace_creation">5.3.1. Namespace management</a></span></dt><dt><span class="section"><a href="#namespace_special">5.3.2. Predefined namespaces</a></span></dt></dl></dd><dt><span class="section"><a href="#table">5.4. Table</a></span></dt><dt><span class="section"><a href="#row">5.5. Row</a></span></dt><dt><span class="section"><a href="#columnfamily">5.6. Column Family</a></span></dt><dt><span class="section"><a href="#cells">5.7. Cells</a></span></dt><dt><span class="section"><a href="#data_model_operations">5.8. Data Model Operations</a></span></dt><dd><dl><dt><span class="section"><a href="#get">5.8.1. Get</a></span></dt><dt><span class="section"><a href="#put">5.8.2. Put</a></span></dt><dt><span class="section"><a href="#scan">5.8.3. Scans</a></span></dt><dt><span class="section"><a href="#delete">5.8.4. Delete</a></span></dt></dl></dd><dt><span class="section"><a href="#versions">5.9. Versions</a></span></dt><dd><dl><dt><span class="section"><a href="#versions.ops">5.9.1. Versions and HBase Operations</a></span></dt><dt><span class="section"><a href="#d366e4325">5.9.2. Current Limitations</a></span></dt></dl></dd><dt><span class="section"><a href="#dm.sort">5.10. Sort Order</a></span></dt><dt><span class="section"><a href="#dm.column.metadata">5.11. Column Metadata</a></span></dt><dt><span class="section"><a href="#joins">5.12. Joins</a></span></dt><dt><span class="section"><a href="#acid">5.13. ACID</a></span></dt></dl></div><p>In short, applications store data into an HBase table.
        Tables are made of rows and columns.
      All columns in HBase belong to a particular column family.
      Table cells -- the intersection of row and column
      coordinates -- are versioned.
      A cell&#8217;s content is an uninterpreted array of bytes.
  </p><p>Table row keys are also byte arrays so almost anything can
      serve as a row key from strings to binary representations of longs or
      even serialized data structures. Rows in HBase tables
      are sorted by row key. The sort is byte-ordered. All table accesses are
      via the table row key -- its primary key.
</p><div class="section" title="5.1.&nbsp;Conceptual View"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="conceptual.view"></a>5.1.&nbsp;Conceptual View</h2></div></div></div><p>
        The following example is a slightly modified form of the one on page
        2 of the <a class="link" href="http://research.google.com/archive/bigtable.html" target="_top">BigTable</a> paper.
    There is a table called <code class="varname">webtable</code> that contains two column families named
    <code class="varname">contents</code> and <code class="varname">anchor</code>.
    In this example, <code class="varname">anchor</code> contains two
    columns (<code class="varname">anchor:cssnsi.com</code>, <code class="varname">anchor:my.look.ca</code>)
    and <code class="varname">contents</code> contains one column (<code class="varname">contents:html</code>).
    </p><div class="note" title="Column Names" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Column Names</h3><p>
      By convention, a column name is made of its column family prefix and a
      <span class="emphasis"><em>qualifier</em></span>. For example, the
      column
      <span class="emphasis"><em>contents:html</em></span> is made up of the column family <code class="varname">contents</code>
      and <code class="varname">html</code> qualifier.
          The colon character (<code class="literal">:</code>) delimits the column family from the
          column family <span class="emphasis"><em>qualifier</em></span>.
    </p></div><p>
    </p><div class="table"><a name="d366e3753"></a><p class="title"><b>Table&nbsp;5.1.&nbsp;Table <code class="varname">webtable</code></b></p><div class="table-contents"><table summary="Table webtable" border="1"><colgroup><col align="left" class="c1"><col align="left" class="c2"><col align="left" class="c3"><col align="left" class="c4"></colgroup><thead><tr><th align="left">Row Key</th><th align="left">Time Stamp</th><th align="left">ColumnFamily <code class="varname">contents</code></th><th align="left">ColumnFamily <code class="varname">anchor</code></th></tr></thead><tbody><tr><td align="left">"com.cnn.www"</td><td align="left">t9</td><td align="left">&nbsp;</td><td align="left"><code class="varname">anchor:cnnsi.com</code> = "CNN"</td></tr><tr><td align="left">"com.cnn.www"</td><td align="left">t8</td><td align="left">&nbsp;</td><td align="left"><code class="varname">anchor:my.look.ca</code> = "CNN.com"</td></tr><tr><td align="left">"com.cnn.www"</td><td align="left">t6</td><td align="left"><code class="varname">contents:html</code> = "&lt;html&gt;..."</td><td align="left">&nbsp;</td></tr><tr><td align="left">"com.cnn.www"</td><td align="left">t5</td><td align="left"><code class="varname">contents:html</code> = "&lt;html&gt;..."</td><td align="left">&nbsp;</td></tr><tr><td align="left">"com.cnn.www"</td><td align="left">t3</td><td align="left"><code class="varname">contents:html</code> = "&lt;html&gt;..."</td><td align="left">&nbsp;</td></tr></tbody></table></div></div><p><br class="table-break">
	</p></div><div class="section" title="5.2.&nbsp;Physical View"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="physical.view"></a>5.2.&nbsp;Physical View</h2></div></div></div><p>
        Although at a conceptual level tables may be viewed as a sparse set of rows.
        Physically they are stored on a per-column family basis.  New columns
        (i.e., <code class="varname">columnfamily:column</code>) can be added to any
        column family without pre-announcing them.
        </p><div class="table"><a name="d366e3837"></a><p class="title"><b>Table&nbsp;5.2.&nbsp;ColumnFamily <code class="varname">anchor</code></b></p><div class="table-contents"><table summary="ColumnFamily anchor" border="1"><colgroup><col align="left" class="c1"><col align="left" class="c2"><col align="left" class="c3"></colgroup><thead><tr><th align="left">Row Key</th><th align="left">Time Stamp</th><th align="left">Column Family <code class="varname">anchor</code></th></tr></thead><tbody><tr><td align="left">"com.cnn.www"</td><td align="left">t9</td><td align="left"><code class="varname">anchor:cnnsi.com</code> = "CNN"</td></tr><tr><td align="left">"com.cnn.www"</td><td align="left">t8</td><td align="left"><code class="varname">anchor:my.look.ca</code> = "CNN.com"</td></tr></tbody></table></div></div><p><br class="table-break">
    </p><div class="table"><a name="d366e3876"></a><p class="title"><b>Table&nbsp;5.3.&nbsp;ColumnFamily <code class="varname">contents</code></b></p><div class="table-contents"><table summary="ColumnFamily contents" border="1"><colgroup><col align="left" class="c1"><col align="left" class="c2"><col align="left" class="c3"></colgroup><thead><tr><th align="left">Row Key</th><th align="left">Time Stamp</th><th align="left">ColumnFamily "contents:"</th></tr></thead><tbody><tr><td align="left">"com.cnn.www"</td><td align="left">t6</td><td align="left"><code class="varname">contents:html</code> = "&lt;html&gt;..."</td></tr><tr><td align="left">"com.cnn.www"</td><td align="left">t5</td><td align="left"><code class="varname">contents:html</code> = "&lt;html&gt;..."</td></tr><tr><td align="left">"com.cnn.www"</td><td align="left">t3</td><td align="left"><code class="varname">contents:html</code> = "&lt;html&gt;..."</td></tr></tbody></table></div></div><p><br class="table-break">
    It is important to note in the diagram above that the empty cells shown in the
    conceptual view are not stored since they need not be in a column-oriented
    storage format. Thus a request for the value of the <code class="varname">contents:html</code>
    column at time stamp <code class="literal">t8</code> would return no value. Similarly, a
    request for an <code class="varname">anchor:my.look.ca</code> value at time stamp
    <code class="literal">t9</code> would return no value.  However, if no timestamp is
    supplied, the most recent value for a particular column would be returned
    and would also be the first one found since timestamps are stored in
    descending order. Thus a request for the values of all columns in the row
    <code class="varname">com.cnn.www</code> if no timestamp is specified would be:
    the value of <code class="varname">contents:html</code> from time stamp
    <code class="literal">t6</code>, the value of <code class="varname">anchor:cnnsi.com</code>
    from time stamp <code class="literal">t9</code>, the value of
    <code class="varname">anchor:my.look.ca</code> from time stamp <code class="literal">t8</code>.
	</p><p>For more information about the internals of how Apache HBase stores data, see <a class="xref" href="#regions.arch" title="9.7.&nbsp;Regions">Section&nbsp;9.7, &#8220;Regions&#8221;</a>.
	</p></div><div class="section" title="5.3.&nbsp;Namespace"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="namespace"></a>5.3.&nbsp;Namespace</h2></div></div></div><p>
      A namespace is a logical grouping of tables analogous to a database in relation database
        systems. This abstraction lays the groundwork for upcoming multi-tenancy related features:
        </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Quota Management (HBASE-8410) - Restrict the amount of resources (ie
            regions, tables) a namespace can consume.</li><li class="listitem">Namespace Security Administration (HBASE-9206) - provide another
            level of security administration for tenants.</li><li class="listitem">Region server groups (HBASE-6721) - A namespace/table can be
            pinned onto a subset of regionservers thus guaranteeing a course level of
            isolation.</li></ul></div><p>
      </p><div class="section" title="5.3.1.&nbsp;Namespace management"><div class="titlepage"><div><div><h3 class="title"><a name="namespace_creation"></a>5.3.1.&nbsp;Namespace management</h3></div></div></div><p>
        A namespace can be created, removed or altered. Namespace membership is determined during
          table creation by specifying a fully-qualified table name of the form:
          </p><p>
            <code class="code">&lt;table namespace&gt;:&lt;table qualifier&gt;</code>
          </p><p>
          </p><p>
            Examples:
          </p><p>
</p><pre class="programlisting">
#Create a namespace
create_namespace 'my_ns'

#create my_table in my_ns namespace
create 'my_ns:my_table', 'fam'

#drop namespace
drop_namespace 'my_ns'

#alter namespace
alter_namespace 'my_ns', {METHOD =&gt; 'set', 'PROPERTY_NAME' =&gt; 'PROPERTY_VALUE'}
</pre><p>
        </p></div><div class="section" title="5.3.2.&nbsp;Predefined namespaces"><div class="titlepage"><div><div><h3 class="title"><a name="namespace_special"></a>5.3.2.&nbsp;Predefined namespaces</h3></div></div></div><p>
          There are two predefined special namespaces:
          </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">hbase - system namespace, used to contain hbase internal tables</li><li class="listitem">default - tables with no explicit specified namespace will automatically
              fall into this namespace.</li></ul></div><p>
        </p><p>
          Examples:
</p><pre class="programlisting">
#namespace=foo and table qualifier=bar
create 'foo:bar', 'fam'

#namespace=default and table qualifier=bar
create 'bar', 'fam'
</pre><p>
        </p></div></div><div class="section" title="5.4.&nbsp;Table"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="table"></a>5.4.&nbsp;Table</h2></div></div></div><p>
      Tables are declared up front at schema definition time.
      </p></div><div class="section" title="5.5.&nbsp;Row"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="row"></a>5.5.&nbsp;Row</h2></div></div></div><p>Row keys are uninterrpreted bytes. Rows are
      lexicographically sorted with the lowest order appearing first
      in a table.  The empty byte array is used to denote both the
      start and end of a tables' namespace.</p></div><div class="section" title="5.6.&nbsp;Column Family"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="columnfamily"></a>5.6.&nbsp;Column Family<a class="indexterm" name="d366e4018"></a></h2></div></div></div><p>
      Columns in Apache HBase are grouped into <span class="emphasis"><em>column families</em></span>.
      All column members of a column family have the same prefix.  For example, the
      columns <span class="emphasis"><em>courses:history</em></span> and
      <span class="emphasis"><em>courses:math</em></span> are both members of the
      <span class="emphasis"><em>courses</em></span> column family.
          The colon character (<code class="literal">:</code>) delimits the column family from the
      <a class="indexterm" name="d366e4038"></a>.
        The column family prefix must be composed of
      <span class="emphasis"><em>printable</em></span> characters. The qualifying tail, the
      column family <span class="emphasis"><em>qualifier</em></span>, can be made of any
      arbitrary bytes. Column families must be declared up front
      at schema definition time whereas columns do not need to be
      defined at schema time but can be conjured on the fly while
      the table is up an running.</p><p>Physically, all column family members are stored together on the
      filesystem.  Because tunings and
      storage specifications are done at the column family level, it is
      advised that all column family members have the same general access
      pattern and size characteristics.</p><p></p></div><div class="section" title="5.7.&nbsp;Cells"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="cells"></a>5.7.&nbsp;Cells<a class="indexterm" name="d366e4057"></a></h2></div></div></div><p>A <span class="emphasis"><em>{row, column, version} </em></span>tuple exactly
      specifies a <code class="literal">cell</code> in HBase.
      Cell content is uninterrpreted bytes</p></div><div class="section" title="5.8.&nbsp;Data Model Operations"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="data_model_operations"></a>5.8.&nbsp;Data Model Operations</h2></div></div></div><p>The four primary data model operations are Get, Put, Scan, and Delete.  Operations are applied via
       <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html" target="_top">HTable</a> instances.
       </p><div class="section" title="5.8.1.&nbsp;Get"><div class="titlepage"><div><div><h3 class="title"><a name="get"></a>5.8.1.&nbsp;Get</h3></div></div></div><p><a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Get.html" target="_top">Get</a> returns
        attributes for a specified row.  Gets are executed via
        <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html#get%28org.apache.hadoop.hbase.client.Get%29" target="_top">
        HTable.get</a>.
        </p></div><div class="section" title="5.8.2.&nbsp;Put"><div class="titlepage"><div><div><h3 class="title"><a name="put"></a>5.8.2.&nbsp;Put</h3></div></div></div><p><a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Put.html" target="_top">Put</a> either
        adds new rows to a table (if the key is new) or can update existing rows (if the key already exists).  Puts are executed via
        <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html#put%28org.apache.hadoop.hbase.client.Put%29" target="_top">
        HTable.put</a> (writeBuffer) or <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html#batch%28java.util.List%29" target="_top">
        HTable.batch</a> (non-writeBuffer).
        </p></div><div class="section" title="5.8.3.&nbsp;Scans"><div class="titlepage"><div><div><h3 class="title"><a name="scan"></a>5.8.3.&nbsp;Scans</h3></div></div></div><p><a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Scan.html" target="_top">Scan</a> allow
          iteration over multiple rows for specified attributes.
          </p><p>The following is an example of a
           on an HTable table instance.  Assume that a table is populated with rows with keys "row1", "row2", "row3",
           and then another set of rows with the keys "abc1", "abc2", and "abc3".  The following example shows how startRow and stopRow
           can be applied to a Scan instance to return the rows beginning with "row".
</p><pre class="programlisting">
public static final byte[] CF = "cf".getBytes();
public static final byte[] ATTR = "attr".getBytes();
...

HTable htable = ...      // instantiate HTable

Scan scan = new Scan();
scan.addColumn(CF, ATTR);
scan.setStartRow(Bytes.toBytes("row")); // start key is inclusive
scan.setStopRow(Bytes.toBytes("rox"));  // stop key is exclusive
ResultScanner rs = htable.getScanner(scan);
try {
  for (Result r = rs.next(); r != null; r = rs.next()) {
  // process result...
} finally {
  rs.close();  // always close the ResultScanner!
}
</pre><p>
         </p><p>Note that generally the easiest way to specify a specific stop point for a scan is by using the <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/InclusiveStopFilter.html" target="_top">InclusiveStopFilter</a> class.
         </p></div><div class="section" title="5.8.4.&nbsp;Delete"><div class="titlepage"><div><div><h3 class="title"><a name="delete"></a>5.8.4.&nbsp;Delete</h3></div></div></div><p><a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Delete.html" target="_top">Delete</a> removes
        a row from a table.  Deletes are executed via
        <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html#delete%28org.apache.hadoop.hbase.client.Delete%29" target="_top">
        HTable.delete</a>.
        </p><p>HBase does not modify data in place, and so deletes are handled by creating new markers called <span class="emphasis"><em>tombstones</em></span>.
        These tombstones, along with the dead values, are cleaned up on major compactions.
        </p><p>See <a class="xref" href="#version.delete" title="5.9.1.5.&nbsp;Delete">Section&nbsp;5.9.1.5, &#8220;Delete&#8221;</a> for more information on deleting versions of columns, and see
        <a class="xref" href="#compaction" title="9.7.6.5.&nbsp;Compaction">Section&nbsp;9.7.6.5, &#8220;Compaction&#8221;</a> for more information on compactions.
        </p></div></div><div class="section" title="5.9.&nbsp;Versions"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="versions"></a>5.9.&nbsp;Versions<a class="indexterm" name="d366e4140"></a></h2></div></div></div><p>A <span class="emphasis"><em>{row, column, version} </em></span>tuple exactly
      specifies a <code class="literal">cell</code> in HBase. It's possible to have an
      unbounded number of cells where the row and column are the same but the
      cell address differs only in its version dimension.</p><p>While rows and column keys are expressed as bytes, the version is
      specified using a long integer. Typically this long contains time
      instances such as those returned by
      <code class="code">java.util.Date.getTime()</code> or
      <code class="code">System.currentTimeMillis()</code>, that is: <span class="quote">&#8220;<span class="quote">the difference,
      measured in milliseconds, between the current time and midnight, January
      1, 1970 UTC</span>&#8221;</span>.</p><p>The HBase version dimension is stored in decreasing order, so that
      when reading from a store file, the most recent values are found
      first.</p><p>There is a lot of confusion over the semantics of
      <code class="literal">cell</code> versions, in HBase. In particular, a couple
      questions that often come up are:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p>If multiple writes to a cell have the same version, are all
            versions maintained or just the last?<sup>[<a name="d366e4173" href="#ftn.d366e4173" class="footnote">15</a>]</sup></p></li><li class="listitem"><p>Is it OK to write cells in a non-increasing version
            order?<sup>[<a name="d366e4179" href="#ftn.d366e4179" class="footnote">16</a>]</sup></p></li></ul></div><p>Below we describe how the version dimension in HBase currently
      works<sup>[<a name="d366e4184" href="#ftn.d366e4184" class="footnote">17</a>]</sup>.</p><div class="section" title="5.9.1.&nbsp;Versions and HBase Operations"><div class="titlepage"><div><div><h3 class="title"><a name="versions.ops"></a>5.9.1.&nbsp;Versions and HBase Operations</h3></div></div></div><p>In this section we look at the behavior of the version dimension
        for each of the core HBase operations.</p><div class="section" title="5.9.1.1.&nbsp;Get/Scan"><div class="titlepage"><div><div><h4 class="title"><a name="d366e4202"></a>5.9.1.1.&nbsp;Get/Scan</h4></div></div></div><p>Gets are implemented on top of Scans. The below discussion of
            <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Get.html" target="_top">Get</a> applies equally to <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Scan.html" target="_top">Scans</a>.</p><p>By default, i.e. if you specify no explicit version, when
          doing a <code class="literal">get</code>, the cell whose version has the
          largest value is returned (which may or may not be the latest one
          written, see later). The default behavior can be modified in the
          following ways:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p>to return more than one version, see <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Get.html#setMaxVersions()" target="_top">Get.setMaxVersions()</a></p></li><li class="listitem"><p>to return versions other than the latest, see <a class="link" href="???" target="_top">Get.setTimeRange()</a></p><p>To retrieve the latest version that is less than or equal
              to a given value, thus giving the 'latest' state of the record
              at a certain point in time, just use a range from 0 to the
              desired version and set the max versions to 1.</p></li></ul></div></div><div class="section" title="5.9.1.2.&nbsp;Default Get Example"><div class="titlepage"><div><div><h4 class="title"><a name="default_get_example"></a>5.9.1.2.&nbsp;Default Get Example</h4></div></div></div><p>The following Get will only retrieve the current version of the row
</p><pre class="programlisting">
public static final byte[] CF = "cf".getBytes();
public static final byte[] ATTR = "attr".getBytes();
...
Get get = new Get(Bytes.toBytes("row1"));
Result r = htable.get(get);
byte[] b = r.getValue(CF, ATTR);  // returns current version of value
</pre><p>
        </p></div><div class="section" title="5.9.1.3.&nbsp;Versioned Get Example"><div class="titlepage"><div><div><h4 class="title"><a name="versioned_get_example"></a>5.9.1.3.&nbsp;Versioned Get Example</h4></div></div></div><p>The following Get will return the last 3 versions of the row.
</p><pre class="programlisting">
public static final byte[] CF = "cf".getBytes();
public static final byte[] ATTR = "attr".getBytes();
...
Get get = new Get(Bytes.toBytes("row1"));
get.setMaxVersions(3);  // will return last 3 versions of row
Result r = htable.get(get);
byte[] b = r.getValue(CF, ATTR);  // returns current version of value
List&lt;KeyValue&gt; kv = r.getColumn(CF, ATTR);  // returns all versions of this column
</pre><p>
        </p></div><div class="section" title="5.9.1.4.&nbsp;Put"><div class="titlepage"><div><div><h4 class="title"><a name="d366e4247"></a>5.9.1.4.&nbsp;Put</h4></div></div></div><p>Doing a put always creates a new version of a
          <code class="literal">cell</code>, at a certain timestamp. By default the
          system uses the server's <code class="literal">currentTimeMillis</code>, but
          you can specify the version (= the long integer) yourself, on a
          per-column level. This means you could assign a time in the past or
          the future, or use the long value for non-time purposes.</p><p>To overwrite an existing value, do a put at exactly the same
          row, column, and version as that of the cell you would
          overshadow.</p><div class="section" title="5.9.1.4.1.&nbsp;Implicit Version Example"><div class="titlepage"><div><div><h5 class="title"><a name="implicit_version_example"></a>5.9.1.4.1.&nbsp;Implicit Version Example</h5></div></div></div><p>The following Put will be implicitly versioned by HBase with the current time.
</p><pre class="programlisting">
public static final byte[] CF = "cf".getBytes();
public static final byte[] ATTR = "attr".getBytes();
...
Put put = new Put(Bytes.toBytes(row));
put.add(CF, ATTR, Bytes.toBytes( data));
htable.put(put);
</pre><p>
          </p></div><div class="section" title="5.9.1.4.2.&nbsp;Explicit Version Example"><div class="titlepage"><div><div><h5 class="title"><a name="explicit_version_example"></a>5.9.1.4.2.&nbsp;Explicit Version Example</h5></div></div></div><p>The following Put has the version timestamp explicitly set.
</p><pre class="programlisting">
public static final byte[] CF = "cf".getBytes();
public static final byte[] ATTR = "attr".getBytes();
...
Put put = new Put( Bytes.toBytes(row));
long explicitTimeInMs = 555;  // just an example
put.add(CF, ATTR, explicitTimeInMs, Bytes.toBytes(data));
htable.put(put);
</pre><p>
          Caution:  the version timestamp is internally by HBase for things like time-to-live calculations.
          It's usually best to avoid setting this timestamp yourself.  Prefer using a separate
          timestamp attribute of the row, or have the timestamp a part of the rowkey, or both.
          </p></div></div><div class="section" title="5.9.1.5.&nbsp;Delete"><div class="titlepage"><div><div><h4 class="title"><a name="version.delete"></a>5.9.1.5.&nbsp;Delete</h4></div></div></div><p>There are three different types of internal delete markers
            <sup>[<a name="d366e4281" href="#ftn.d366e4281" class="footnote">18</a>]</sup>:
            </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p>Delete:  for a specific version of a column.</p></li><li class="listitem"><p>Delete column:  for all versions of a column.</p></li><li class="listitem"><p>Delete family:  for all columns of a particular ColumnFamily</p></li></ul></div><p>
          When deleting an entire row, HBase will internally create a tombstone for each ColumnFamily (i.e., not each individual column).
         </p><p>Deletes work by creating <span class="emphasis"><em>tombstone</em></span>
          markers. For example, let's suppose we want to delete a row. For
          this you can specify a version, or else by default the
          <code class="literal">currentTimeMillis</code> is used. What this means is
          <span class="quote">&#8220;<span class="quote">delete all cells where the version is less than or equal to
          this version</span>&#8221;</span>. HBase never modifies data in place, so for
          example a delete will not immediately delete (or mark as deleted)
          the entries in the storage file that correspond to the delete
          condition. Rather, a so-called <span class="emphasis"><em>tombstone</em></span> is
          written, which will mask the deleted values<sup>[<a name="d366e4312" href="#ftn.d366e4312" class="footnote">19</a>]</sup>. If the version you specified when deleting a row is
          larger than the version of any value in the row, then you can
          consider the complete row to be deleted.</p><p>For an informative discussion on how deletes and versioning interact, see
          the thread <a class="link" href="http://comments.gmane.org/gmane.comp.java.hadoop.hbase.user/28421" target="_top">Put w/ timestamp -&gt; Deleteall -&gt; Put w/ timestamp fails</a>
          up on the user mailing list.</p><p>Also see <a class="xref" href="#keyvalue" title="9.7.6.4.&nbsp;KeyValue">Section&nbsp;9.7.6.4, &#8220;KeyValue&#8221;</a> for more information on the internal KeyValue format.
          </p></div></div><div class="section" title="5.9.2.&nbsp;Current Limitations"><div class="titlepage"><div><div><h3 class="title"><a name="d366e4325"></a>5.9.2.&nbsp;Current Limitations</h3></div></div></div><div class="section" title="5.9.2.1.&nbsp;Deletes mask Puts"><div class="titlepage"><div><div><h4 class="title"><a name="d366e4328"></a>5.9.2.1.&nbsp;Deletes mask Puts</h4></div></div></div><p>Deletes mask puts, even puts that happened after the delete
          was entered<sup>[<a name="d366e4333" href="#ftn.d366e4333" class="footnote">20</a>]</sup>. Remember that a delete writes a tombstone, which only
          disappears after then next major compaction has run. Suppose you do
          a delete of everything &lt;= T. After this you do a new put with a
          timestamp &lt;= T. This put, even if it happened after the delete,
          will be masked by the delete tombstone. Performing the put will not
          fail, but when you do a get you will notice the put did have no
          effect. It will start working again after the major compaction has
          run. These issues should not be a problem if you use
          always-increasing versions for new puts to a row. But they can occur
          even if you do not care about time: just do delete and put
          immediately after each other, and there is some chance they happen
          within the same millisecond.</p></div><div class="section" title="5.9.2.2.&nbsp;Major compactions change query results"><div class="titlepage"><div><div><h4 class="title"><a name="d366e4338"></a>5.9.2.2.&nbsp;Major compactions change query results</h4></div></div></div><p><span class="quote">&#8220;<span class="quote">...create three cell versions at t1, t2 and t3, with a
          maximum-versions setting of 2. So when getting all versions, only
          the values at t2 and t3 will be returned. But if you delete the
          version at t2 or t3, the one at t1 will appear again. Obviously,
          once a major compaction has run, such behavior will not be the case
          anymore...<sup>[<a name="d366e4344" href="#ftn.d366e4344" class="footnote">21</a>]</sup></span>&#8221;</span></p></div></div></div><div class="section" title="5.10.&nbsp;Sort Order"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="dm.sort"></a>5.10.&nbsp;Sort Order</h2></div></div></div><p>All data model operations HBase return data in sorted order.  First by row,
      then by ColumnFamily, followed by column qualifier, and finally timestamp (sorted
      in reverse, so newest records are returned first).
      </p></div><div class="section" title="5.11.&nbsp;Column Metadata"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="dm.column.metadata"></a>5.11.&nbsp;Column Metadata</h2></div></div></div><p>There is no store of column metadata outside of the internal KeyValue instances for a ColumnFamily.
      Thus, while HBase can support not only a wide number of columns per row, but a heterogenous set of columns
      between rows as well, it is your responsibility to keep track of the column names.
      </p><p>The only way to get a complete set of columns that exist for a ColumnFamily is to process all the rows.
      For more information about how HBase stores data internally, see <a class="xref" href="#keyvalue" title="9.7.6.4.&nbsp;KeyValue">Section&nbsp;9.7.6.4, &#8220;KeyValue&#8221;</a>.
	  </p></div><div class="section" title="5.12.&nbsp;Joins"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="joins"></a>5.12.&nbsp;Joins</h2></div></div></div><p>Whether HBase supports joins is a common question on the dist-list, and there is a simple answer:  it doesn't,
      at not least in the way that RDBMS' support them (e.g., with equi-joins or outer-joins in SQL).  As has been illustrated
      in this chapter, the read data model operations in HBase are Get and Scan.
      </p><p>However, that doesn't mean that equivalent join functionality can't be supported in your application, but
      you have to do it yourself.  The two primary strategies are either denormalizing the data upon writing to HBase,
      or to have lookup tables and do the join between HBase tables in your application or MapReduce code (and as RDBMS'
      demonstrate, there are several strategies for this depending on the size of the tables, e.g., nested loops vs.
      hash-joins).  So which is the best approach?  It depends on what you are trying to do, and as such there isn't a single
      answer that works for every use case.
      </p></div><div class="section" title="5.13.&nbsp;ACID"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="acid"></a>5.13.&nbsp;ACID</h2></div></div></div><span style="color: red">&lt;pre&gt;See <a class="link" href="http://hbase.apache.org/acid-semantics.html" target="_top">ACID Semantics</a>.
            Lars Hofhansl has also written a note on
            <a class="link" href="http://hadoop-hbase.blogspot.com/2012/03/acid-in-hbase.html" target="_top">ACID in HBase</a>.&lt;/pre&gt;</span></div><div class="footnotes"><br><hr width="100" align="left"><div class="footnote"><p><sup>[<a id="ftn.d366e4173" href="#d366e4173" class="para">15</a>] </sup>Currently, only the last written is fetchable.</p></div><div class="footnote"><p><sup>[<a id="ftn.d366e4179" href="#d366e4179" class="para">16</a>] </sup>Yes</p></div><div class="footnote"><p><sup>[<a id="ftn.d366e4184" href="#d366e4184" class="para">17</a>] </sup>See <a class="link" href="https://issues.apache.org/jira/browse/HBASE-2406" target="_top">HBASE-2406</a>
          for discussion of HBase versions. <a class="link" href="http://outerthought.org/blog/417-ot.html" target="_top">Bending time
          in HBase</a> makes for a good read on the version, or time,
          dimension in HBase. It has more detail on versioning than is
          provided here. As of this writing, the limiitation
          <span class="emphasis"><em>Overwriting values at existing timestamps</em></span>
          mentioned in the article no longer holds in HBase. This section is
          basically a synopsis of this article by Bruno Dumon.</p></div><div class="footnote"><p><sup>[<a id="ftn.d366e4281" href="#d366e4281" class="para">18</a>] </sup>See Lars Hofhansl's blog for discussion of his attempt
            adding another, <a class="link" href="http://hadoop-hbase.blogspot.com/2012/01/scanning-in-hbase.html" target="_top">Scanning in HBase: Prefix Delete Marker</a></p></div><div class="footnote"><p><sup>[<a id="ftn.d366e4312" href="#d366e4312" class="para">19</a>] </sup>When HBase does a major compaction, the tombstones are
              processed to actually remove the dead values, together with the
              tombstones themselves.</p></div><div class="footnote"><p><sup>[<a id="ftn.d366e4333" href="#d366e4333" class="para">20</a>] </sup><a class="link" href="https://issues.apache.org/jira/browse/HBASE-2256" target="_top">HBASE-2256</a></p></div><div class="footnote"><p><sup>[<a id="ftn.d366e4344" href="#d366e4344" class="para">21</a>] </sup>See <span class="emphasis"><em>Garbage Collection</em></span> in <a class="link" href="http://outerthought.org/blog/417-ot.html" target="_top">Bending
              time in HBase</a> </p></div></div></div><div class="chapter" title="Chapter&nbsp;6.&nbsp;HBase and Schema Design"><div class="titlepage"><div><div><h2 class="title"><a name="schema"></a>Chapter&nbsp;6.&nbsp;HBase and Schema Design</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#schema.creation">6.1. 
      Schema Creation
  </a></span></dt><dd><dl><dt><span class="section"><a href="#schema.updates">6.1.1. Schema Updates</a></span></dt></dl></dd><dt><span class="section"><a href="#number.of.cfs">6.2. 
      On the number of column families
  </a></span></dt><dd><dl><dt><span class="section"><a href="#number.of.cfs.card">6.2.1. Cardinality of ColumnFamilies</a></span></dt></dl></dd><dt><span class="section"><a href="#rowkey.design">6.3. Rowkey Design</a></span></dt><dd><dl><dt><span class="section"><a href="#timeseries">6.3.1. 
    Monotonically Increasing Row Keys/Timeseries Data
    </a></span></dt><dt><span class="section"><a href="#keysize">6.3.2. Try to minimize row and column sizes</a></span></dt><dt><span class="section"><a href="#reverse.timestamp">6.3.3. Reverse Timestamps</a></span></dt><dt><span class="section"><a href="#rowkey.scope">6.3.4. Rowkeys and ColumnFamilies</a></span></dt><dt><span class="section"><a href="#changing.rowkeys">6.3.5. Immutability of Rowkeys</a></span></dt><dt><span class="section"><a href="#rowkey.regionsplits">6.3.6. Relationship Between RowKeys and Region Splits</a></span></dt></dl></dd><dt><span class="section"><a href="#schema.versions">6.4. 
  Number of Versions
  </a></span></dt><dd><dl><dt><span class="section"><a href="#schema.versions.max">6.4.1. Maximum Number of Versions</a></span></dt><dt><span class="section"><a href="#schema.minversions">6.4.2. 
    Minimum Number of Versions
    </a></span></dt></dl></dd><dt><span class="section"><a href="#supported.datatypes">6.5. 
  Supported Datatypes
  </a></span></dt><dd><dl><dt><span class="section"><a href="#counters">6.5.1. Counters</a></span></dt></dl></dd><dt><span class="section"><a href="#schema.joins">6.6. Joins</a></span></dt><dt><span class="section"><a href="#ttl">6.7. Time To Live (TTL)</a></span></dt><dt><span class="section"><a href="#cf.keep.deleted">6.8. 
  Keeping Deleted Cells
  </a></span></dt><dt><span class="section"><a href="#secondary.indexes">6.9. 
  Secondary Indexes and Alternate Query Paths
  </a></span></dt><dd><dl><dt><span class="section"><a href="#secondary.indexes.filter">6.9.1. 
       Filter Query
      </a></span></dt><dt><span class="section"><a href="#secondary.indexes.periodic">6.9.2. 
       Periodic-Update Secondary Index
      </a></span></dt><dt><span class="section"><a href="#secondary.indexes.dualwrite">6.9.3. 
       Dual-Write Secondary Index
      </a></span></dt><dt><span class="section"><a href="#secondary.indexes.summary">6.9.4. 
       Summary Tables
      </a></span></dt><dt><span class="section"><a href="#secondary.indexes.coproc">6.9.5. 
       Coprocessor Secondary Index
      </a></span></dt></dl></dd><dt><span class="section"><a href="#constraints">6.10. Constraints</a></span></dt><dt><span class="section"><a href="#schema.casestudies">6.11. Schema Design Case Studies</a></span></dt><dd><dl><dt><span class="section"><a href="#schema.casestudies.log-timeseries">6.11.1. Case Study - Log Data and Timeseries Data</a></span></dt><dt><span class="section"><a href="#schema.casestudies.log-steroids">6.11.2. Case Study - Log Data and Timeseries Data on Steroids</a></span></dt><dt><span class="section"><a href="#schema.casestudies.custorder">6.11.3. Case Study - Customer/Order</a></span></dt><dt><span class="section"><a href="#schema.smackdown">6.11.4. Case Study - "Tall/Wide/Middle" Schema Design Smackdown</a></span></dt><dt><span class="section"><a href="#casestudies.schema.listdata">6.11.5. Case Study - List Data</a></span></dt></dl></dd><dt><span class="section"><a href="#schema.ops">6.12. Operational and Performance Configuration Options</a></span></dt></dl></div><p>A good general introduction on the strength and weaknesses modelling on
          the various non-rdbms datastores is Ian Varley's Master thesis,
          <a class="link" href="http://ianvarley.com/UT/MR/Varley_MastersReport_Full_2009-08-07.pdf" target="_top">No Relation: The Mixed Blessings of Non-Relational Databases</a>.
          Recommended.  Also, read <a class="xref" href="#keyvalue" title="9.7.6.4.&nbsp;KeyValue">Section&nbsp;9.7.6.4, &#8220;KeyValue&#8221;</a> for how HBase stores data internally, and the section on 
          <a class="xref" href="#schema.casestudies" title="6.11.&nbsp;Schema Design Case Studies">Section&nbsp;6.11, &#8220;Schema Design Case Studies&#8221;</a>.
      </p><div class="section" title="6.1.&nbsp; Schema Creation"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="schema.creation"></a>6.1.&nbsp;
      Schema Creation
  </h2></div></div></div><p>HBase schemas can be created or updated with <a class="xref" href="#shell" title="Chapter&nbsp;4.&nbsp;The Apache HBase Shell">Chapter&nbsp;4, <i>The Apache HBase Shell</i></a>
      or by using <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HBaseAdmin.html" target="_top">HBaseAdmin</a> in the Java API.
      </p><p>Tables must be disabled when making ColumnFamily modifications, for example..
      </p><pre class="programlisting">
Configuration config = HBaseConfiguration.create();
HBaseAdmin admin = new HBaseAdmin(conf);
String table = "myTable";

admin.disableTable(table);

HColumnDescriptor cf1 = ...;
admin.addColumn(table, cf1);      // adding new ColumnFamily
HColumnDescriptor cf2 = ...;
admin.modifyColumn(table, cf2);    // modifying existing ColumnFamily

admin.enableTable(table);
      </pre><p>
      </p>See <a class="xref" href="#client_dependencies" title="2.3.4.&nbsp;Client configuration and dependencies connecting to an HBase cluster">Section&nbsp;2.3.4, &#8220;Client configuration and dependencies connecting to an HBase cluster&#8221;</a> for more information about configuring client connections.
      <p>Note:  online schema changes are supported in the 0.92.x codebase, but the 0.90.x codebase requires the table
      to be disabled.
      </p><div class="section" title="6.1.1.&nbsp;Schema Updates"><div class="titlepage"><div><div><h3 class="title"><a name="schema.updates"></a>6.1.1.&nbsp;Schema Updates</h3></div></div></div><p>When changes are made to either Tables or ColumnFamilies (e.g., region size, block size), these changes
      take effect the next time there is a major compaction and the StoreFiles get re-written.
      </p><p>See <a class="xref" href="#store" title="9.7.6.&nbsp;Store">Section&nbsp;9.7.6, &#8220;Store&#8221;</a> for more information on StoreFiles.
      </p></div></div><div class="section" title="6.2.&nbsp; On the number of column families"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="number.of.cfs"></a>6.2.&nbsp;
      On the number of column families
  </h2></div></div></div><p>
      HBase currently does not do well with anything above two or three column families so keep the number
      of column families in your schema low.  Currently, flushing and compactions are done on a per Region basis so
      if one column family is carrying the bulk of the data bringing on flushes, the adjacent families
      will also be flushed though the amount of data they carry is small.  When many column families the
      flushing and compaction interaction can make for a bunch of needless i/o loading (To be addressed by
      changing flushing and compaction to work on a per column family basis).  For more information
      on compactions, see <a class="xref" href="#compaction" title="9.7.6.5.&nbsp;Compaction">Section&nbsp;9.7.6.5, &#8220;Compaction&#8221;</a>.
    </p><p>Try to make do with one column family if you can in your schemas.  Only introduce a
        second and third column family in the case where data access is usually column scoped;
        i.e. you query one column family or the other but usually not both at the one time.
    </p><div class="section" title="6.2.1.&nbsp;Cardinality of ColumnFamilies"><div class="titlepage"><div><div><h3 class="title"><a name="number.of.cfs.card"></a>6.2.1.&nbsp;Cardinality of ColumnFamilies</h3></div></div></div><p>Where multiple ColumnFamilies exist in a single table, be aware of the cardinality (i.e., number of rows).
      If ColumnFamilyA has 1 million rows and ColumnFamilyB has 1 billion rows, ColumnFamilyA's data will likely be spread
      across many, many regions (and RegionServers).  This makes mass scans for ColumnFamilyA less efficient.
      </p></div></div><div class="section" title="6.3.&nbsp;Rowkey Design"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="rowkey.design"></a>6.3.&nbsp;Rowkey Design</h2></div></div></div><div class="section" title="6.3.1.&nbsp; Monotonically Increasing Row Keys/Timeseries Data"><div class="titlepage"><div><div><h3 class="title"><a name="timeseries"></a>6.3.1.&nbsp;
    Monotonically Increasing Row Keys/Timeseries Data
    </h3></div></div></div><p>
      In the HBase chapter of Tom White's book Hadoop: The Definitive Guide (O'Reilly) there is a an optimization note on watching out for a phenomenon where an import process walks in lock-step with all clients in concert pounding one of the table's regions (and thus, a single node), then moving onto the next region, etc.  With monotonically increasing row-keys (i.e., using a timestamp), this will happen.  See this comic by IKai Lan on why monotonically increasing row keys are problematic in BigTable-like datastores:
      <a class="link" href="http://ikaisays.com/2011/01/25/app-engine-datastore-tip-monotonically-increasing-values-are-bad/" target="_top">monotonically increasing values are bad</a>.  The pile-up on a single region brought on
      by monotonically increasing keys can be mitigated by randomizing the input records to not be in sorted order, but in general it's best to avoid using a timestamp or a sequence (e.g. 1, 2, 3) as the row-key.
    </p><p>If you do need to upload time series data into HBase, you should
    study <a class="link" href="http://opentsdb.net/" target="_top">OpenTSDB</a> as a
    successful example.  It has a page describing the <a class="link" href=" http://opentsdb.net/schema.html" target="_top">schema</a> it uses in
    HBase.  The key format in OpenTSDB is effectively [metric_type][event_timestamp], which would appear at first glance to contradict the previous advice about not using a timestamp as the key.  However, the difference is that the timestamp is not in the <span class="emphasis"><em>lead</em></span> position of the key, and the design assumption is that there are dozens or hundreds (or more) of different metric types.  Thus, even with a continual stream of input data with a mix of metric types, the Puts are distributed across various points of regions in the table.
   </p><p>See <a class="xref" href="#schema.casestudies" title="6.11.&nbsp;Schema Design Case Studies">Section&nbsp;6.11, &#8220;Schema Design Case Studies&#8221;</a> for some rowkey design examples.
   </p></div><div class="section" title="6.3.2.&nbsp;Try to minimize row and column sizes"><div class="titlepage"><div><div><h3 class="title"><a name="keysize"></a>6.3.2.&nbsp;Try to minimize row and column sizes</h3></div><div><h4 class="subtitle">Or why are my StoreFile indices large?</h4></div></div></div><p>In HBase, values are always freighted with their coordinates; as a
          cell value passes through the system, it'll be accompanied by its
          row, column name, and timestamp - always.  If your rows and column names
          are large, especially compared to the size of the cell value, then
          you may run up against some interesting scenarios.  One such is
          the case described by Marc Limotte at the tail of
          HBASE-3551
          (recommended!).
          Therein, the indices that are kept on HBase storefiles (<a class="xref" href="#hfile" title="9.7.6.2.&nbsp;StoreFile (HFile)">Section&nbsp;9.7.6.2, &#8220;StoreFile (HFile)&#8221;</a>)
                  to facilitate random access may end up occupyng large chunks of the HBase
                  allotted RAM because the cell value coordinates are large.
                  Mark in the above cited comment suggests upping the block size so
                  entries in the store file index happen at a larger interval or
                  modify the table schema so it makes for smaller rows and column
                  names.
                  Compression will also make for larger indices.  See
                  the thread <a class="link" href="http://search-hadoop.com/m/hemBv1LiN4Q1/a+question+storefileIndexSize&amp;subj=a+question+storefileIndexSize" target="_top">a question storefileIndexSize</a>
                  up on the user mailing list.
       </p><p>Most of the time small inefficiencies don't matter all that much.  Unfortunately,
         this is a case where they do.  Whatever patterns are selected for ColumnFamilies, attributes, and rowkeys they could be repeated
       several billion times in your data. </p><p>See <a class="xref" href="#keyvalue" title="9.7.6.4.&nbsp;KeyValue">Section&nbsp;9.7.6.4, &#8220;KeyValue&#8221;</a> for more information on HBase stores data internally to see why this is important.</p><div class="section" title="6.3.2.1.&nbsp;Column Families"><div class="titlepage"><div><div><h4 class="title"><a name="keysize.cf"></a>6.3.2.1.&nbsp;Column Families</h4></div></div></div><p>Try to keep the ColumnFamily names as small as possible, preferably one character (e.g. "d" for data/default).
         </p><p>See <a class="xref" href="#keyvalue" title="9.7.6.4.&nbsp;KeyValue">Section&nbsp;9.7.6.4, &#8220;KeyValue&#8221;</a> for more information on HBase stores data internally to see why this is important.</p></div><div class="section" title="6.3.2.2.&nbsp;Attributes"><div class="titlepage"><div><div><h4 class="title"><a name="keysize.atttributes"></a>6.3.2.2.&nbsp;Attributes</h4></div></div></div><p>Although verbose attribute names (e.g., "myVeryImportantAttribute") are easier to read, prefer shorter attribute names (e.g., "via")
         to store in HBase.
         </p><p>See <a class="xref" href="#keyvalue" title="9.7.6.4.&nbsp;KeyValue">Section&nbsp;9.7.6.4, &#8220;KeyValue&#8221;</a> for more information on HBase stores data internally to see why this is important.</p></div><div class="section" title="6.3.2.3.&nbsp;Rowkey Length"><div class="titlepage"><div><div><h4 class="title"><a name="keysize.row"></a>6.3.2.3.&nbsp;Rowkey Length</h4></div></div></div><p>Keep them as short as is reasonable such that they can still be useful for required data access (e.g., Get vs. Scan).
         A short key that is useless for data access is not better than a longer key with better get/scan properties.  Expect tradeoffs
         when designing rowkeys.
         </p></div><div class="section" title="6.3.2.4.&nbsp;Byte Patterns"><div class="titlepage"><div><div><h4 class="title"><a name="keysize.patterns"></a>6.3.2.4.&nbsp;Byte Patterns</h4></div></div></div><p>A long is 8 bytes.  You can store an unsigned number up to 18,446,744,073,709,551,615 in those eight bytes.
            If you stored this number as a String -- presuming a byte per character -- you need nearly 3x the bytes.
         </p><p>Not convinced?  Below is some sample code that you can run on your own.
</p><pre class="programlisting">
// long
//
long l = 1234567890L;
byte[] lb = Bytes.toBytes(l);
System.out.println("long bytes length: " + lb.length);   // returns 8

String s = "" + l;
byte[] sb = Bytes.toBytes(s);
System.out.println("long as string length: " + sb.length);    // returns 10

// hash
//
MessageDigest md = MessageDigest.getInstance("MD5");
byte[] digest = md.digest(Bytes.toBytes(s));
System.out.println("md5 digest bytes length: " + digest.length);    // returns 16

String sDigest = new String(digest);
byte[] sbDigest = Bytes.toBytes(sDigest);
System.out.println("md5 digest as string length: " + sbDigest.length);    // returns 26
</pre><p>
         </p><p>Unfortunately, using a binary representation of a type will make your data harder to read outside of your code. For example,
               this is what you will see in the shell when you increment a value:
</p><pre class="programlisting">
hbase(main):001:0&gt; incr 't', 'r', 'f:q', 1
COUNTER VALUE = 1

hbase(main):002:0&gt; get 't', 'r'
COLUMN                                        CELL
 f:q                                          timestamp=1369163040570, value=\x00\x00\x00\x00\x00\x00\x00\x01
1 row(s) in 0.0310 seconds
</pre><p>
               The shell makes a best effort to print a string, and it this case it decided to just print the hex. The same will
               happen to your row keys inside the region names. It can be okay if you know what's being stored, but it might also
               be unreadable if arbitrary data can be put in the same cells. This is the main trade-off.
         </p></div></div><div class="section" title="6.3.3.&nbsp;Reverse Timestamps"><div class="titlepage"><div><div><h3 class="title"><a name="reverse.timestamp"></a>6.3.3.&nbsp;Reverse Timestamps</h3></div></div></div><p>A common problem in database processing is quickly finding the most recent version of a value.  A technique using reverse timestamps
    as a part of the key can help greatly with a special case of this problem.  Also found in the HBase chapter of Tom White's book Hadoop:  The Definitive Guide (O'Reilly),
    the technique involves appending (<code class="code">Long.MAX_VALUE - timestamp</code>) to the end of any key, e.g., [key][reverse_timestamp].
    </p><p>The most recent value for [key] in a table can be found by performing a Scan for [key] and obtaining the first record.  Since HBase keys
    are in sorted order, this key sorts before any older row-keys for [key] and thus is first.
    </p><p>This technique would be used instead of using <a class="xref" href="#schema.versions" title="6.4.&nbsp; Number of Versions">Section&nbsp;6.4, &#8220;
  Number of Versions
  &#8221;</a> where the intent is to hold onto all versions
    "forever" (or a very long time) and at the same time quickly obtain access to any other version by using the same Scan technique.
    </p></div><div class="section" title="6.3.4.&nbsp;Rowkeys and ColumnFamilies"><div class="titlepage"><div><div><h3 class="title"><a name="rowkey.scope"></a>6.3.4.&nbsp;Rowkeys and ColumnFamilies</h3></div></div></div><p>Rowkeys are scoped to ColumnFamilies.  Thus, the same rowkey could exist in each ColumnFamily that exists in a table without collision.
    </p></div><div class="section" title="6.3.5.&nbsp;Immutability of Rowkeys"><div class="titlepage"><div><div><h3 class="title"><a name="changing.rowkeys"></a>6.3.5.&nbsp;Immutability of Rowkeys</h3></div></div></div><p>Rowkeys cannot be changed.  The only way they can be "changed" in a table is if the row is deleted and then re-inserted.
    This is a fairly common question on the HBase dist-list so it pays to get the rowkeys right the first time (and/or before you've
    inserted a lot of data).
    </p></div><div class="section" title="6.3.6.&nbsp;Relationship Between RowKeys and Region Splits"><div class="titlepage"><div><div><h3 class="title"><a name="rowkey.regionsplits"></a>6.3.6.&nbsp;Relationship Between RowKeys and Region Splits</h3></div></div></div><p>If you pre-split your table, it is <span class="emphasis"><em>critical</em></span> to understand how your rowkey will be distributed across
    the region boundaries.  As an example of why this is important, consider the example of using displayable hex characters as the
    lead position of the key (e.g., ""0000000000000000" to "ffffffffffffffff").  Running those key ranges through <code class="code">Bytes.split</code>
    (which is the split strategy used when creating regions in <code class="code">HBaseAdmin.createTable(byte[] startKey, byte[] endKey, numRegions)</code>
    for 10 regions will generate the following splits...
    </p><p>
    </p><pre class="programlisting">
48 48 48 48 48 48 48 48 48 48 48 48 48 48 48 48                                // 0
54 -10 -10 -10 -10 -10 -10 -10 -10 -10 -10 -10 -10 -10 -10 -10                 // 6
61 -67 -67 -67 -67 -67 -67 -67 -67 -67 -67 -67 -67 -67 -67 -68                 // =
68 -124 -124 -124 -124 -124 -124 -124 -124 -124 -124 -124 -124 -124 -124 -126  // D
75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 72                                // K
82 18 18 18 18 18 18 18 18 18 18 18 18 18 18 14                                // R
88 -40 -40 -40 -40 -40 -40 -40 -40 -40 -40 -40 -40 -40 -40 -44                 // X
95 -97 -97 -97 -97 -97 -97 -97 -97 -97 -97 -97 -97 -97 -97 -102                // _
102 102 102 102 102 102 102 102 102 102 102 102 102 102 102 102                // f
    </pre><p>
    ... (note:  the lead byte is listed to the right as a comment.)  Given that the first split is a '0' and the last split is an 'f',
    everything is great, right?  Not so fast.
    </p><p>The problem is that all the data is going to pile up in the first 2 regions and the last region thus creating a "lumpy" (and
    possibly "hot") region problem.  To understand why, refer to an  <a class="link" href="http://www.asciitable.com" target="_top">ASCII Table</a>.
    '0' is byte 48, and 'f' is byte 102, but there is a huge gap in byte values (bytes 58 to 96) that will <span class="emphasis"><em>never appear in this
    keyspace</em></span> because the only values are [0-9] and [a-f].  Thus, the middle regions regions will
    never be used.  To make pre-spliting work with this example keyspace, a custom definition of splits (i.e., and not relying on the
    built-in split method) is required.
    </p><p>Lesson #1:  Pre-splitting tables is generally a best practice, but you need to pre-split them in such a way that all the
    regions are accessible in the keyspace.  While this example demonstrated the problem with a hex-key keyspace, the same problem can happen
     with <span class="emphasis"><em>any</em></span> keyspace.  Know your data.
    </p><p>Lesson #2:  While generally not advisable, using hex-keys (and more generally, displayable data) can still work with pre-split
    tables as long as all the created regions are accessible in the keyspace.
    </p><p>To conclude this example, the following is an example of  how appropriate splits can be pre-created for hex-keys:.
	    </p><pre class="programlisting">public static boolean createTable(HBaseAdmin admin, HTableDescriptor table, byte[][] splits)
throws IOException {
  try {
    admin.createTable( table, splits );
    return true;
  } catch (TableExistsException e) {
    logger.info("table " + table.getNameAsString() + " already exists");
    // the table already exists...
    return false;
  }
}

public static byte[][] getHexSplits(String startKey, String endKey, int numRegions) {
  byte[][] splits = new byte[numRegions-1][];
  BigInteger lowestKey = new BigInteger(startKey, 16);
  BigInteger highestKey = new BigInteger(endKey, 16);
  BigInteger range = highestKey.subtract(lowestKey);
  BigInteger regionIncrement = range.divide(BigInteger.valueOf(numRegions));
  lowestKey = lowestKey.add(regionIncrement);
  for(int i=0; i &lt; numRegions-1;i++) {
    BigInteger key = lowestKey.add(regionIncrement.multiply(BigInteger.valueOf(i)));
    byte[] b = String.format("%016x", key).getBytes();
    splits[i] = b;
  }
  return splits;
}</pre></div></div><div class="section" title="6.4.&nbsp; Number of Versions"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="schema.versions"></a>6.4.&nbsp;
  Number of Versions
  </h2></div></div></div><div class="section" title="6.4.1.&nbsp;Maximum Number of Versions"><div class="titlepage"><div><div><h3 class="title"><a name="schema.versions.max"></a>6.4.1.&nbsp;Maximum Number of Versions</h3></div></div></div><p>The maximum number of row versions to store is configured per column
      family via <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HColumnDescriptor.html" target="_top">HColumnDescriptor</a>.
      The default for max versions is 3.
      This is an important parameter because as described in <a class="xref" href="#datamodel" title="Chapter&nbsp;5.&nbsp;Data Model">Chapter&nbsp;5, <i>Data Model</i></a>
      section HBase does <span class="emphasis"><em>not</em></span> overwrite row values, but rather
      stores different values per row by time (and qualifier).  Excess versions are removed during major
      compactions.  The number of max versions may need to be increased or decreased depending on application needs.
      </p><p>It is not recommended setting the number of max versions to an exceedingly high level (e.g., hundreds or more) unless those old values are
      very dear to you because this will greatly increase StoreFile size.
      </p></div><div class="section" title="6.4.2.&nbsp; Minimum Number of Versions"><div class="titlepage"><div><div><h3 class="title"><a name="schema.minversions"></a>6.4.2.&nbsp;
    Minimum Number of Versions
    </h3></div></div></div><p>Like maximum number of row versions, the minimum number of row versions to keep is configured per column
      family via <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HColumnDescriptor.html" target="_top">HColumnDescriptor</a>.
      The default for min versions is 0, which means the feature is disabled.
      The minimum number of row versions parameter is used together with the time-to-live parameter and can be combined with the
      number of row versions parameter to allow configurations such as
      "keep the last T minutes worth of data, at most N versions, <span class="emphasis"><em>but keep at least M versions around</em></span>"
      (where M is the value for minimum number of row versions, M&lt;N).
      This parameter should only be set when time-to-live is enabled for a column family and must be less than the
      number of row versions.
    </p></div></div><div class="section" title="6.5.&nbsp; Supported Datatypes"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="supported.datatypes"></a>6.5.&nbsp;
  Supported Datatypes
  </h2></div></div></div><p>HBase supports a "bytes-in/bytes-out" interface via <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Put.html" target="_top">Put</a> and
  <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Result.html" target="_top">Result</a>, so anything that can be
  converted to an array of bytes can be stored as a value.  Input could be strings, numbers, complex objects, or even images as long as they can rendered as bytes.
  </p><p>There are practical limits to the size of values (e.g., storing 10-50MB objects in HBase would probably be too much to ask);
  search the mailling list for conversations on this topic. All rows in HBase conform to the <a class="xref" href="#datamodel" title="Chapter&nbsp;5.&nbsp;Data Model">Chapter&nbsp;5, <i>Data Model</i></a>, and
  that includes versioning.  Take that into consideration when making your design, as well as block size for the ColumnFamily.
  </p><div class="section" title="6.5.1.&nbsp;Counters"><div class="titlepage"><div><div><h3 class="title"><a name="counters"></a>6.5.1.&nbsp;Counters</h3></div></div></div><p>
      One supported datatype that deserves special mention are "counters" (i.e., the ability to do atomic increments of numbers).  See
      <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html#increment%28org.apache.hadoop.hbase.client.Increment%29" target="_top">Increment</a> in HTable.
      </p><p>Synchronization on counters are done on the RegionServer, not in the client.
      </p></div></div><div class="section" title="6.6.&nbsp;Joins"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="schema.joins"></a>6.6.&nbsp;Joins</h2></div></div></div><p>If you have multiple tables, don't forget to factor in the potential for <a class="xref" href="#joins" title="5.12.&nbsp;Joins">Section&nbsp;5.12, &#8220;Joins&#8221;</a> into the schema design.
    </p></div><div class="section" title="6.7.&nbsp;Time To Live (TTL)"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="ttl"></a>6.7.&nbsp;Time To Live (TTL)</h2></div></div></div><p>ColumnFamilies can set a TTL length in seconds, and HBase will automatically delete rows once the expiration time is reached.
  This applies to <span class="emphasis"><em>all</em></span> versions of a row - even the current one.  The TTL time encoded in the HBase for the row is specified in UTC.
  </p><p>See <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HColumnDescriptor.html" target="_top">HColumnDescriptor</a> for more information.
  </p></div><div class="section" title="6.8.&nbsp; Keeping Deleted Cells"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="cf.keep.deleted"></a>6.8.&nbsp;
  Keeping Deleted Cells
  </h2></div></div></div><p>ColumnFamilies can optionally keep deleted cells. That means deleted cells can still be retrieved with
  <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Get.html" target="_top">Get</a> or
  <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Scan.html" target="_top">Scan</a> operations,
  as long these operations have a time range specified that ends before the timestamp of any delete that would affect the cells.
  This allows for point in time queries even in the presence of deletes.
  </p><p>
  Deleted cells are still subject to TTL and there will never be more than "maximum number of versions" deleted cells.
  A new "raw" scan options returns all deleted rows and the delete markers.
  </p><p>See <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HColumnDescriptor.html" target="_top">HColumnDescriptor</a> for more information.
  </p></div><div class="section" title="6.9.&nbsp; Secondary Indexes and Alternate Query Paths"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="secondary.indexes"></a>6.9.&nbsp;
  Secondary Indexes and Alternate Query Paths
  </h2></div></div></div><p>This section could also be titled "what if my table rowkey looks like <span class="emphasis"><em>this</em></span> but I also want to query my table like <span class="emphasis"><em>that</em></span>."
  A common example on the dist-list is where a row-key is of the format "user-timestamp" but there are reporting requirements on activity across users for certain
  time ranges.  Thus, selecting by user is easy because it is in the lead position of the key, but time is not.
  </p><p>There is no single answer on the best way to handle this because it depends on...
   </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Number of users</li><li class="listitem">Data size and data arrival rate</li><li class="listitem">Flexibility of reporting requirements (e.g., completely ad-hoc date selection vs. pre-configured ranges) </li><li class="listitem">Desired execution speed of query (e.g., 90 seconds may be reasonable to some for an ad-hoc report, whereas it may be too long for others) </li></ul></div><p>
   ... and solutions are also influenced by the size of the cluster and how much processing power you have to throw at the solution.
   Common techniques are in sub-sections below.  This is a comprehensive, but not exhaustive, list of approaches.
  </p><p>It should not be a surprise that secondary indexes require additional cluster space and processing.
  This is precisely what happens in an RDBMS because the act of creating an alternate index requires both space and processing cycles to update.  RBDMS products
  are more advanced in this regard to handle alternative index management out of the box.  However, HBase scales better at larger data volumes, so this is a feature trade-off.
  </p><p>Pay attention to <a class="xref" href="#performance" title="Chapter&nbsp;12.&nbsp;Apache HBase Performance Tuning">Chapter&nbsp;12, <i>Apache HBase Performance Tuning</i></a> when implementing any of these approaches.</p><p>Additionally, see the David Butler response in this dist-list thread <a class="link" href="http://search-hadoop.com/m/nvbiBp2TDP/Stargate%252Bhbase&amp;subj=Stargate+hbase" target="_top">HBase, mail # user - Stargate+hbase</a>
   </p><div class="section" title="6.9.1.&nbsp; Filter Query"><div class="titlepage"><div><div><h3 class="title"><a name="secondary.indexes.filter"></a>6.9.1.&nbsp;
       Filter Query
      </h3></div></div></div><p>Depending on the case, it may be appropriate to use <a class="xref" href="#client.filter" title="9.4.&nbsp;Client Request Filters">Section&nbsp;9.4, &#8220;Client Request Filters&#8221;</a>.  In this case, no secondary index is created.
      However, don't try a full-scan on a large table like this from an application (i.e., single-threaded client).
      </p></div><div class="section" title="6.9.2.&nbsp; Periodic-Update Secondary Index"><div class="titlepage"><div><div><h3 class="title"><a name="secondary.indexes.periodic"></a>6.9.2.&nbsp;
       Periodic-Update Secondary Index
      </h3></div></div></div><p>A secondary index could be created in an other table which is periodically updated via a MapReduce job.  The job could be executed intra-day, but depending on
      load-strategy it could still potentially be out of sync with the main data table.</p><p>See <a class="xref" href="#mapreduce.example.readwrite" title="7.2.2.&nbsp;HBase MapReduce Read/Write Example">Section&nbsp;7.2.2, &#8220;HBase MapReduce Read/Write Example&#8221;</a> for more information.</p></div><div class="section" title="6.9.3.&nbsp; Dual-Write Secondary Index"><div class="titlepage"><div><div><h3 class="title"><a name="secondary.indexes.dualwrite"></a>6.9.3.&nbsp;
       Dual-Write Secondary Index
      </h3></div></div></div><p>Another strategy is to build the secondary index while publishing data to the cluster (e.g., write to data table, write to index table).
      If this is approach is taken after a data table already exists, then bootstrapping will be needed for the secondary index with a MapReduce job (see <a class="xref" href="#secondary.indexes.periodic" title="6.9.2.&nbsp; Periodic-Update Secondary Index">Section&nbsp;6.9.2, &#8220;
       Periodic-Update Secondary Index
      &#8221;</a>).</p></div><div class="section" title="6.9.4.&nbsp; Summary Tables"><div class="titlepage"><div><div><h3 class="title"><a name="secondary.indexes.summary"></a>6.9.4.&nbsp;
       Summary Tables
      </h3></div></div></div><p>Where time-ranges are very wide (e.g., year-long report) and where the data is voluminous, summary tables are a common approach.
      These would be generated with MapReduce jobs into another table.</p><p>See <a class="xref" href="#mapreduce.example.summary" title="7.2.4.&nbsp;HBase MapReduce Summary to HBase Example">Section&nbsp;7.2.4, &#8220;HBase MapReduce Summary to HBase Example&#8221;</a> for more information.</p></div><div class="section" title="6.9.5.&nbsp; Coprocessor Secondary Index"><div class="titlepage"><div><div><h3 class="title"><a name="secondary.indexes.coproc"></a>6.9.5.&nbsp;
       Coprocessor Secondary Index
      </h3></div></div></div><p>Coprocessors act like RDBMS triggers.  These were added in 0.92.  For more information, see <a class="xref" href="#coprocessors" title="9.6.3.&nbsp;Coprocessors">Section&nbsp;9.6.3, &#8220;Coprocessors&#8221;</a>
      </p></div></div><div class="section" title="6.10.&nbsp;Constraints"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="constraints"></a>6.10.&nbsp;Constraints</h2></div></div></div><p>HBase currently supports 'constraints' in traditional (SQL) database parlance. The advised usage for Constraints is in enforcing business rules for attributes in the table (eg. make sure values are in the range 1-10).
    Constraints could also be used to enforce referential integrity, but this is strongly discouraged as it will dramatically decrease the write throughput of the tables where integrity checking is enabled.
    Extensive documentation on using Constraints can be found at: <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/constraint" target="_top">Constraint</a> since version 0.94.
    </p></div><div class="section" title="6.11.&nbsp;Schema Design Case Studies"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="schema.casestudies"></a>6.11.&nbsp;Schema Design Case Studies</h2></div></div></div><p>The following will describe some typical data ingestion use-cases with HBase, and how the rowkey design and construction
   can be approached.  Note:  this is just an illustration of potential approaches, not an exhaustive list. 
   Know your data, and know your processing requirements.
  </p><p>It is highly recommended that you read the rest of the <a class="xref" href="#schema" title="Chapter&nbsp;6.&nbsp;HBase and Schema Design">Chapter&nbsp;6, <i>HBase and Schema Design</i></a> first, before reading
  these case studies.
  </p><p>Thee following case studies are described:    
      </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Log Data / Timeseries Data</li><li class="listitem">Log Data / Timeseries on Steroids</li><li class="listitem">Customer/Order</li><li class="listitem">Tall/Wide/Middle Schema Design</li><li class="listitem">List Data</li></ul></div><p> 
  </p><div class="section" title="6.11.1.&nbsp;Case Study - Log Data and Timeseries Data"><div class="titlepage"><div><div><h3 class="title"><a name="schema.casestudies.log-timeseries"></a>6.11.1.&nbsp;Case Study - Log Data and Timeseries Data</h3></div></div></div><p>Assume that the following data elements are being collected.
        </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Hostname</li><li class="listitem">Timestamp</li><li class="listitem">Log event</li><li class="listitem">Value/message</li></ul></div><p>
        We can store them in an HBase table called LOG_DATA, but what will the rowkey be?  
       From these attributes the rowkey will be some combination of hostname, timestamp, and log-event - but what specifically?        
      </p><div class="section" title="6.11.1.1.&nbsp;Timestamp In The Rowkey Lead Position"><div class="titlepage"><div><div><h4 class="title"><a name="schema.casestudies.log-timeseries.tslead"></a>6.11.1.1.&nbsp;Timestamp In The Rowkey Lead Position</h4></div></div></div><p>The rowkey <code class="code">[timestamp][hostname][log-event]</code> suffers from the monotonically increasing rowkey problem 
        described in <a class="xref" href="#timeseries" title="6.3.1.&nbsp; Monotonically Increasing Row Keys/Timeseries Data">Section&nbsp;6.3.1, &#8220;
    Monotonically Increasing Row Keys/Timeseries Data
    &#8221;</a>.
        </p><p>There is another pattern frequently mentioned in the dist-lists about &#8220;bucketing&#8221; timestamps, by performing a mod operation 
        on the timestamp.  If time-oriented scans are important, this could be a useful approach.  Attention must be paid to the number
        of buckets, because this will require the same number of scans to return results.
</p><pre class="programlisting">
long bucket = timestamp % numBuckets;
</pre><p>
        &#8230; to construct:
</p><pre class="programlisting">
[bucket][timestamp][hostname][log-event]
</pre><p>        
          As stated above, to select data for a particular timerange, a Scan will need to be performed for each bucket.  100 buckets,
          for example, will provide a wide distribution in the keyspace but it will require 100 Scans to obtain data for a single
          timestamp, so there are trade-offs. 
        </p></div><div class="section" title="6.11.1.2.&nbsp;Host In The Rowkey Lead Position"><div class="titlepage"><div><div><h4 class="title"><a name="schema.casestudies.log-timeseries.hostlead"></a>6.11.1.2.&nbsp;Host In The Rowkey Lead Position</h4></div></div></div><p>The rowkey <code class="code">[hostname][log-event][timestamp]</code> is a candidate if there is a large-ish number of hosts to spread
        the writes and reads across the keyspace.  This approach would be useful if scanning by hostname was a priority.
        </p></div><div class="section" title="6.11.1.3.&nbsp;Timestamp, or Reverse Timestamp?"><div class="titlepage"><div><div><h4 class="title"><a name="schema.casestudies.log-timeseries.revts"></a>6.11.1.3.&nbsp;Timestamp, or Reverse Timestamp?</h4></div></div></div><p>If the most important access path is to pull most recent events, then storing the timestamps as reverse-timestamps 
        (e.g., <code class="code">timestamp = Long.MAX_VALUE &#8211; timestamp</code>) will create the property of being able to do a Scan on
        <code class="code">[hostname][log-event]</code> to obtain the quickly obtain the most recently captured events.
        </p><p>Neither approach is wrong, it just depends on what is most appropriate for the situation.
        </p></div><div class="section" title="6.11.1.4.&nbsp;Variangle Length or Fixed Length Rowkeys?"><div class="titlepage"><div><div><h4 class="title"><a name="schema.casestudies.log-timeseries.varkeys"></a>6.11.1.4.&nbsp;Variangle Length or Fixed Length Rowkeys?</h4></div></div></div><p>It is critical to remember that rowkeys are stamped on every column in HBase.  If the hostname is &#8220;a&#8221; and the event type
         is &#8220;e1&#8221; then the resulting rowkey would be quite small.  However, what if the ingested hostname is
          &#8220;myserver1.mycompany.com&#8221; and the event type is &#8220;com.package1.subpackage2.subsubpackage3.ImportantService&#8221;?  
         </p><p>It might make sense to use some substitution in the rowkey.  There are at least two approaches:  hashed and numeric.
         In the Hostname In The Rowkey Lead Position example, it might look like this:
        </p><p>Composite Rowkey With Hashes:  
           </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">[MD5 hash of hostname] = 16 bytes</li><li class="listitem">[MD5 hash of event-type] = 16 bytes</li><li class="listitem">[timestamp] = 8 bytes</li></ul></div><p>
        </p><p>Composite Rowkey With Numeric Substitution: 
        </p><p>For this approach another lookup table would be needed in addition to LOG_DATA, called LOG_TYPES.  
        The rowkey of LOG_TYPES would be:
		  </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">[type]  (e.g., byte indicating hostname vs. event-type)</li><li class="listitem">[bytes]  variable length bytes for raw hostname or event-type.</li></ul></div><p>
        A column for this rowkey could be a long with an assigned number, which could be obtained by using an 
		<a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html#incrementColumnValue%28byte[],%20byte[],%20byte[],%20long%29" target="_top">HBase counter</a>.
        </p><p>So the resulting composite rowkey would be:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">[substituted long for hostname] = 8 bytes</li><li class="listitem">[substituted long for event type] = 8 bytes</li><li class="listitem">[timestamp] = 8 bytes</li></ul></div><p>
		In either the Hash or Numeric substitution approach, the raw values for hostname and event-type can be stored as columns.
        </p></div></div><div class="section" title="6.11.2.&nbsp;Case Study - Log Data and Timeseries Data on Steroids"><div class="titlepage"><div><div><h3 class="title"><a name="schema.casestudies.log-steroids"></a>6.11.2.&nbsp;Case Study - Log Data and Timeseries Data on Steroids</h3></div></div></div><p>This effectively is the OpenTSDB approach.  What OpenTSDB does is re-write data and pack rows into columns for 
        certain time-periods.  For a detailed explanation, see:  <a class="link" href="http://opentsdb.net/schema.html" target="_top">http://opentsdb.net/schema.html</a>, 
        and <a class="link" href="http://www.cloudera.com/content/cloudera/en/resources/library/hbasecon/video-hbasecon-2012-lessons-learned-from-opentsdb.html" target="_top">Lessons Learned from OpenTSDB</a>
	    from HBaseCon2012.
      </p><p>But this is how the general concept works:  data is ingested, for example, in this manner&#8230;
</p><pre class="programlisting">
[hostname][log-event][timestamp1]
[hostname][log-event][timestamp2]
[hostname][log-event][timestamp3]
</pre><p>
       &#8230; with separate rowkeys for each detailed event, but is re-written like this&#8230; 
       </p><p><code class="code">[hostname][log-event][timerange]</code>
       </p><p>&#8230; and each of the above events are converted into columns stored with a time-offset relative to the beginning timerange 
       (e.g., every 5 minutes).  This is obviously a very advanced processing technique, but HBase makes this possible.
      </p></div><div class="section" title="6.11.3.&nbsp;Case Study - Customer/Order"><div class="titlepage"><div><div><h3 class="title"><a name="schema.casestudies.custorder"></a>6.11.3.&nbsp;Case Study - Customer/Order</h3></div></div></div><p>Assume that HBase is used to store customer and order information.  There are two core record-types being ingested:  
        a Customer record type, and Order record type.
      </p><p>The Customer record type would include all the things that you&#8217;d typically expect:
        </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Customer number</li><li class="listitem">Customer name</li><li class="listitem">Address (e.g., city, state, zip)</li><li class="listitem">Phone numbers, etc.</li></ul></div><p>
     </p><p>The Order record type would include things like:
        </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Customer number</li><li class="listitem">Order number</li><li class="listitem">Sales date</li><li class="listitem">A series of nested objects for shipping locations and line-items (see <a class="xref" href="#schema.casestudies.custorder.obj" title="6.11.3.2.&nbsp;Order Object Design">Section&nbsp;6.11.3.2, &#8220;Order Object Design&#8221;</a>
           for details)</li></ul></div><p>
    </p><p>Assuming that the combination of customer number and sales order uniquely identify an order, these two attributes will compose
 the rowkey, and specifically a composite key such as:
    </p><p><code class="code">[customer number][order number]</code>
    </p><p>&#8230; for a ORDER table.  However, there are more design decisions to make:  are the <span class="emphasis"><em>raw</em></span> values the best choices for rowkeys?
    </p><p>The same design questions in the Log Data use-case confront us here.  What is the keyspace of the customer number, and what is the 
format (e.g., numeric?  alphanumeric?) As it is advantageous to use fixed-length keys in HBase, as well as keys that can support a 
reasonable spread in the keyspace, similar options appear:
    </p><p>Composite Rowkey With Hashes:  
      </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">[MD5 of customer number] = 16 bytes</li><li class="listitem">[MD5 of order number] = 16 bytes</li></ul></div><p>
    </p><p>Composite Numeric/Hash Combo Rowkey: 
      </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">[substituted long for customer number] = 8 bytes</li><li class="listitem">[MD5 of order number] = 16 bytes</li></ul></div><p>
     </p><div class="section" title="6.11.3.1.&nbsp;Single Table? Multiple Tables?"><div class="titlepage"><div><div><h4 class="title"><a name="schema.casestudies.custorder.tables"></a>6.11.3.1.&nbsp;Single Table?  Multiple Tables?</h4></div></div></div><p>A traditional design approach would have separate tables for CUSTOMER and SALES.  Another option is to pack multiple 
            record types into a single table (e.g., CUSTOMER++).            
            </p><p>Customer Record Type Rowkey:
              </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">[customer-id]</li><li class="listitem">[type] = type indicating &#8216;1&#8217; for customer record type</li></ul></div><p>
            </p><p>Order Record Type Rowkey:
              </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">[customer-id]</li><li class="listitem">[type] = type indicating &#8216;2&#8217; for order record type</li><li class="listitem">[order]</li></ul></div><p>
            </p><p>The advantage of this particular CUSTOMER++ approach is that organizes many different record-types by customer-id 
            (e.g., a single scan could get you everything about that customer).  The disadvantage is that it&#8217;s not as easy to scan for
            a particular record-type.
            </p></div><div class="section" title="6.11.3.2.&nbsp;Order Object Design"><div class="titlepage"><div><div><h4 class="title"><a name="schema.casestudies.custorder.obj"></a>6.11.3.2.&nbsp;Order Object Design</h4></div></div></div><p>Now we need to address how to model the Order object.  Assume that the class structure is as follows:
</p><pre class="programlisting">
<code class="filename">Order</code>
     <code class="filename">ShippingLocation</code>     (an Order can have multiple ShippingLocations)
          <code class="filename">LineItem</code>               (a ShippingLocation can have multiple LineItems)
</pre><p>
	       ... there are multiple options on storing this data.
	      </p><div class="section" title="6.11.3.2.1.&nbsp;Completely Normalized"><div class="titlepage"><div><div><h5 class="title"><a name="schema.casestudies.custorder.obj.norm"></a>6.11.3.2.1.&nbsp;Completely Normalized</h5></div></div></div><p>With this approach, there would be separate tables for ORDER, SHIPPING_LOCATION, and LINE_ITEM.          
	        </p><p>The ORDER table's rowkey was described above: <a class="xref" href="#schema.casestudies.custorder" title="6.11.3.&nbsp;Case Study - Customer/Order">Section&nbsp;6.11.3, &#8220;Case Study - Customer/Order&#8221;</a>
	        </p><p>The SHIPPING_LOCATION's composite rowkey would be something like this:
	          </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">[order-rowkey]</li><li class="listitem">[shipping location number] (e.g., 1st location, 2nd, etc.)</li></ul></div><p>
	        </p><p>The LINE_ITEM table's composite rowkey would be something like this:
	          </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">[order-rowkey]</li><li class="listitem">[shipping location number] (e.g., 1st location, 2nd, etc.)</li><li class="listitem">[line item number] (e.g., 1st lineitem, 2nd, etc.)</li></ul></div><p>
	        </p><p>Such a normalized model is likely to be the approach with an RDBMS, but that's not your only option with HBase.
	        The cons of such an approach is that to retrieve information about any Order, you will need:
	          </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Get on the ORDER table for the Order</li><li class="listitem">Scan on the SHIPPING_LOCATION table for that order to get the ShippingLocation instances</li><li class="listitem">Scan on the LINE_ITEM for each ShippingLocation</li></ul></div><p>
	          ... granted, this is what an RDBMS would do under the covers anyway, but since there are no joins in HBase
	          you're just more aware of this fact.
	        </p></div><div class="section" title="6.11.3.2.2.&nbsp;Single Table With Record Types"><div class="titlepage"><div><div><h5 class="title"><a name="schema.casestudies.custorder.obj.rectype"></a>6.11.3.2.2.&nbsp;Single Table With Record Types</h5></div></div></div><p>With this approach, there would exist a single table ORDER that would contain 
	        </p><p>The Order rowkey was described above: <a class="xref" href="#schema.casestudies.custorder" title="6.11.3.&nbsp;Case Study - Customer/Order">Section&nbsp;6.11.3, &#8220;Case Study - Customer/Order&#8221;</a>
	          </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">[order-rowkey]</li><li class="listitem">[ORDER record type]</li></ul></div><p>
	        </p><p>The ShippingLocation composite rowkey would be something like this:
	          </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">[order-rowkey]</li><li class="listitem">[SHIPPING record type]</li><li class="listitem">[shipping location number] (e.g., 1st location, 2nd, etc.)</li></ul></div><p>
	        </p><p>The LineItem composite rowkey would be something like this:
	          </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">[order-rowkey]</li><li class="listitem">[LINE record type]</li><li class="listitem">[shipping location number] (e.g., 1st location, 2nd, etc.)</li><li class="listitem">[line item number] (e.g., 1st lineitem, 2nd, etc.)</li></ul></div><p>
	        </p></div><div class="section" title="6.11.3.2.3.&nbsp;Denormalized"><div class="titlepage"><div><div><h5 class="title"><a name="schema.casestudies.custorder.obj.denorm"></a>6.11.3.2.3.&nbsp;Denormalized</h5></div></div></div><p>A variant of the Single Table With Record Types approach is to denormalize and flatten some of the object 
	        hierarchy, such as collapsing the ShippingLocation attributes onto each LineItem instance.
	        </p><p>The LineItem composite rowkey would be something like this:
	          </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">[order-rowkey]</li><li class="listitem">[LINE record type]</li><li class="listitem">[line item number] (e.g., 1st lineitem, 2nd, etc. - care must be taken that there are unique across the entire order)</li></ul></div><p>
	        </p><p>... and the LineItem columns would be something like this:
	          </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">itemNumber</li><li class="listitem">quantity</li><li class="listitem">price</li><li class="listitem">shipToLine1 (denormalized from ShippingLocation)</li><li class="listitem">shipToLine2 (denormalized from ShippingLocation)</li><li class="listitem">shipToCity (denormalized from ShippingLocation)</li><li class="listitem">shipToState (denormalized from ShippingLocation)</li><li class="listitem">shipToZip (denormalized from ShippingLocation)</li></ul></div><p>
	        </p><p>The pros of this approach include a less complex object heirarchy, but one of the cons is that updating gets more 
	        complicated in case any of this information changes.
	        </p></div><div class="section" title="6.11.3.2.4.&nbsp;Object BLOB"><div class="titlepage"><div><div><h5 class="title"><a name="schema.casestudies.custorder.obj.singleobj"></a>6.11.3.2.4.&nbsp;Object BLOB</h5></div></div></div><p>With this approach, the entire Order object graph is treated, in one way or another, as a BLOB.  For example, the 
	        ORDER table's rowkey was described above: <a class="xref" href="#schema.casestudies.custorder" title="6.11.3.&nbsp;Case Study - Customer/Order">Section&nbsp;6.11.3, &#8220;Case Study - Customer/Order&#8221;</a>, and a 
	        single column called "order" would contain an object that could be deserialized that contained a container Order, 
	        ShippingLocations, and LineItems.
	        </p><p>There are many options here:  JSON, XML, Java Serialization, Avro, Hadoop Writables, etc.  All of them are variants
	        of the same approach:  encode the object graph to a byte-array.  Care should be taken with this approach to ensure backward 
	        compatibilty in case the object model changes such that older persisted structures can still be read back out of HBase.
	        </p><p>Pros are being able to manage complex object graphs with minimal I/O (e.g., a single HBase Get per
	        Order in this example), but the cons include the aforementioned warning about backward compatiblity of serialization,
	        language dependencies of serialization (e.g., Java Serialization only works with Java clients), the fact that
	        you have to deserialize the entire object to get any piece of information inside the BLOB, and the difficulty in 
	        getting frameworks like Hive to work with custom objects like this.
	        </p></div></div></div><div class="section" title="6.11.4.&nbsp;Case Study - &#34;Tall/Wide/Middle&#34; Schema Design Smackdown"><div class="titlepage"><div><div><h3 class="title"><a name="schema.smackdown"></a>6.11.4.&nbsp;Case Study - "Tall/Wide/Middle" Schema Design Smackdown</h3></div></div></div><p>This section will describe additional schema design questions that appear on the dist-list, specifically about
	  tall and wide tables.  These are general guidelines and not laws - each application must consider its own needs.
	  </p><div class="section" title="6.11.4.1.&nbsp;Rows vs. Versions"><div class="titlepage"><div><div><h4 class="title"><a name="schema.smackdown.rowsversions"></a>6.11.4.1.&nbsp;Rows vs. Versions</h4></div></div></div><p>A common question is whether one should prefer rows or HBase's built-in-versioning.  The context is typically where there are
	    "a lot" of versions of a row to be retained (e.g., where it is significantly above the HBase default of 1 max versions).  The
	    rows-approach would require storing a timstamp in some portion of the rowkey so that they would not overwite with each successive update.
	    </p><p>Preference:  Rows (generally speaking).
	    </p></div><div class="section" title="6.11.4.2.&nbsp;Rows vs. Columns"><div class="titlepage"><div><div><h4 class="title"><a name="schema.smackdown.rowscols"></a>6.11.4.2.&nbsp;Rows vs. Columns</h4></div></div></div><p>Another common question is whether one should prefer rows or columns.  The context is typically in extreme cases of wide
	    tables, such as having 1 row with 1 million attributes, or 1 million rows with 1 columns apiece.
	    </p><p>Preference:  Rows (generally speaking).  To be clear, this guideline is in the context is in extremely wide cases, not in the
	    standard use-case where one needs to store a few dozen or hundred columns.  But there is also a middle path between these two
	    options, and that is "Rows as Columns."
	    </p></div><div class="section" title="6.11.4.3.&nbsp;Rows as Columns"><div class="titlepage"><div><div><h4 class="title"><a name="schema.smackdown.rowsascols"></a>6.11.4.3.&nbsp;Rows as Columns</h4></div></div></div><p>The middle path between Rows vs. Columns is packing data that would be a separate row into columns, for certain rows.
	    OpenTSDB is the best example of this case where a single row represents a defined time-range, and then discrete events are treated as
	    columns.  This approach is often more complex, and may require the additional complexity of re-writing your data, but has the
	    advantage of being I/O efficient.  For an overview of this approach, see
	    <a class="xref" href="#">???</a>.
	    </p></div></div><div class="section" title="6.11.5.&nbsp;Case Study - List Data"><div class="titlepage"><div><div><h3 class="title"><a name="casestudies.schema.listdata"></a>6.11.5.&nbsp;Case Study - List Data</h3></div></div></div><p>The following is an exchange from the user dist-list regarding a fairly common question:  
    		how to handle per-user list data in Apache HBase. 
    		</p><p>*** QUESTION ***</p><p>
    		We're looking at how to store a large amount of (per-user) list data in
HBase, and we were trying to figure out what kind of access pattern made
the most sense.  One option is store the majority of the data in a key, so
we could have something like:
    		</p><pre class="programlisting">
&lt;FixedWidthUserName&gt;&lt;FixedWidthValueId1&gt;:"" (no value)
&lt;FixedWidthUserName&gt;&lt;FixedWidthValueId2&gt;:"" (no value)
&lt;FixedWidthUserName&gt;&lt;FixedWidthValueId3&gt;:"" (no value)
			</pre>

The other option we had was to do this entirely using:
    		<pre class="programlisting">
&lt;FixedWidthUserName&gt;&lt;FixedWidthPageNum0&gt;:&lt;FixedWidthLength&gt;&lt;FixedIdNextPageNum&gt;&lt;ValueId1&gt;&lt;ValueId2&gt;&lt;ValueId3&gt;...
&lt;FixedWidthUserName&gt;&lt;FixedWidthPageNum1&gt;:&lt;FixedWidthLength&gt;&lt;FixedIdNextPageNum&gt;&lt;ValueId1&gt;&lt;ValueId2&gt;&lt;ValueId3&gt;...
    		</pre><p>
where each row would contain multiple values.
So in one case reading the first thirty values would be:
			</p><pre class="programlisting">
scan { STARTROW =&gt; 'FixedWidthUsername' LIMIT =&gt; 30}
    		</pre>
And in the second case it would be
    		<pre class="programlisting">
get 'FixedWidthUserName\x00\x00\x00\x00'
    		</pre><p>
The general usage pattern would be to read only the first 30 values of
these lists, with infrequent access reading deeper into the lists.  Some
users would have &lt;= 30 total values in these lists, and some users would
have millions (i.e. power-law distribution)
			</p><p>
 The single-value format seems like it would take up more space on HBase,
but would offer some improved retrieval / pagination flexibility.  Would
there be any significant performance advantages to be able to paginate via
gets vs paginating with scans?
			</p><p>
  My initial understanding was that doing a scan should be faster if our
paging size is unknown (and caching is set appropriately), but that gets
should be faster if we'll always need the same page size.  I've ended up
hearing different people tell me opposite things about performance.  I
assume the page sizes would be relatively consistent, so for most use cases
we could guarantee that we only wanted one page of data in the
fixed-page-length case.  I would also assume that we would have infrequent
updates, but may have inserts into the middle of these lists (meaning we'd
need to update all subsequent rows).
			</p><p>
Thanks for help / suggestions / follow-up questions.
			</p><p>*** ANSWER ***</p><p>
If I understand you correctly, you're ultimately trying to store
triples in the form "user, valueid, value", right? E.g., something
like:
			</p><pre class="programlisting">
"user123, firstname, Paul",
"user234, lastname, Smith"
			</pre><p>
(But the usernames are fixed width, and the valueids are fixed width).
			</p><p>
And, your access pattern is along the lines of: "for user X, list the
next 30 values, starting with valueid Y". Is that right? And these
values should be returned sorted by valueid?
			</p><p>
The tl;dr version is that you should probably go with one row per
user+value, and not build a complicated intra-row pagination scheme on
your own unless you're really sure it is needed.
			</p><p>
Your two options mirror a common question people have when designing
HBase schemas: should I go "tall" or "wide"? Your first schema is
"tall": each row represents one value for one user, and so there are
many rows in the table for each user; the row key is user + valueid,
and there would be (presumably) a single column qualifier that means
"the value". This is great if you want to scan over rows in sorted
order by row key (thus my question above, about whether these ids are
sorted correctly). You can start a scan at any user+valueid, read the
next 30, and be done. What you're giving up is the ability to have
transactional guarantees around all the rows for one user, but it
doesn't sound like you need that. Doing it this way is generally
recommended (see
here <a class="link" href="http://hbase.apache.org/book.html#schema.smackdown" target="_top">http://hbase.apache.org/book.html#schema.smackdown</a>).
			</p><p>
Your second option is "wide": you store a bunch of values in one row,
using different qualifiers (where the qualifier is the valueid). The
simple way to do that would be to just store ALL values for one user
in a single row. I'm guessing you jumped to the "paginated" version
because you're assuming that storing millions of columns in a single
row would be bad for performance, which may or may not be true; as
long as you're not trying to do too much in a single request, or do
things like scanning over and returning all of the cells in the row,
it shouldn't be fundamentally worse. The client has methods that allow
you to get specific slices of columns.
			</p><p>
Note that neither case fundamentally uses more disk space than the
other; you're just "shifting" part of the identifying information for
a value either to the left (into the row key, in option one) or to the
right (into the column qualifiers in option 2). Under the covers,
every key/value still stores the whole row key, and column family
name. (If this is a bit confusing, take an hour and watch Lars
George's excellent video about understanding HBase schema design:
<a class="link" href="http://www.youtube.com/watch?v=_HLoH_PgrLk)" target="_top">http://www.youtube.com/watch?v=_HLoH_PgrLk)</a>.
			</p><p>
A manually paginated version has lots more complexities, as you note,
like having to keep track of how many things are in each page,
re-shuffling if new values are inserted, etc. That seems significantly
more complex. It might have some slight speed advantages (or
disadvantages!) at extremely high throughput, and the only way to
really know that would be to try it out. If you don't have time to
build it both ways and compare, my advice would be to start with the
simplest option (one row per user+value). Start simple and iterate! :)
			</p></div></div><div class="section" title="6.12.&nbsp;Operational and Performance Configuration Options"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="schema.ops"></a>6.12.&nbsp;Operational and Performance Configuration Options</h2></div></div></div><p>See the Performance section <a class="xref" href="#perf.schema" title="12.6.&nbsp;Schema Design">Section&nbsp;12.6, &#8220;Schema Design&#8221;</a> for more information operational and performance
    schema design options, such as Bloom Filters, Table-configured regionsizes, compression, and blocksizes.
    </p></div></div><div class="chapter" title="Chapter&nbsp;7.&nbsp;HBase and MapReduce"><div class="titlepage"><div><div><h2 class="title"><a name="mapreduce"></a>Chapter&nbsp;7.&nbsp;HBase and MapReduce</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#splitter">7.1. Map-Task Splitting</a></span></dt><dd><dl><dt><span class="section"><a href="#splitter.default">7.1.1. The Default HBase MapReduce Splitter</a></span></dt><dt><span class="section"><a href="#splitter.custom">7.1.2. Custom Splitters</a></span></dt></dl></dd><dt><span class="section"><a href="#mapreduce.example">7.2. HBase MapReduce Examples</a></span></dt><dd><dl><dt><span class="section"><a href="#mapreduce.example.read">7.2.1. HBase MapReduce Read Example</a></span></dt><dt><span class="section"><a href="#mapreduce.example.readwrite">7.2.2. HBase MapReduce Read/Write Example</a></span></dt><dt><span class="section"><a href="#mapreduce.example.readwrite.multi">7.2.3. HBase MapReduce Read/Write Example With Multi-Table Output</a></span></dt><dt><span class="section"><a href="#mapreduce.example.summary">7.2.4. HBase MapReduce Summary to HBase Example</a></span></dt><dt><span class="section"><a href="#mapreduce.example.summary.file">7.2.5. HBase MapReduce Summary to File Example</a></span></dt><dt><span class="section"><a href="#mapreduce.example.summary.noreducer">7.2.6. HBase MapReduce Summary to HBase Without Reducer</a></span></dt><dt><span class="section"><a href="#mapreduce.example.summary.rdbms">7.2.7. HBase MapReduce Summary to RDBMS</a></span></dt></dl></dd><dt><span class="section"><a href="#mapreduce.htable.access">7.3. Accessing Other HBase Tables in a MapReduce Job</a></span></dt><dt><span class="section"><a href="#mapreduce.specex">7.4. Speculative Execution</a></span></dt></dl></div><p>See <a class="link" href="http://hbase.org/apidocs/org/apache/hadoop/hbase/mapreduce/package-summary.html#package_description" target="_top">
  HBase and MapReduce</a> up in javadocs.
  Start there.  Below is some additional help.</p><p>For more information about MapReduce (i.e., the framework in general), see the Hadoop site (TODO: Need good links here --
      we used to have some but they rotted against apache hadoop).</p><div class="caution" title="Notice to Mapreduce users of HBase 0.96.1 and above" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Notice to Mapreduce users of HBase 0.96.1 and above</h3><p>Some mapreduce jobs that use HBase fail to launch. The symptom is an
    exception similar to the following:
    </p><pre class="programlisting">
Exception in thread "main" java.lang.IllegalAccessError: class
    com.google.protobuf.ZeroCopyLiteralByteString cannot access its superclass
    com.google.protobuf.LiteralByteString
    at java.lang.ClassLoader.defineClass1(Native Method)
    at java.lang.ClassLoader.defineClass(ClassLoader.java:792)
    at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
    at java.net.URLClassLoader.defineClass(URLClassLoader.java:449)
    at java.net.URLClassLoader.access$100(URLClassLoader.java:71)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:361)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    at
    org.apache.hadoop.hbase.protobuf.ProtobufUtil.toScan(ProtobufUtil.java:818)
    at
    org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.convertScanToString(TableMapReduceUtil.java:433)
    at
    org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableMapperJob(TableMapReduceUtil.java:186)
    at
    org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableMapperJob(TableMapReduceUtil.java:147)
    at
    org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableMapperJob(TableMapReduceUtil.java:270)
    at
    org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableMapperJob(TableMapReduceUtil.java:100)
...
</pre><p>
    This is because of an optimization introduced in <a class="link" href="https://issues.apache.org/jira/browse/HBASE-9867" target="_top">HBASE-9867</a>
    that inadvertently introduced a classloader dependency.
    </p><p>This affects both jobs using the <code class="code">-libjars</code> option and
    "fat jar," those which package their runtime dependencies in a nested
    <code class="code">lib</code> folder.</p><p>In order to satisfy the new classloader requirements,
    hbase-protocol.jar must be included in Hadoop's classpath. This can be
    resolved system-wide by including a reference to the hbase-protocol.jar in
    hadoop's lib directory, via a symlink or by copying the jar into the new
    location.</p><p>This can also be achieved on a per-job launch basis by including it
    in the <code class="code">HADOOP_CLASSPATH</code> environment variable at job submission
    time. When launching jobs that package their dependencies, all three of the
    following job launching commands satisfy this requirement:</p><pre class="programlisting">
$ HADOOP_CLASSPATH=/path/to/hbase-protocol.jar:/path/to/hbase/conf hadoop jar MyJob.jar MyJobMainClass
$ HADOOP_CLASSPATH=$(hbase mapredcp):/path/to/hbase/conf hadoop jar MyJob.jar MyJobMainClass
$ HADOOP_CLASSPATH=$(hbase classpath) hadoop jar MyJob.jar MyJobMainClass
</pre><p>For jars that do not package their dependencies, the following command
  structure is necessary:</p><pre class="programlisting">
$ HADOOP_CLASSPATH=$(hbase mapredcp):/etc/hbase/conf hadoop jar MyApp.jar MyJobMainClass -libjars $(hbase mapredcp | tr ':' ',') ...
</pre><p>See also <a class="link" href="https://issues.apache.org/jira/browse/HBASE-10304" target="_top">HBASE-10304</a>
  for further discussion of this issue.</p></div><div class="section" title="7.1.&nbsp;Map-Task Splitting"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="splitter"></a>7.1.&nbsp;Map-Task Splitting</h2></div></div></div><div class="section" title="7.1.1.&nbsp;The Default HBase MapReduce Splitter"><div class="titlepage"><div><div><h3 class="title"><a name="splitter.default"></a>7.1.1.&nbsp;The Default HBase MapReduce Splitter</h3></div></div></div><p>When <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/TableInputFormat.html" target="_top">TableInputFormat</a>
    is used to source an HBase table in a MapReduce job,
    its splitter will make a map task for each region of the table.
    Thus, if there are 100 regions in the table, there will be
    100 map-tasks for the job - regardless of how many column families are selected in the Scan.</p></div><div class="section" title="7.1.2.&nbsp;Custom Splitters"><div class="titlepage"><div><div><h3 class="title"><a name="splitter.custom"></a>7.1.2.&nbsp;Custom Splitters</h3></div></div></div><p>For those interested in implementing custom splitters, see the method <code class="code">getSplits</code> in
    <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.html" target="_top">TableInputFormatBase</a>.
    That is where the logic for map-task assignment resides.
    </p></div></div><div class="section" title="7.2.&nbsp;HBase MapReduce Examples"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="mapreduce.example"></a>7.2.&nbsp;HBase MapReduce Examples</h2></div></div></div><div class="section" title="7.2.1.&nbsp;HBase MapReduce Read Example"><div class="titlepage"><div><div><h3 class="title"><a name="mapreduce.example.read"></a>7.2.1.&nbsp;HBase MapReduce Read Example</h3></div></div></div><p>The following is an example of using HBase as a MapReduce source in read-only manner.  Specifically,
    there is a Mapper instance but no Reducer, and nothing is being emitted from the Mapper.  There job would be defined
    as follows...
	</p><pre class="programlisting">
Configuration config = HBaseConfiguration.create();
Job job = new Job(config, "ExampleRead");
job.setJarByClass(MyReadJob.class);     // class that contains mapper

Scan scan = new Scan();
scan.setCaching(500);        // 1 is the default in Scan, which will be bad for MapReduce jobs
scan.setCacheBlocks(false);  // don't set to true for MR jobs
// set other scan attrs
...

TableMapReduceUtil.initTableMapperJob(
  tableName,        // input HBase table name
  scan,             // Scan instance to control CF and attribute selection
  MyMapper.class,   // mapper
  null,             // mapper output key
  null,             // mapper output value
  job);
job.setOutputFormatClass(NullOutputFormat.class);   // because we aren't emitting anything from mapper

boolean b = job.waitForCompletion(true);
if (!b) {
  throw new IOException("error with job!");
}
  </pre><p>
  ...and the mapper instance would extend <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/TableMapper.html" target="_top">TableMapper</a>...
	</p><pre class="programlisting">
public static class MyMapper extends TableMapper&lt;Text, Text&gt; {

  public void map(ImmutableBytesWritable row, Result value, Context context) throws InterruptedException, IOException {
    // process data for the row from the Result instance.
   }
}
    </pre><p>
  	  </p></div><div class="section" title="7.2.2.&nbsp;HBase MapReduce Read/Write Example"><div class="titlepage"><div><div><h3 class="title"><a name="mapreduce.example.readwrite"></a>7.2.2.&nbsp;HBase MapReduce Read/Write Example</h3></div></div></div><p>The following is an example of using HBase both as a source and as a sink with MapReduce.
    This example will simply copy data from one table to another.
    </p><pre class="programlisting">
Configuration config = HBaseConfiguration.create();
Job job = new Job(config,"ExampleReadWrite");
job.setJarByClass(MyReadWriteJob.class);    // class that contains mapper

Scan scan = new Scan();
scan.setCaching(500);        // 1 is the default in Scan, which will be bad for MapReduce jobs
scan.setCacheBlocks(false);  // don't set to true for MR jobs
// set other scan attrs

TableMapReduceUtil.initTableMapperJob(
	sourceTable,      // input table
	scan,	          // Scan instance to control CF and attribute selection
	MyMapper.class,   // mapper class
	null,	          // mapper output key
	null,	          // mapper output value
	job);
TableMapReduceUtil.initTableReducerJob(
	targetTable,      // output table
	null,             // reducer class
	job);
job.setNumReduceTasks(0);

boolean b = job.waitForCompletion(true);
if (!b) {
    throw new IOException("error with job!");
}
    </pre><p>
	An explanation is required of what <code class="classname">TableMapReduceUtil</code> is doing, especially with the reducer.
	<a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/TableOutputFormat.html" target="_top">TableOutputFormat</a> is being used
	as the outputFormat class, and several parameters are being set on the config (e.g., TableOutputFormat.OUTPUT_TABLE), as
	well as setting the reducer output key to <code class="classname">ImmutableBytesWritable</code> and reducer value to <code class="classname">Writable</code>.
	These could be set by the programmer on the job and conf, but <code class="classname">TableMapReduceUtil</code> tries to make things easier.
	</p><p>The following is the example mapper, which will create a <code class="classname">Put</code> and matching the input <code class="classname">Result</code>
	and emit it.  Note:  this is what the CopyTable utility does.
	</p><p>
    </p><pre class="programlisting">
public static class MyMapper extends TableMapper&lt;ImmutableBytesWritable, Put&gt;  {

	public void map(ImmutableBytesWritable row, Result value, Context context) throws IOException, InterruptedException {
		// this example is just copying the data from the source table...
   		context.write(row, resultToPut(row,value));
   	}

  	private static Put resultToPut(ImmutableBytesWritable key, Result result) throws IOException {
  		Put put = new Put(key.get());
 		for (KeyValue kv : result.raw()) {
			put.add(kv);
		}
		return put;
   	}
}
    </pre><p>
    </p><p>There isn't actually a reducer step, so <code class="classname">TableOutputFormat</code> takes care of sending the <code class="classname">Put</code>
    to the target table.
    </p><p>
    </p><p>This is just an example, developers could choose not to use <code class="classname">TableOutputFormat</code> and connect to the
    target table themselves.
    </p><p>
    </p></div><div class="section" title="7.2.3.&nbsp;HBase MapReduce Read/Write Example With Multi-Table Output"><div class="titlepage"><div><div><h3 class="title"><a name="mapreduce.example.readwrite.multi"></a>7.2.3.&nbsp;HBase MapReduce Read/Write Example With Multi-Table Output</h3></div></div></div><p>TODO:  example for <code class="classname">MultiTableOutputFormat</code>.
    </p></div><div class="section" title="7.2.4.&nbsp;HBase MapReduce Summary to HBase Example"><div class="titlepage"><div><div><h3 class="title"><a name="mapreduce.example.summary"></a>7.2.4.&nbsp;HBase MapReduce Summary to HBase Example</h3></div></div></div><p>The following example uses HBase as a MapReduce source and sink with a summarization step.  This example will
    count the number of distinct instances of a value in a table and write those summarized counts in another table.
    </p><pre class="programlisting">
Configuration config = HBaseConfiguration.create();
Job job = new Job(config,"ExampleSummary");
job.setJarByClass(MySummaryJob.class);     // class that contains mapper and reducer

Scan scan = new Scan();
scan.setCaching(500);        // 1 is the default in Scan, which will be bad for MapReduce jobs
scan.setCacheBlocks(false);  // don't set to true for MR jobs
// set other scan attrs

TableMapReduceUtil.initTableMapperJob(
	sourceTable,        // input table
	scan,               // Scan instance to control CF and attribute selection
	MyMapper.class,     // mapper class
	Text.class,         // mapper output key
	IntWritable.class,  // mapper output value
	job);
TableMapReduceUtil.initTableReducerJob(
	targetTable,        // output table
	MyTableReducer.class,    // reducer class
	job);
job.setNumReduceTasks(1);   // at least one, adjust as required

boolean b = job.waitForCompletion(true);
if (!b) {
	throw new IOException("error with job!");
}
    </pre><p>
    In this example mapper a column with a String-value is chosen as the value to summarize upon.
    This value is used as the key to emit from the mapper, and an <code class="classname">IntWritable</code> represents an instance counter.
    </p><pre class="programlisting">
public static class MyMapper extends TableMapper&lt;Text, IntWritable&gt;  {
	public static final byte[] CF = "cf".getBytes();
	public static final byte[] ATTR1 = "attr1".getBytes();

	private final IntWritable ONE = new IntWritable(1);
   	private Text text = new Text();

   	public void map(ImmutableBytesWritable row, Result value, Context context) throws IOException, InterruptedException {
        	String val = new String(value.getValue(CF, ATTR1));
          	text.set(val);     // we can only emit Writables...

        	context.write(text, ONE);
   	}
}
    </pre><p>
    In the reducer, the "ones" are counted (just like any other MR example that does this), and then emits a <code class="classname">Put</code>.
    </p><pre class="programlisting">
public static class MyTableReducer extends TableReducer&lt;Text, IntWritable, ImmutableBytesWritable&gt;  {
	public static final byte[] CF = "cf".getBytes();
	public static final byte[] COUNT = "count".getBytes();

 	public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {
    		int i = 0;
    		for (IntWritable val : values) {
    			i += val.get();
    		}
    		Put put = new Put(Bytes.toBytes(key.toString()));
    		put.add(CF, COUNT, Bytes.toBytes(i));

    		context.write(null, put);
   	}
}
    </pre><p>
    </p></div><div class="section" title="7.2.5.&nbsp;HBase MapReduce Summary to File Example"><div class="titlepage"><div><div><h3 class="title"><a name="mapreduce.example.summary.file"></a>7.2.5.&nbsp;HBase MapReduce Summary to File Example</h3></div></div></div><p>This very similar to the summary example above, with exception that this is using HBase as a MapReduce source
       but HDFS as the sink.  The differences are in the job setup and in the reducer.  The mapper remains the same.
       </p><pre class="programlisting">
Configuration config = HBaseConfiguration.create();
Job job = new Job(config,"ExampleSummaryToFile");
job.setJarByClass(MySummaryFileJob.class);     // class that contains mapper and reducer

Scan scan = new Scan();
scan.setCaching(500);        // 1 is the default in Scan, which will be bad for MapReduce jobs
scan.setCacheBlocks(false);  // don't set to true for MR jobs
// set other scan attrs

TableMapReduceUtil.initTableMapperJob(
	sourceTable,        // input table
	scan,               // Scan instance to control CF and attribute selection
	MyMapper.class,     // mapper class
	Text.class,         // mapper output key
	IntWritable.class,  // mapper output value
	job);
job.setReducerClass(MyReducer.class);    // reducer class
job.setNumReduceTasks(1);    // at least one, adjust as required
FileOutputFormat.setOutputPath(job, new Path("/tmp/mr/mySummaryFile"));  // adjust directories as required

boolean b = job.waitForCompletion(true);
if (!b) {
	throw new IOException("error with job!");
}
    </pre>
    As stated above, the previous Mapper can run unchanged with this example.
    As for the Reducer, it is a "generic" Reducer instead of extending TableMapper and emitting Puts.
    <pre class="programlisting">
 public static class MyReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;  {

	public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {
		int i = 0;
		for (IntWritable val : values) {
			i += val.get();
		}
		context.write(key, new IntWritable(i));
	}
}
    </pre></div><div class="section" title="7.2.6.&nbsp;HBase MapReduce Summary to HBase Without Reducer"><div class="titlepage"><div><div><h3 class="title"><a name="mapreduce.example.summary.noreducer"></a>7.2.6.&nbsp;HBase MapReduce Summary to HBase Without Reducer</h3></div></div></div><p>It is also possible to perform summaries without a reducer - if you use HBase as the reducer.
       </p><p>An HBase target table would need to exist for the job summary.  The HTable method <code class="code">incrementColumnValue</code>
       would be used to atomically increment values.  From a performance perspective, it might make sense to keep a Map
       of values with their values to be incremeneted for each map-task, and make one update per key at during the <code class="code">
       cleanup</code> method of the mapper.  However, your milage may vary depending on the number of rows to be processed and
       unique keys.
       </p><p>In the end, the summary results are in HBase.
       </p></div><div class="section" title="7.2.7.&nbsp;HBase MapReduce Summary to RDBMS"><div class="titlepage"><div><div><h3 class="title"><a name="mapreduce.example.summary.rdbms"></a>7.2.7.&nbsp;HBase MapReduce Summary to RDBMS</h3></div></div></div><p>Sometimes it is more appropriate to generate summaries to an RDBMS.  For these cases, it is possible
       to generate summaries directly to an RDBMS via a custom reducer.  The <code class="code">setup</code> method
       can connect to an RDBMS (the connection information can be passed via custom parameters in the context) and the
       cleanup method can close the connection.
       </p><p>It is critical to understand that number of reducers for the job affects the summarization implementation, and
       you'll have to design this into your reducer.  Specifically, whether it is designed to run as a singleton (one reducer)
       or multiple reducers.  Neither is right or wrong, it depends on your use-case.  Recognize that the more reducers that
       are assigned to the job, the more simultaneous connections to the RDBMS will be created - this will scale, but only to a point.
       </p><pre class="programlisting">
 public static class MyRdbmsReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;  {

	private Connection c = null;

	public void setup(Context context) {
  		// create DB connection...
  	}

	public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {
		// do summarization
		// in this example the keys are Text, but this is just an example
	}

	public void cleanup(Context context) {
  		// close db connection
  	}

}
    </pre><p>In the end, the summary results are written to your RDBMS table/s.
       </p></div></div><div class="section" title="7.3.&nbsp;Accessing Other HBase Tables in a MapReduce Job"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="mapreduce.htable.access"></a>7.3.&nbsp;Accessing Other HBase Tables in a MapReduce Job</h2></div></div></div><p>Although the framework currently allows one HBase table as input to a
    MapReduce job, other HBase tables can
	be accessed as lookup tables, etc., in a
    MapReduce job via creating an HTable instance in the setup method of the Mapper.
	</p><pre class="programlisting">public class MyMapper extends TableMapper&lt;Text, LongWritable&gt; {
  private HTable myOtherTable;

  public void setup(Context context) {
    myOtherTable = new HTable("myOtherTable");
  }

  public void map(ImmutableBytesWritable row, Result value, Context context) throws IOException, InterruptedException {
	// process Result...
	// use 'myOtherTable' for lookups
  }

  </pre><p>
   </p></div><div class="section" title="7.4.&nbsp;Speculative Execution"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="mapreduce.specex"></a>7.4.&nbsp;Speculative Execution</h2></div></div></div><p>It is generally advisable to turn off speculative execution for
      MapReduce jobs that use HBase as a source.  This can either be done on a
      per-Job basis through properties, on on the entire cluster.  Especially
      for longer running jobs, speculative execution will create duplicate
      map-tasks which will double-write your data to HBase; this is probably
      not what you want.
  </p><p>See <a class="xref" href="#spec.ex" title="2.5.2.9.&nbsp;Speculative Execution">Section&nbsp;2.5.2.9, &#8220;Speculative Execution&#8221;</a> for more information.
  </p></div></div><div class="chapter" title="Chapter&nbsp;8.&nbsp;Secure Apache HBase"><div class="titlepage"><div><div><h2 class="title"><a name="security"></a>Chapter&nbsp;8.&nbsp;Secure Apache HBase</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#hbase.secure.configuration">8.1. Secure Client Access to Apache HBase</a></span></dt><dd><dl><dt><span class="section"><a href="#d366e5475">8.1.1. Prerequisites</a></span></dt><dt><span class="section"><a href="#d366e5516">8.1.2. Server-side Configuration for Secure Operation</a></span></dt><dt><span class="section"><a href="#d366e5528">8.1.3. Client-side Configuration for Secure Operation</a></span></dt><dt><span class="section"><a href="#d366e5567">8.1.4. Client-side Configuration for Secure Operation - Thrift Gateway</a></span></dt><dt><span class="section"><a href="#d366e5599">8.1.5. Client-side Configuration for Secure Operation - REST Gateway</a></span></dt><dt><span class="section"><a href="#d366e5633">8.1.6. REST Gateway Impersonation Configuration</a></span></dt></dl></dd><dt><span class="section"><a href="#hbase.secure.simpleconfiguration">8.2. Simple User Access to Apache HBase</a></span></dt><dd><dl><dt><span class="section"><a href="#d366e5675">8.2.1. Simple Versus Secure Access</a></span></dt></dl></dd><dt><span class="section"><a href="#hbase.tags">8.3. Tags</a></span></dt><dt><span class="section"><a href="#hbase.accesscontrol.configuration">8.4. Access Control</a></span></dt><dd><dl><dt><span class="section"><a href="#d366e5815">8.4.1. Prerequisites</a></span></dt><dt><span class="section"><a href="#d366e5828">8.4.2. Overview</a></span></dt><dt><span class="section"><a href="#d366e5992">8.4.3. Server-side Configuration for Access Control</a></span></dt><dt><span class="section"><a href="#d366e6004">8.4.4. Cell level Access Control using Tags</a></span></dt><dt><span class="section"><a href="#d366e6030">8.4.5. Shell Enhancements for Access Control</a></span></dt></dl></dd><dt><span class="section"><a href="#hbase.secure.bulkload">8.5. Secure Bulk Load</a></span></dt><dt><span class="section"><a href="#hbase.visibility.labels">8.6. Visibility Labels</a></span></dt><dd><dl><dt><span class="section"><a href="#hbase.visibility.label.administration">8.6.1. Visibility Label Administration</a></span></dt><dt><span class="section"><a href="#hbase.visibility.label.configuration">8.6.2. Server  Side Configuration</a></span></dt></dl></dd><dt><span class="section"><a href="#hbase.encryption.server">8.7. Transparent Server Side Encryption</a></span></dt><dd><dl><dt><span class="section"><a href="#hbase.encryption.server.configuration">8.7.1. Configuration</a></span></dt><dt><span class="section"><a href="#hbase.encryption.server.schema">8.7.2. Setting Encryption on a CF</a></span></dt><dt><span class="section"><a href="#hbase.encryption.server.data_key_rotation">8.7.3. Data Key Rotation</a></span></dt><dt><span class="section"><a href="#hbase.encryption.server.master_key_rotation">8.7.4. Master Key Rotation</a></span></dt></dl></dd></dl></div><div class="section" title="8.1.&nbsp;Secure Client Access to Apache HBase"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="hbase.secure.configuration"></a>8.1.&nbsp;Secure Client Access to Apache HBase</h2></div></div></div><p>Newer releases of Apache HBase (&gt;= 0.92) support optional SASL authentication of clients<sup>[<a name="d366e5466" href="#ftn.d366e5466" class="footnote">22</a>]</sup>.</p><p>This describes how to set up Apache HBase and clients for connection to secure HBase resources.</p><div class="section" title="8.1.1.&nbsp;Prerequisites"><div class="titlepage"><div><div><h3 class="title"><a name="d366e5475"></a>8.1.1.&nbsp;Prerequisites</h3></div></div></div><p>
        You need to have a working Kerberos KDC.
    </p><p>
        A HBase configured for secure client access is expected to be running
        on top of a secured HDFS cluster. HBase must be able to authenticate
        to HDFS services. HBase needs Kerberos credentials to interact with
        the Kerberos-enabled HDFS daemons. Authenticating a service should be
        done using a keytab file. The procedure for creating keytabs for HBase
        service is the same as for creating keytabs for Hadoop. Those steps
        are omitted here. Copy the resulting keytab files to wherever HBase
        Master and RegionServer processes are deployed and make them readable
        only to the user account under which the HBase daemons will run.
    </p><p>
        A Kerberos principal has three parts, with the form
        <code class="code">username/fully.qualified.domain.name@YOUR-REALM.COM</code>. We
        recommend using <code class="code">hbase</code> as the username portion.
    </p><p>
        The following is an example of the configuration properties for
        Kerberos operation that must be added to the
        <code class="code">hbase-site.xml</code> file on every server machine in the
        cluster. Required for even the most basic interactions with a
        secure Hadoop configuration, independent of HBase security.
    </p><pre class="programlisting">
      &lt;property&gt;
        &lt;name&gt;hbase.regionserver.kerberos.principal&lt;/name&gt;
        &lt;value&gt;hbase/_HOST@YOUR-REALM.COM&lt;/value&gt;
      &lt;/property&gt;
      &lt;property&gt;
        &lt;name&gt;hbase.regionserver.keytab.file&lt;/name&gt;
        &lt;value&gt;/etc/hbase/conf/keytab.krb5&lt;/value&gt;
      &lt;/property&gt;
      &lt;property&gt;
        &lt;name&gt;hbase.master.kerberos.principal&lt;/name&gt;
        &lt;value&gt;hbase/_HOST@YOUR-REALM.COM&lt;/value&gt;
      &lt;/property&gt;
      &lt;property&gt;
        &lt;name&gt;hbase.master.keytab.file&lt;/name&gt;
        &lt;value&gt;/etc/hbase/conf/keytab.krb5&lt;/value&gt;
      &lt;/property&gt;
    </pre><p>
        Each HBase client user should also be given a Kerberos principal. This
        principal should have a password assigned to it (as opposed to a
        keytab file). The client principal's <code class="code">maxrenewlife</code> should
        be set so that it can be renewed enough times for the HBase client
        process to complete. For example, if a user runs a long-running HBase
        client process that takes at most 3 days, we might create this user's
        principal within <code class="code">kadmin</code> with: <code class="code">addprinc -maxrenewlife
        3days</code>
    </p><p>
        Long running daemons with indefinite lifetimes that require client
        access to HBase can instead be configured to log in from a keytab. For
        each host running such daemons, create a keytab with
        <code class="code">kadmin</code> or <code class="code">kadmin.local</code>. The procedure for
        creating keytabs for HBase service is the same as for creating
        keytabs for Hadoop. Those steps are omitted here. Copy the resulting
        keytab files to where the client daemon will execute and make them
        readable only to the user account under which the daemon will run.
    </p></div><div class="section" title="8.1.2.&nbsp;Server-side Configuration for Secure Operation"><div class="titlepage"><div><div><h3 class="title"><a name="d366e5516"></a>8.1.2.&nbsp;Server-side Configuration for Secure Operation</h3></div></div></div><p>
        Add the following to the <code class="code">hbase-site.xml</code> file on every server machine in the cluster:
    </p><pre class="programlisting">
      &lt;property&gt;
        &lt;name&gt;hbase.security.authentication&lt;/name&gt;
        &lt;value&gt;kerberos&lt;/value&gt;
      &lt;/property&gt;
      &lt;property&gt;
        &lt;name&gt;hbase.security.authorization&lt;/name&gt;
        &lt;value&gt;true&lt;/value&gt;
      &lt;/property&gt;
      &lt;property&gt;
      &lt;name&gt;hbase.coprocessor.region.classes&lt;/name&gt;
        &lt;value&gt;org.apache.hadoop.hbase.security.token.TokenProvider&lt;/value&gt;
      &lt;/property&gt;
    </pre><p>
       A full shutdown and restart of HBase service is required when deploying
       these configuration changes.
    </p></div><div class="section" title="8.1.3.&nbsp;Client-side Configuration for Secure Operation"><div class="titlepage"><div><div><h3 class="title"><a name="d366e5528"></a>8.1.3.&nbsp;Client-side Configuration for Secure Operation</h3></div></div></div><p>
        Add the following to the <code class="code">hbase-site.xml</code> file on every client:
    </p><pre class="programlisting">
      &lt;property&gt;
        &lt;name&gt;hbase.security.authentication&lt;/name&gt;
        &lt;value&gt;kerberos&lt;/value&gt;
      &lt;/property&gt;
    </pre><p>
        The client environment must be logged in to Kerberos from KDC or
        keytab via the <code class="code">kinit</code> command before communication with
        the HBase cluster will be possible.
    </p><p>
        Be advised that if the <code class="code">hbase.security.authentication</code>
        in the client- and server-side site files do not match, the client will
        not be able to communicate with the cluster.
    </p><p>
        Once HBase is configured for secure RPC it is possible to optionally
        configure encrypted communication. To do so, add the following to the
        <code class="code">hbase-site.xml</code> file on every client:
    </p><pre class="programlisting">
      &lt;property&gt;
        &lt;name&gt;hbase.rpc.protection&lt;/name&gt;
        &lt;value&gt;privacy&lt;/value&gt;
      &lt;/property&gt;
    </pre><p>
        This configuration property can also be set on a per connection basis.
        Set it in the <code class="code">Configuration</code> supplied to
        <code class="code">HTable</code>:
    </p><pre class="programlisting">
      Configuration conf = HBaseConfiguration.create();
      conf.set("hbase.rpc.protection", "privacy");
      HTable table = new HTable(conf, tablename);
    </pre><p>
        Expect a ~10% performance penalty for encrypted communication.
    </p></div><div class="section" title="8.1.4.&nbsp;Client-side Configuration for Secure Operation - Thrift Gateway"><div class="titlepage"><div><div><h3 class="title"><a name="d366e5567"></a>8.1.4.&nbsp;Client-side Configuration for Secure Operation - Thrift Gateway</h3></div></div></div><p>
        Add the following to the <code class="code">hbase-site.xml</code> file for every Thrift gateway:
    </p><pre class="programlisting">
    &lt;property&gt;
      &lt;name&gt;hbase.thrift.keytab.file&lt;/name&gt;
      &lt;value&gt;/etc/hbase/conf/hbase.keytab&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
      &lt;name&gt;hbase.thrift.kerberos.principal&lt;/name&gt;
      &lt;value&gt;$USER/_HOST@HADOOP.LOCALDOMAIN&lt;/value&gt;
      &lt;!-- TODO: This may need to be  HTTP/_HOST@&lt;REALM&gt; and _HOST may not work.
       You may have  to put the concrete full hostname.
       --&gt;
    &lt;/property&gt;
    </pre><p>
    </p><p>
        Substitute the appropriate credential and keytab for $USER and $KEYTAB
        respectively.
    </p><p>In order to use the Thrift API principal to interact with HBase, it is also necessary to add the <code class="code">hbase.thrift.kerberos.principal</code> to the <code class="code">_acl_</code> table. For example, to give the Thrift API principal, <code class="code">thrift_server</code>, administrative access, a command such as this one will suffice:
    </p><pre class="programlisting">
    grant 'thrift_server', 'RWCA'
    </pre><p> For more information about ACLs, please see the <a class="link" href="#hbase.accesscontrol.configuration" title="8.4.&nbsp;Access Control">Access Control</a> section
    </p><p>
        The Thrift gateway will authenticate with HBase using the supplied
        credential. No authentication will be performed by the Thrift gateway
        itself. All client access via the Thrift gateway will use the Thrift
        gateway's credential and have its privilege.
    </p></div><div class="section" title="8.1.5.&nbsp;Client-side Configuration for Secure Operation - REST Gateway"><div class="titlepage"><div><div><h3 class="title"><a name="d366e5599"></a>8.1.5.&nbsp;Client-side Configuration for Secure Operation - REST Gateway</h3></div></div></div><p>
        Add the following to the <code class="code">hbase-site.xml</code> file for every REST gateway:
    </p><pre class="programlisting">
    &lt;property&gt;
      &lt;name&gt;hbase.rest.keytab.file&lt;/name&gt;
      &lt;value&gt;$KEYTAB&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
      &lt;name&gt;hbase.rest.kerberos.principal&lt;/name&gt;
      &lt;value&gt;$USER/_HOST@HADOOP.LOCALDOMAIN&lt;/value&gt;
    &lt;/property&gt;
    </pre><p>
    </p><p>
        Substitute the appropriate credential and keytab for $USER and $KEYTAB
        respectively.
    </p><p>
        The REST gateway will authenticate with HBase using the supplied
        credential. No authentication will be performed by the REST gateway
        itself. All client access via the REST gateway will use the REST
        gateway's credential and have its privilege.
    </p><p>In order to use the REST API principal to interact with HBase, it is also necessary to add the <code class="code">hbase.rest.kerberos.principal</code> to the <code class="code">_acl_</code> table. For example, to give the REST API principal, <code class="code">rest_server</code>, administrative access, a command such as this one will suffice:
    </p><pre class="programlisting">
    grant 'rest_server', 'RWCA'
    </pre><p> For more information about ACLs, please see the <a class="link" href="#hbase.accesscontrol.configuration" title="8.4.&nbsp;Access Control">Access Control</a> section
    </p><p>
        It should be possible for clients to authenticate with the HBase
        cluster through the REST gateway in a pass-through manner via SPEGNO
        HTTP authentication. This is future work.
    </p></div><div class="section" title="8.1.6.&nbsp;REST Gateway Impersonation Configuration"><div class="titlepage"><div><div><h3 class="title"><a name="d366e5633"></a>8.1.6.&nbsp;REST Gateway Impersonation Configuration</h3></div></div></div><p>
        By default, the REST gateway doesn't support impersonation. It accesses
        the HBase on behalf of clients as the user configured as in the previous
        section. To the HBase server, all requests are from the REST gateway user.
        The actual users are unknown. You can turn on the impersonation support.
        With impersonation, the REST gateway user is a proxy user. The HBase server
        knows the acutal/real user of each request. So it can apply proper
        authorizations.
    </p><p>
        To turn on REST gateway impersonation, we need to configure HBase servers
        (masters and region servers) to allow proxy users; configure REST gateway
        to enable impersonation.
    </p><p>
        To allow proxy users, add the following to the <code class="code">hbase-site.xml</code>
        file for every HBase server:
    </p><pre class="programlisting">
   &lt;property&gt;
      &lt;name&gt;hadoop.security.authorization&lt;/name&gt;
      &lt;value&gt;true&lt;/value&gt;
   &lt;/property&gt;
   &lt;property&gt;
      &lt;name&gt;hadoop.proxyuser.$USER.groups&lt;/name&gt;
      &lt;value&gt;$GROUPS&lt;/value&gt;
   &lt;/property&gt;
   &lt;property&gt;
      &lt;name&gt;hadoop.proxyuser.$USER.hosts&lt;/name&gt;
      &lt;value&gt;$GROUPS&lt;/value&gt;
   &lt;/property&gt;
    </pre><p>
    </p><p>
        Substitute the REST gateway proxy user for $USER, and the allowed
        group list for $GROUPS.
    </p><p>
        To enable REST gateway impersonation, add the following to the
        <code class="code">hbase-site.xml</code> file for every REST gateway.
    </p><pre class="programlisting">
   &lt;property&gt;
      &lt;name&gt;hbase.rest.authentication.type&lt;/name&gt;
      &lt;value&gt;kerberos&lt;/value&gt;
   &lt;/property&gt;
   &lt;property&gt;
      &lt;name&gt;hbase.rest.authentication.kerberos.principal&lt;/name&gt;
      &lt;value&gt;HTTP/_HOST@HADOOP.LOCALDOMAIN&lt;/value&gt;
   &lt;/property&gt;
   &lt;property&gt;
      &lt;name&gt;hbase.rest.authentication.kerberos.keytab&lt;/name&gt;
      &lt;value&gt;$KEYTAB&lt;/value&gt;
   &lt;/property&gt;
    </pre><p>
    </p><p>
        Substitute the keytab for HTTP for $KEYTAB.
    </p></div></div><div class="section" title="8.2.&nbsp;Simple User Access to Apache HBase"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="hbase.secure.simpleconfiguration"></a>8.2.&nbsp;Simple User Access to Apache HBase</h2></div></div></div><p>Newer releases of Apache HBase (&gt;= 0.92) support optional SASL authentication of clients<sup>[<a name="d366e5666" href="#ftn.d366e5666" class="footnote">23</a>]</sup>.</p><p>This describes how to set up Apache HBase and clients for simple user access to HBase resources.</p><div class="section" title="8.2.1.&nbsp;Simple Versus Secure Access"><div class="titlepage"><div><div><h3 class="title"><a name="d366e5675"></a>8.2.1.&nbsp;Simple Versus Secure Access</h3></div></div></div><p>
        The following section shows how to set up simple user access. Simple user access is
        not a secure method of operating HBase. This method is used to prevent users from making
        mistakes. It can be used to mimic the Access Control using on a development system without having to
        set up Kerberos.
    </p><p>
        This method is not used to prevent malicious or hacking attempts. To make HBase secure against these
        types of attacks, you must configure HBase for secure operation. Refer to the section
        <a class="link" href="#hbase.accesscontrol.configuration" title="8.4.&nbsp;Access Control">Secure Client Access to HBase</a> and complete all of the steps described
        there.
    </p><div class="section" title="8.2.1.1.&nbsp;Prerequisites"><div class="titlepage"><div><div><h4 class="title"><a name="d366e5685"></a>8.2.1.1.&nbsp;Prerequisites</h4></div></div></div><p>
        None
    </p><div class="section" title="8.2.1.1.1.&nbsp;Server-side Configuration for Simple User Access Operation"><div class="titlepage"><div><div><h5 class="title"><a name="d366e5690"></a>8.2.1.1.1.&nbsp;Server-side Configuration for Simple User Access Operation</h5></div></div></div><p>
        Add the following to the <code class="code">hbase-site.xml</code> file on every server machine in the cluster:
    </p><pre class="programlisting">
      &lt;property&gt;
        &lt;name&gt;hbase.security.authentication&lt;/name&gt;
        &lt;value&gt;simple&lt;/value&gt;
      &lt;/property&gt;
      &lt;property&gt;
        &lt;name&gt;hbase.security.authorization&lt;/name&gt;
        &lt;value&gt;true&lt;/value&gt;
      &lt;/property&gt;
      &lt;property&gt;
        &lt;name&gt;hbase.coprocessor.master.classes&lt;/name&gt;
        &lt;value&gt;org.apache.hadoop.hbase.security.access.AccessController&lt;/value&gt;
      &lt;/property&gt;
      &lt;property&gt;
        &lt;name&gt;hbase.coprocessor.region.classes&lt;/name&gt;
        &lt;value&gt;org.apache.hadoop.hbase.security.access.AccessController&lt;/value&gt;
      &lt;/property&gt;
    </pre><p>
        For 0.94, add the following to the <code class="code">hbase-site.xml</code> file on every server machine in the cluster:
    </p><pre class="programlisting">
      &lt;property&gt;
        &lt;name&gt;hbase.rpc.engine&lt;/name&gt;
        &lt;value&gt;org.apache.hadoop.hbase.ipc.SecureRpcEngine&lt;/value&gt;
      &lt;/property&gt;
      &lt;property&gt;
        &lt;name&gt;hbase.coprocessor.master.classes&lt;/name&gt;
        &lt;value&gt;org.apache.hadoop.hbase.security.access.AccessController&lt;/value&gt;
      &lt;/property&gt;
      &lt;property&gt;
        &lt;name&gt;hbase.coprocessor.region.classes&lt;/name&gt;
        &lt;value&gt;org.apache.hadoop.hbase.security.access.AccessController&lt;/value&gt;
      &lt;/property&gt; 
    </pre><p>
       A full shutdown and restart of HBase service is required when deploying
       these configuration changes.
    </p></div><div class="section" title="8.2.1.1.2.&nbsp;Client-side Configuration for Simple User Access Operation"><div class="titlepage"><div><div><h5 class="title"><a name="d366e5709"></a>8.2.1.1.2.&nbsp;Client-side Configuration for Simple User Access Operation</h5></div></div></div><p>
        Add the following to the <code class="code">hbase-site.xml</code> file on every client:
    </p><pre class="programlisting">
      &lt;property&gt;
        &lt;name&gt;hbase.security.authentication&lt;/name&gt;
        &lt;value&gt;simple&lt;/value&gt;
      &lt;/property&gt;
    </pre><p>
        For 0.94, add the following to the <code class="code">hbase-site.xml</code> file on every server machine in the cluster:
    </p><pre class="programlisting">
      &lt;property&gt;
        &lt;name&gt;hbase.rpc.engine&lt;/name&gt;
        &lt;value&gt;org.apache.hadoop.hbase.ipc.SecureRpcEngine&lt;/value&gt;
      &lt;/property&gt;
    </pre><p>
        Be advised that if the <code class="code">hbase.security.authentication</code>
        in the client- and server-side site files do not match, the client will
        not be able to communicate with the cluster.
    </p></div><div class="section" title="8.2.1.1.3.&nbsp;Client-side Configuration for Simple User Access Operation - Thrift Gateway"><div class="titlepage"><div><div><h5 class="title"><a name="d366e5731"></a>8.2.1.1.3.&nbsp;Client-side Configuration for Simple User Access Operation - Thrift Gateway</h5></div></div></div><p>The Thrift gateway user will need access. For example, to give the Thrift API user, <code class="code">thrift_server</code>, administrative access, a command such as this one will suffice:
    </p><pre class="programlisting">
    grant 'thrift_server', 'RWCA'
    </pre><p> For more information about ACLs, please see the <a class="link" href="#hbase.accesscontrol.configuration" title="8.4.&nbsp;Access Control">Access Control</a> section
    </p><p>
        The Thrift gateway will authenticate with HBase using the supplied
        credential. No authentication will be performed by the Thrift gateway
        itself. All client access via the Thrift gateway will use the Thrift
        gateway's credential and have its privilege.
    </p></div><div class="section" title="8.2.1.1.4.&nbsp;Client-side Configuration for Simple User Access Operation - REST Gateway"><div class="titlepage"><div><div><h5 class="title"><a name="d366e5747"></a>8.2.1.1.4.&nbsp;Client-side Configuration for Simple User Access Operation - REST Gateway</h5></div></div></div><p>
        The REST gateway will authenticate with HBase using the supplied
        credential. No authentication will be performed by the REST gateway
        itself. All client access via the REST gateway will use the REST
        gateway's credential and have its privilege.
    </p><p>The REST gateway user will need access. For example, to give the REST API user, <code class="code">rest_server</code>, administrative access, a command such as this one will suffice:
    </p><pre class="programlisting">
    grant 'rest_server', 'RWCA'
    </pre><p> For more information about ACLs, please see the <a class="link" href="#hbase.accesscontrol.configuration" title="8.4.&nbsp;Access Control">Access Control</a> section
    </p><p>
        It should be possible for clients to authenticate with the HBase
        cluster through the REST gateway in a pass-through manner via SPEGNO
        HTTP authentication. This is future work.
    </p></div></div></div></div><div class="section" title="8.3.&nbsp;Tags"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="hbase.tags"></a>8.3.&nbsp;Tags</h2></div></div></div><p>
	Every cell can have metadata associated with it.  Adding metadata in the data part of every cell would make things difficult.
</p><p>
	The 0.98 version of HBase solves this problem by providing Tags along with the cell format. 
	Some of the usecases that uses the tags are Visibility labels, Cell level ACLs, etc.
</p><p>
	HFile V3 version from 0.98 onwards supports tags and this feature can be turned on using the following configuration
</p><pre class="programlisting">
      &lt;property&gt;
	    &lt;name&gt;hfile.format.version&lt;/name&gt;
        &lt;value&gt;3&lt;/value&gt;
      &lt;/property&gt;
    </pre><p>
	Every cell can have zero or more tags. Every tag has a type and the actual tag byte array.
	The types <span class="command"><strong>0-31</strong></span> are reserved for System tags.  For example &#8216;1&#8217; is reserved for ACL and &#8216;2&#8217; is reserved for Visibility tags.
</p><p>
	The way rowkeys, column families, qualifiers and values are encoded using different Encoding Algos, similarly the tags can also be encoded.  
	Tag encoding can be turned on per CF.  Default is always turn ON.
	To turn on the tag encoding on the HFiles use
</p><pre class="programlisting">
    HColumnDescriptor#setCompressTags(boolean compressTags)
    </pre><p>
	Note that encoding of tags takes place only if the DataBlockEncoder is enabled for the CF.
</p><p>
	As we compress the WAL entries using Dictionary the tags present in the WAL can also be compressed using Dictionary.  
	Every tag is compressed individually using WAL Dictionary.  To turn ON tag compression in WAL dictionary enable the property
</p><pre class="programlisting">
    &lt;property&gt;
    	&lt;name&gt;hbase.regionserver.wal.tags.enablecompression&lt;/name&gt;
    	&lt;value&gt;true&lt;/value&gt;
	&lt;/property&gt;
    </pre><p>
	To add tags to every cell during Puts, the following apis are provided
</p><pre class="programlisting">
	Put#add(byte[] family, byte [] qualifier, byte [] value, Tag[] tag)
	Put#add(byte[] family, byte[] qualifier, long ts, byte[] value, Tag[] tag)
    </pre><p>
	Some of the feature developed using tags are Cell level ACLs and Visibility labels.  
	These are some features that use tags framework and allows users to gain better security features on cell level.
</p><p>
	For details checkout 
</p><p>
    <a class="link" href="#hbase.accesscontrol.configuration" title="8.4.&nbsp;Access Control">Access Control</a>
    <a class="link" href="#hbase.visibility.labels" title="8.6.&nbsp;Visibility Labels">Visibility labels</a>
</p></div><div class="section" title="8.4.&nbsp;Access Control"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="hbase.accesscontrol.configuration"></a>8.4.&nbsp;Access Control</h2></div></div></div><p>
        Newer releases of Apache HBase (&gt;= 0.92) support optional access control
        list (ACL-) based protection of resources on a column family and/or
        table basis.
    </p><p>
        This describes how to set up Secure HBase for access control, with an
        example of granting and revoking user permission on table resources
        provided.
    </p><div class="section" title="8.4.1.&nbsp;Prerequisites"><div class="titlepage"><div><div><h3 class="title"><a name="d366e5815"></a>8.4.1.&nbsp;Prerequisites</h3></div></div></div><p>
       You must configure HBase for secure or simple user access operation. Refer to the
       <a class="link" href="#hbase.accesscontrol.configuration" title="8.4.&nbsp;Access Control">Secure Client Access to HBase</a> or
       <a class="link" href="#">Simple User Access to HBase</a>
        sections and complete all of the steps described
       there.
    </p><p>
       For secure access, you must also configure ZooKeeper for secure operation. Changes to ACLs
       are synchronized throughout the cluster using ZooKeeper. Secure
       authentication to ZooKeeper must be enabled or otherwise it will be
       possible to subvert HBase access control via direct client access to
       ZooKeeper. Refer to the section on secure ZooKeeper configuration and
       complete all of the steps described there.
    </p></div><div class="section" title="8.4.2.&nbsp;Overview"><div class="titlepage"><div><div><h3 class="title"><a name="d366e5828"></a>8.4.2.&nbsp;Overview</h3></div></div></div><p>
        With Secure RPC and Access Control enabled, client access to HBase is
        authenticated and user data is private unless access has been
        explicitly granted. Access to data can be granted at a table or per
        column family basis.
    </p><p>
        However, the following items have been left out of the initial
        implementation for simplicity:
    </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Row-level or per value (cell): Using Tags in HFile V3</p></li><li class="listitem"><p>Push down of file ownership to HDFS: HBase is not designed for the case where files may have different permissions than the HBase system principal. Pushing file ownership down into HDFS would necessitate changes to core code. Also, while HDFS file ownership would make applying quotas easy, and possibly make bulk imports more straightforward, it is not clear that it would offer a more secure setup.</p></li><li class="listitem"><p>HBase managed "roles" as collections of permissions: We will not model "roles" internally in HBase to begin with. We instead allow group names to be granted permissions, which allows external modeling of roles via group membership. Groups are created and manipulated externally to HBase, via the Hadoop group mapping service.</p></li></ol></div><p>
Access control mechanisms are mature and fairly standardized in the relational database world. The HBase implementation approximates current convention, but HBase has a simpler feature set than relational databases, especially in terms of client operations. We don't distinguish between an insert (new record) and update (of existing record), for example, as both collapse down into a Put. Accordingly, the important operations condense to four permissions: READ, WRITE, CREATE, and ADMIN.
    </p><div class="table"><a name="d366e5847"></a><p class="title"><b>Table&nbsp;8.1.&nbsp;Operation To Permission Mapping</b></p><div class="table-contents"><table summary="Operation To Permission Mapping" border="1"><colgroup><col align="center" class="c1"><col align="left" class="c2"></colgroup><thead><tr><th align="center">Permission</th><th align="left">Operation</th></tr></thead><tbody><tr><td align="center">Read</td><td align="left">Get</td></tr><tr><td align="center">&nbsp;</td><td align="left">Exists</td></tr><tr><td align="center">&nbsp;</td><td align="left">Scan</td></tr><tr><td align="center">Write</td><td align="left">Put</td></tr><tr><td align="center">&nbsp;</td><td align="left">Delete</td></tr><tr><td align="center">&nbsp;</td><td align="left">Lock/UnlockRow</td></tr><tr><td align="center">&nbsp;</td><td align="left">IncrementColumnValue</td></tr><tr><td align="center">&nbsp;</td><td align="left">CheckAndDelete/Put</td></tr><tr><td align="center">&nbsp;</td><td align="left">Flush</td></tr><tr><td align="center">&nbsp;</td><td align="left">Compact</td></tr><tr><td align="center">Create</td><td align="left">Create</td></tr><tr><td align="center">&nbsp;</td><td align="left">Alter</td></tr><tr><td align="center">&nbsp;</td><td align="left">Drop</td></tr><tr><td align="center">Admin</td><td align="left">Enable/Disable</td></tr><tr><td align="center">&nbsp;</td><td align="left">Snapshot/Restore/Clone</td></tr><tr><td align="center">&nbsp;</td><td align="left">Split</td></tr><tr><td align="center">&nbsp;</td><td align="left">Major Compact</td></tr><tr><td align="center">&nbsp;</td><td align="left">Grant</td></tr><tr><td align="center">&nbsp;</td><td align="left">Revoke</td></tr><tr><td align="center">&nbsp;</td><td align="left">Shutdown</td></tr></tbody></table></div></div><br class="table-break"><p>
        Permissions can be granted in any of the following scopes, though
        CREATE and ADMIN permissions are effective only at table scope.
    </p><p>
    </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p>Table</p><p>
        </p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem"><p>Read: User can read from any column family in table</p></li><li class="listitem"><p>Write: User can write to any column family in table</p></li><li class="listitem"><p>Create: User can alter table attributes; add, alter, or drop column families; and drop the table.</p></li><li class="listitem"><p>Admin: User can alter table attributes; add, alter, or drop column families; and enable, disable, or drop the table. User can also trigger region (re)assignments or relocation.</p></li></ul></div><p>
        </p></li><li class="listitem"><p>Column Family</p><p>
        </p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem"><p>Read: User can read from the column family</p></li><li class="listitem"><p>Write: User can write to the column family</p></li></ul></div><p>
        </p></li></ul></div><p>
    </p><p>
       There is also an implicit global scope for the superuser.
    </p><p>
       The superuser is a principal, specified in the HBase site configuration
       file, that has equivalent access to HBase as the 'root' user would on a
       UNIX derived system. Normally this is the principal that the HBase
       processes themselves authenticate as. Although future versions of HBase
       Access Control may support multiple superusers, the superuser privilege
       will always include the principal used to run the HMaster process. Only
       the superuser is allowed to create tables, switch the balancer on or
       off, or take other actions with global consequence. Furthermore, the
       superuser has an implicit grant of all permissions to all resources.
    </p><p>
       Tables have a new metadata attribute: OWNER, the user principal who owns
       the table. By default this will be set to the user principal who creates
       the table, though it may be changed at table creation time or during an
       alter operation by setting or changing the OWNER table attribute. Only a
       single user principal can own a table at a given time. A table owner will
       have all permissions over a given table.
    </p></div><div class="section" title="8.4.3.&nbsp;Server-side Configuration for Access Control"><div class="titlepage"><div><div><h3 class="title"><a name="d366e5992"></a>8.4.3.&nbsp;Server-side Configuration for Access Control</h3></div></div></div><p>
        Enable the AccessController coprocessor in the cluster configuration
        and restart HBase. The restart can be a rolling one. Complete the
        restart of all Master and RegionServer processes before setting up
        ACLs.
    </p><p>
        To enable the AccessController, modify the <code class="code">hbase-site.xml</code> file on every server machine in the cluster to look like:
    </p><pre class="programlisting">
      &lt;property&gt;
        &lt;name&gt;hbase.coprocessor.master.classes&lt;/name&gt;
        &lt;value&gt;org.apache.hadoop.hbase.security.access.AccessController&lt;/value&gt;
      &lt;/property&gt;
      &lt;property&gt;
      &lt;name&gt;hbase.coprocessor.region.classes&lt;/name&gt;
        &lt;value&gt;org.apache.hadoop.hbase.security.token.TokenProvider,
        org.apache.hadoop.hbase.security.access.AccessController&lt;/value&gt;
      &lt;/property&gt;
    </pre></div><div class="section" title="8.4.4.&nbsp;Cell level Access Control using Tags"><div class="titlepage"><div><div><h3 class="title"><a name="d366e6004"></a>8.4.4.&nbsp;Cell level Access Control using Tags</h3></div></div></div><p>
    	Prior to HBase 0.98 access control was restricted to table and column family level.  Thanks to tags feature in 0.98 that allows Access control on a cell level.
		The existing Access Controller coprocessor helps in achieving cell level access control also.
		For details on configuring it refer to <a class="link" href="#hbase.accesscontrol.configuration" title="8.4.&nbsp;Access Control">Access Control</a> section.
    </p><p>
    	The ACLs can be specified for every mutation using the APIs
    </p><pre class="programlisting">
    	Mutation.setACL(String user, Permission perms)
	  	Mutation.setACL(Map&lt;String, Permission&gt; perms)
    </pre><p>
    	For example, to provide read permission to an user &#8216;user1&#8217; then
    </p><pre class="programlisting">
    	put.setACL(&#8220;user1&#8221;, new Permission(Permission.Action.READ))
    </pre><p>
    	Generally the ACL applied on the table and CF takes precedence over Cell level ACL.  In order to make the cell level ACL to take precedence use the following API,
    </p><pre class="programlisting">
    	Mutation.setACLStrategy(boolean cellFirstStrategy)
    </pre><p>
    	Please note that inorder to use this feature, HFile V3 version should be turned on.
    </p><pre class="programlisting">
   		&lt;property&gt;
			&lt;name&gt;hfile.format.version&lt;/name&gt;
			&lt;value&gt;3&lt;/value&gt;
		&lt;/property&gt;
     </pre><p>
    	Note that deletes with ACLs do not have any effect.
		To keep things simple the ACLs applied on the current Put does not change the ACL of any previous Put in the sense
		that the ACL on the current put does not affect older versions of Put for the same row.
    </p></div><div class="section" title="8.4.5.&nbsp;Shell Enhancements for Access Control"><div class="titlepage"><div><div><h3 class="title"><a name="d366e6030"></a>8.4.5.&nbsp;Shell Enhancements for Access Control</h3></div></div></div><p>
The HBase shell has been extended to provide simple commands for editing and updating user permissions. The following commands have been added for access control list management:
    </p>
    Grant
    <p>
    </p><pre class="programlisting">
    grant &lt;user|@group&gt; &lt;permissions&gt; [ &lt;table&gt; [ &lt;column family&gt; [ &lt;column qualifier&gt; ] ] ]
    </pre><p>
    </p><p>
    <code class="code">&lt;user|@group&gt;</code> is user or group  (start with character '@'), Groups are created and manipulated via the Hadoop group mapping service.
    </p><p>
    <code class="code">&lt;permissions&gt;</code> is zero or more letters from the set "RWCA": READ('R'), WRITE('W'), CREATE('C'), ADMIN('A').
    </p><p>
    Note: Grants and revocations of individual permissions on a resource are both accomplished using the <code class="code">grant</code> command. A separate <code class="code">revoke</code> command is also provided by the shell, but this is for fast revocation of all of a user's access rights to a given resource only.
    </p><p>
    Revoke
    </p><p>
    </p><pre class="programlisting">
    revoke &lt;user|@group&gt; [ &lt;table&gt; [ &lt;column family&gt; [ &lt;column qualifier&gt; ] ] ]
    </pre><p>
    </p><p>
    Alter
    </p><p>
    The <code class="code">alter</code> command has been extended to allow ownership assignment:
    </p><pre class="programlisting">
      alter 'tablename', {OWNER =&gt; 'username|@group'}
    </pre><p>
    </p><p>
    User Permission
    </p><p>
    The <code class="code">user_permission</code> command shows all access permissions for the current user for a given table:
    </p><pre class="programlisting">
      user_permission &lt;table&gt;
    </pre><p>
    </p></div></div><div class="section" title="8.5.&nbsp;Secure Bulk Load"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="hbase.secure.bulkload"></a>8.5.&nbsp;Secure Bulk Load</h2></div></div></div><p>
	Bulk loading in secure mode is a bit more involved than normal setup, since the client has to transfer the ownership of the files generated from the mapreduce job to HBase. Secure bulk loading is implemented by a coprocessor, named <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/security/access/SecureBulkLoadEndpoint.html" target="_top">SecureBulkLoadEndpoint</a>. SecureBulkLoadEndpoint uses a staging directory <code class="code">"hbase.bulkload.staging.dir"</code>, which defaults to <code class="code">/tmp/hbase-staging/</code>. The algorithm is as follows.
	</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Create an hbase owned staging directory which is world traversable (<code class="code">-rwx--x--x, 711</code>) <code class="code">/tmp/hbase-staging</code>. </li><li class="listitem">A user writes out data to his secure output directory: /user/foo/data </li><li class="listitem">A call is made to hbase to create a secret staging directory
  which is globally readable/writable (<code class="code">-rwxrwxrwx, 777</code>): /tmp/hbase-staging/averylongandrandomdirectoryname</li><li class="listitem">The user makes the data world readable and writable, then moves it
  into the random staging directory, then calls bulkLoadHFiles()</li></ul></div><p>
  </p><p>
  Like delegation tokens the strength of the security lies in the length
  and randomness of the secret directory.
    </p><p>
        You have to enable the secure bulk load to work properly. You can modify the <code class="code">hbase-site.xml</code> file on every server machine in the cluster and add the SecureBulkLoadEndpoint class to the list of regionserver coprocessors:
    </p><pre class="programlisting">
      &lt;property&gt;
        &lt;name&gt;hbase.bulkload.staging.dir&lt;/name&gt;
        &lt;value&gt;/tmp/hbase-staging&lt;/value&gt;
      &lt;/property&gt;
      &lt;property&gt;
        &lt;name&gt;hbase.coprocessor.region.classes&lt;/name&gt;
        &lt;value&gt;org.apache.hadoop.hbase.security.token.TokenProvider,
        org.apache.hadoop.hbase.security.access.AccessController,org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint&lt;/value&gt;
      &lt;/property&gt;
    </pre></div><div class="section" title="8.6.&nbsp;Visibility Labels"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="hbase.visibility.labels"></a>8.6.&nbsp;Visibility Labels</h2></div></div></div><p>
	This feature provides cell level security with labeled visibility for the cells. Cells can be associated with a visibility expression. The visibility expression can contain labels joined with logical expressions '&amp;', '|' and '!'. Also using '(', ')' one can specify the precedence order. For example, consider the label set { confidential, secret, topsecret, probationary }, where the first three are sensitivity classifications and the last describes if an employee is probationary or not. If a cell is stored with this visibility expression:
	( secret | topsecret ) &amp; !probationary
	</p><p>
	Then any user associated with the secret or topsecret label will be able to view the cell, as long as the user is not also associated with the probationary label. Furthermore, any user only associated with the confidential label, whether probationary or not, will not see the cell or even know of its existence.
	</p><p>
	Visibility expressions like the above can be added when storing or mutating a cell using the API,
	</p><p><code class="code">Mutation#setCellVisibility(new CellVisibility(String labelExpession));</code></p>
	Where the labelExpression could be '( secret | topsecret ) &amp; !probationary'
	
	<p>
	We build the user's label set in the RPC context when a request is first received by the HBase RegionServer. How users are associated with labels is pluggable. The default plugin passes through labels specified in Authorizations added to the Get or Scan and checks those against the calling user's authenticated labels list. When client passes some labels for which the user is not authenticated, this default algorithm will drop those. One can pass a subset of user authenticated labels via the Scan/Get authorizations.
	</p><p><code class="code">Get#setAuthorizations(new Authorizations(String,...));</code></p><p><code class="code">Scan#setAuthorizations(new Authorizations(String,...));</code></p><div class="section" title="8.6.1.&nbsp;Visibility Label Administration"><div class="titlepage"><div><div><h3 class="title"><a name="hbase.visibility.label.administration"></a>8.6.1.&nbsp;Visibility Label Administration</h3></div></div></div><p>
		There are new client side Java APIs and shell commands for performing visibility labels administrative actions. Only the HBase super user is authorized to perform these operations.
		</p><div class="section" title="8.6.1.1.&nbsp;Adding Labels"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.visibility.label.administration.add.label"></a>8.6.1.1.&nbsp;Adding Labels</h4></div></div></div><p>A set of labels can be added to the system either by using the Java API</p><p><code class="code">VisibilityClient#addLabels(Configuration conf, final String[] labels)</code></p><p>Or by using the shell command</p><p><code class="code">add_labels [label1, label2]</code></p><p>
			Valid label can include alphanumeric characters and characters '-', '_', ':', '.' and '/'
			</p></div><div class="section" title="8.6.1.2.&nbsp;User Label Association"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.visibility.label.administration.add.label"></a>8.6.1.2.&nbsp;User Label Association</h4></div></div></div><p>A set of labels can be associated with a user by using the API</p><p><code class="code">VisibilityClient#setAuths(Configuration conf, final String[] auths, final String user)</code></p><p>Or by using the shell command</p><p><code class="code">set_auths user,[label1, label2].</code></p><p>Labels can be disassociated from a user using API</p><p><code class="code">VisibilityClient#clearAuths(Configuration conf, final String[] auths, final String user)</code></p><p>Or by using shell command</p><p><code class="code">clear_auths user,[label1, label2]</code></p><p>
			One can use the API <code class="code">VisibilityClient#getAuths(Configuration conf, final String user)</code> or <code class="code">get_auths</code> shell command to get the list of labels associated for a given user. The labels and user auths information will be stored in the system table "labels".
			</p></div></div><div class="section" title="8.6.2.&nbsp;Server Side Configuration"><div class="titlepage"><div><div><h3 class="title"><a name="hbase.visibility.label.configuration"></a>8.6.2.&nbsp;Server  Side Configuration</h3></div></div></div><p>
		HBase stores cell level labels as cell tags. HFile version 3 adds the cell tags support. Be sure to use HFile version 3 by setting this property in every server site configuration file:
		</p><pre class="programlisting">
		  &lt;property&gt;
		    &lt;name&gt;hfile.format.version&lt;/name&gt;
			&lt;value&gt;3&lt;/value&gt;
		  &lt;/property&gt;
		</pre><p>
		You will also need to make sure the VisibilityController coprocessor is active on every table to protect by adding it to the list of system coprocessors in the server site configuration files:
		</p><pre class="programlisting">
		  &lt;property&gt;
		    &lt;name&gt;hbase.coprocessor.master.classes&lt;/name&gt;
			&lt;value&gt;org.apache.hadoop.hbase.security.visibility.VisibilityController&lt;/value&gt;
		  &lt;/property&gt;
		  &lt;property&gt;
		    &lt;name&gt;hbase.coprocessor.region.classes&lt;/name&gt;
			&lt;value&gt;org.apache.hadoop.hbase.security.visibility.VisibilityController&lt;/value&gt;
		  &lt;/property&gt;
		</pre><p>
		As said above, finding out labels authenticated for a given get/scan request is a pluggable algorithm. A custom implementation can be plugged in using the property <code class="code">hbase.regionserver.scan.visibility.label.generator.class</code>. The default implementation class is <code class="code">org.apache.hadoop.hbase.security.visibility.DefaultScanLabelGenerator</code>
		</p></div></div><div class="section" title="8.7.&nbsp;Transparent Server Side Encryption"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="hbase.encryption.server"></a>8.7.&nbsp;Transparent Server Side Encryption</h2></div></div></div><p>
This feature provides transparent encryption for protecting HFile and WAL data at rest, using a two-tier key architecture for flexible and non-intrusive key rotation.
    </p><p>
First, the administrator provisions a cluster master key, stored into a key provider accessable to every trusted HBase process: the Master, the RegionServers, and clients (e.g. the shell) on administrative workstations. The default key provider integrates with the Java KeyStore API and any key management system with support for it. How HBase retrieves key material is configurable via the site file. The master key may be stored on the cluster servers, protected by a secure KeyStore file, or on an external keyserver, or in a hardware security module. This master key is resolved as needed by HBase processes through the configured key provider.
    </p><p>
Then, encryption keys can be specified in schema on a per column family basis, by creating or modifying a column descriptor to include two additional attributes: the name of the encryption algorithm to use (currently only "AES" is supported), and, optionally, a data key wrapped (encrypted) with the cluster master key. Per CF keys facilitates low impact incremental key rotation and reduces the scope of any external leak of key material. The wrapped data key is stored in the CF schema metadata, and in each HFile for the CF, encrypted with the cluster master key. Once the CF is configured for encryption, any new HFiles will be written encrypted. To insure encryption of all HFiles, trigger a major compaction after first enabling this feature. The key for decryption, encrypted with the cluster master key, is stored in the HFiles in a new meta block. At file open time the data key will be extracted from the HFile, decrypted with the cluster master key, and used for decryption of the remainder of the HFile. The HFile will be unreadable if the master key is not available. Should remote users somehow acquire access to the HFile data because of some lapse in HDFS permissions or from inappropriately discarded media, there will be no means to decrypt either the data key or the file data.
    </p><p>
Specifying a data key in the CF schema is optional. If one is not present, a random data key will be created for each HFile.
    </p><p>
A new configuration option for encrypting the WAL is also introduced. Even though WALs are transient, it is necessary to encrypt the WALEdits to avoid circumventing HFile protections for encrypted column families.
    </p><div class="section" title="8.7.1.&nbsp;Configuration"><div class="titlepage"><div><div><h3 class="title"><a name="hbase.encryption.server.configuration"></a>8.7.1.&nbsp;Configuration</h3></div></div></div><p>
Create a secret key of appropriate length for AES.
        </p><pre class="programlisting">
        $ keytool -keystore /path/to/hbase/conf/hbase.jks \
          -storetype jceks -storepass &lt;password&gt; \
          -genseckey -keyalg AES -keysize 128 \
          -alias &lt;alias&gt;
	</pre><p>
where &lt;password&gt; is the password for the KeyStore file and &lt;alias&gt;is the user name of the HBase service account, typically "hbase". Simply press RETURN to store the key with the same password as the store. The resulting file should be distributed to all nodes running HBase daemons, with file ownership and permissions set to be readable only by the HBase service account.
        </p><p>
Configure HBase daemons to use a key provider backed by the KeyStore files for retrieving the cluster master key as needed.
        </p><pre class="programlisting">
        &lt;property&gt;
            &lt;name&gt;hbase.crypto.keyprovider&lt;/name&gt;
            &lt;value&gt;org.apache.hadoop.hbase.io.crypto.KeyStoreKeyProvider&lt;/value&gt;
        &lt;/property&gt;
        &lt;property&gt;
            &lt;name&gt;hbase.crypto.keyprovider.parameters&lt;/name&gt;
            &lt;value&gt;jceks:///path/to/hbase/conf/hbase.jks?password=&lt;password&gt;&lt;/value&gt;
        &lt;/property&gt;
        </pre><p>
By default the HBase service account name will be used to resolve the cluster master key, but you can store it with any arbitrary alias and configure HBase appropriately:
        </p><pre class="programlisting">
        &lt;property&gt;
            &lt;name&gt;hbase.crypto.master.key.name&lt;/name&gt;
            &lt;value&gt;hbase&lt;/value&gt;
        &lt;/property&gt;
        </pre><p>
Because the password to the key store is sensitive information, the HBase site XML file should also have its permissions set to be readable only by the HBase service account.
        </p><p>
Transparent encryption is a feature of HFile version 3. Be sure to use HFile version 3 by setting this property in every server site configuration file:
        </p><pre class="programlisting">
        &lt;property&gt;
            &lt;name&gt;hfile.format.version&lt;/name&gt;
            &lt;value&gt;3&lt;/value&gt;
        &lt;/property&gt;
        </pre><p>
Finally, configure the secure WAL in every server site configuration file:
        </p><pre class="programlisting">
        &lt;property&gt;
            &lt;name&gt;hbase.regionserver.hlog.reader.impl&lt;/name&gt;
            &lt;value&gt;org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogReader&lt;/value&gt;
        &lt;/property&gt;
        &lt;property&gt;
            &lt;name&gt;hbase.regionserver.hlog.writer.impl&lt;/name&gt;
            &lt;value&gt;org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogWriter&lt;/value&gt;
        &lt;/property&gt;
        &lt;property&gt;
            &lt;name&gt;hbase.regionserver.wal.encryption&lt;/name&gt;
            &lt;value&gt;true&lt;/value&gt;
        &lt;/property&gt;
        </pre></div><div class="section" title="8.7.2.&nbsp;Setting Encryption on a CF"><div class="titlepage"><div><div><h3 class="title"><a name="hbase.encryption.server.schema"></a>8.7.2.&nbsp;Setting Encryption on a CF</h3></div></div></div><p>
To enable encryption on a CF, use <code class="code">HBaseAdmin#modifyColumn</code> or the HBase shell to modify the column descriptor. The attribute 'ENCRYPTION' specifies the encryption algorithm to use. Currently only "AES" is supported. If creating a new table, simply set this attribute; no subsequent table modification will be necessary.
        </p><p>
If setting a specific data key, the attribute 'ENCRYPTION_KEY' should contain the data key wrapped by the cluster master key. The static methods <code class="code">wrapKey</code> and <code class="code">unwrapKey</code> in <code class="code">org.apache.hadoop.hbase.security.EncryptionUtil</code> can be used in conjunction with <code class="code">HColumnDescriptor#setEncryptionKey</code> for this purpose. Because this must be done programatically, setting a data key with the shell is not supported.
        </p><p>
To disable encryption on a CF, simply remove the 'ENCRYPTION' (and 'ENCRYPTION_KEY', if it was set) attributes from the column schema, using <code class="code">HBaseAdmin#modifyColumn</code> or the HBase shell. All new HFiles for the CF will be written without encryption. Trigger a major compaction to rewrite all files.
	</p></div><div class="section" title="8.7.3.&nbsp;Data Key Rotation"><div class="titlepage"><div><div><h3 class="title"><a name="hbase.encryption.server.data_key_rotation"></a>8.7.3.&nbsp;Data Key Rotation</h3></div></div></div><p>
Data key rotation is made simple by this design. First, change the CF key in the column descriptor. Then, trigger major compaction. Once compaction has completed, all files will be (re)encrypted with the new key material. While this process is ongoing, HFiles encrypted with old key material will still be readable. 
        </p></div><div class="section" title="8.7.4.&nbsp;Master Key Rotation"><div class="titlepage"><div><div><h3 class="title"><a name="hbase.encryption.server.master_key_rotation"></a>8.7.4.&nbsp;Master Key Rotation</h3></div></div></div><p>
Master key rotation can be achieved by updating the KeyStore to contain a new master key, as described above, with also the old master key added to the KeyStore under a different alias. Then, configure fallback to the old master key in the HBase site file:
        </p><pre class="programlisting">
        &lt;property&gt;
            &lt;name&gt;hbase.crypto.master.alternate.key.name&lt;/name&gt;
            &lt;value&gt;hbase.old&lt;/value&gt;
        &lt;/property&gt;
        </pre><p>
This will require a rolling restart of the HBase daemons to take effect. As with data key rotation, trigger a major compaction and wait for it to complete. Once compaction has completed, all files will be (re)encrypted with data keys wrapped by the new cluster master key. The old master key, and its associated site file configuration, can then be removed, and all trace of the old master key will be gone after the next rolling restart. A second rolling restart is not immediately necessary.
        </p></div></div><div class="footnotes"><br><hr width="100" align="left"><div class="footnote"><p><sup>[<a id="ftn.d366e5466" href="#d366e5466" class="para">22</a>] </sup>See
    also Matteo Bertozzi's article on <a class="link" href="http://www.cloudera.com/blog/2012/09/understanding-user-authentication-and-authorization-in-apache-hbase/" target="_top">Understanding User Authentication and Authorization in Apache HBase</a>.</p></div><div class="footnote"><p><sup>[<a id="ftn.d366e5666" href="#d366e5666" class="para">23</a>] </sup>See
    also Matteo Bertozzi's article on <a class="link" href="http://www.cloudera.com/blog/2012/09/understanding-user-authentication-and-authorization-in-apache-hbase/" target="_top">Understanding User Authentication and Authorization in Apache HBase</a>.</p></div></div></div><div class="chapter" title="Chapter&nbsp;9.&nbsp;Architecture"><div class="titlepage"><div><div><h2 class="title"><a name="architecture"></a>Chapter&nbsp;9.&nbsp;Architecture</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#arch.overview">9.1. Overview</a></span></dt><dd><dl><dt><span class="section"><a href="#arch.overview.nosql">9.1.1. NoSQL?</a></span></dt><dt><span class="section"><a href="#arch.overview.when">9.1.2. When Should I Use HBase?</a></span></dt><dt><span class="section"><a href="#arch.overview.hbasehdfs">9.1.3. What Is The Difference Between HBase and Hadoop/HDFS?</a></span></dt></dl></dd><dt><span class="section"><a href="#arch.catalog">9.2. Catalog Tables</a></span></dt><dd><dl><dt><span class="section"><a href="#arch.catalog.root">9.2.1. ROOT</a></span></dt><dt><span class="section"><a href="#arch.catalog.meta">9.2.2. META</a></span></dt><dt><span class="section"><a href="#arch.catalog.startup">9.2.3. Startup Sequencing</a></span></dt></dl></dd><dt><span class="section"><a href="#client">9.3. Client</a></span></dt><dd><dl><dt><span class="section"><a href="#client.connections">9.3.1. Connections</a></span></dt><dt><span class="section"><a href="#client.writebuffer">9.3.2. WriteBuffer and Batch Methods</a></span></dt><dt><span class="section"><a href="#client.external">9.3.3. External Clients</a></span></dt><dt><span class="section"><a href="#client.rowlocks">9.3.4. RowLocks</a></span></dt></dl></dd><dt><span class="section"><a href="#client.filter">9.4. Client Request Filters</a></span></dt><dd><dl><dt><span class="section"><a href="#client.filter.structural">9.4.1. Structural</a></span></dt><dt><span class="section"><a href="#client.filter.cv">9.4.2. Column Value</a></span></dt><dt><span class="section"><a href="#client.filter.cvp">9.4.3. Column Value Comparators</a></span></dt><dt><span class="section"><a href="#client.filter.kvm">9.4.4. KeyValue Metadata</a></span></dt><dt><span class="section"><a href="#client.filter.row">9.4.5. RowKey</a></span></dt><dt><span class="section"><a href="#client.filter.utility">9.4.6. Utility</a></span></dt></dl></dd><dt><span class="section"><a href="#master">9.5. Master</a></span></dt><dd><dl><dt><span class="section"><a href="#master.startup">9.5.1. Startup Behavior</a></span></dt><dt><span class="section"><a href="#master.runtime">9.5.2. Runtime Impact</a></span></dt><dt><span class="section"><a href="#master.api">9.5.3. Interface</a></span></dt><dt><span class="section"><a href="#master.processes">9.5.4. Processes</a></span></dt></dl></dd><dt><span class="section"><a href="#regionserver.arch">9.6. RegionServer</a></span></dt><dd><dl><dt><span class="section"><a href="#regionserver.arch.api">9.6.1. Interface</a></span></dt><dt><span class="section"><a href="#regionserver.arch.processes">9.6.2. Processes</a></span></dt><dt><span class="section"><a href="#coprocessors">9.6.3. Coprocessors</a></span></dt><dt><span class="section"><a href="#block.cache">9.6.4. Block Cache</a></span></dt><dt><span class="section"><a href="#wal">9.6.5. Write Ahead Log (WAL)</a></span></dt></dl></dd><dt><span class="section"><a href="#regions.arch">9.7. Regions</a></span></dt><dd><dl><dt><span class="section"><a href="#arch.regions.size">9.7.1. </a></span></dt><dt><span class="section"><a href="#regions.arch.assignment">9.7.2. Region-RegionServer Assignment</a></span></dt><dt><span class="section"><a href="#regions.arch.locality">9.7.3. Region-RegionServer Locality</a></span></dt><dt><span class="section"><a href="#arch.region.splits">9.7.4. Region Splits</a></span></dt><dt><span class="section"><a href="#d366e7236">9.7.5. Online Region Merges</a></span></dt><dt><span class="section"><a href="#store">9.7.6. Store</a></span></dt></dl></dd><dt><span class="section"><a href="#arch.bulk.load">9.8. Bulk Loading</a></span></dt><dd><dl><dt><span class="section"><a href="#arch.bulk.load.overview">9.8.1. Overview</a></span></dt><dt><span class="section"><a href="#arch.bulk.load.limitations">9.8.2. Bulk Load Limitations</a></span></dt><dt><span class="section"><a href="#arch.bulk.load.arch">9.8.3. Bulk Load Architecture</a></span></dt><dt><span class="section"><a href="#arch.bulk.load.import">9.8.4. Importing the prepared data using the completebulkload tool</a></span></dt><dt><span class="section"><a href="#arch.bulk.load.also">9.8.5. See Also</a></span></dt><dt><span class="section"><a href="#arch.bulk.load.adv">9.8.6. Advanced Usage</a></span></dt></dl></dd><dt><span class="section"><a href="#arch.hdfs">9.9. HDFS</a></span></dt><dd><dl><dt><span class="section"><a href="#arch.hdfs.nn">9.9.1. NameNode</a></span></dt><dt><span class="section"><a href="#arch.hdfs.dn">9.9.2. DataNode</a></span></dt></dl></dd></dl></div><div class="section" title="9.1.&nbsp;Overview"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="arch.overview"></a>9.1.&nbsp;Overview</h2></div></div></div><div class="section" title="9.1.1.&nbsp;NoSQL?"><div class="titlepage"><div><div><h3 class="title"><a name="arch.overview.nosql"></a>9.1.1.&nbsp;NoSQL?</h3></div></div></div><p>HBase is a type of "NoSQL" database.  "NoSQL" is a general term meaning that the database isn't an RDBMS which
	  supports SQL as its primary access language, but there are many types of NoSQL databases:  BerkeleyDB is an
	  example of a local NoSQL database, whereas HBase is very much a distributed database.  Technically speaking,
	  HBase is really more a "Data Store" than "Data Base" because it lacks many of the features you find in an RDBMS,
	  such as typed columns, secondary indexes, triggers, and advanced query languages, etc.
	  </p><p>However, HBase has many features which supports both linear and modular scaling.  HBase clusters expand
	  by adding RegionServers that are hosted on commodity class servers. If a cluster expands from 10 to 20
	  RegionServers, for example, it doubles both in terms of storage and as well as processing capacity.
	  RDBMS can scale well, but only up to a point - specifically, the size of a single database server - and for the best
	  performance requires specialized hardware and storage devices.  HBase features of note are:
	        </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Strongly consistent reads/writes:  HBase is not an "eventually consistent" DataStore.  This
              makes it very suitable for tasks such as high-speed counter aggregation.  </li><li class="listitem">Automatic sharding:  HBase tables are distributed on the cluster via regions, and regions are
              automatically split and re-distributed as your data grows.</li><li class="listitem">Automatic RegionServer failover</li><li class="listitem">Hadoop/HDFS Integration:  HBase supports HDFS out of the box as its distributed file system.</li><li class="listitem">MapReduce:  HBase supports massively parallelized processing via MapReduce for using HBase as both
              source and sink.</li><li class="listitem">Java Client API:  HBase supports an easy to use Java API for programmatic access.</li><li class="listitem">Thrift/REST API:  HBase also supports Thrift and REST for non-Java front-ends.</li><li class="listitem">Block Cache and Bloom Filters:  HBase supports a Block Cache and Bloom Filters for high volume query optimization.</li><li class="listitem">Operational Management:  HBase provides build-in web-pages for operational insight as well as JMX metrics.</li></ul></div><p>
	  </p></div><div class="section" title="9.1.2.&nbsp;When Should I Use HBase?"><div class="titlepage"><div><div><h3 class="title"><a name="arch.overview.when"></a>9.1.2.&nbsp;When Should I Use HBase?</h3></div></div></div><p>HBase isn't suitable for every problem.</p><p>First, make sure you have enough data.  If you have hundreds of millions or billions of rows, then
	            HBase is a good candidate.  If you only have a few thousand/million rows, then using a traditional RDBMS
	            might be a better choice due to the fact that all of your data might wind up on a single node (or two) and
	            the rest of the cluster may be sitting idle.
	          </p><p>Second, make sure you can live without all the extra features that an RDBMS provides (e.g., typed columns,
	          secondary indexes, transactions, advanced query languages, etc.)  An application built against an RDBMS cannot be
	          "ported" to HBase by simply changing a JDBC driver, for example.  Consider moving from an RDBMS to HBase as a
	          complete redesign as opposed to a port.
              </p><p>Third, make sure you have enough hardware.  Even HDFS doesn't do well with anything less than
                5 DataNodes (due to things such as HDFS block replication which has a default of 3), plus a NameNode.
                </p><p>HBase can run quite well stand-alone on a laptop - but this should be considered a development
                configuration only.
                </p></div><div class="section" title="9.1.3.&nbsp;What Is The Difference Between HBase and Hadoop/HDFS?"><div class="titlepage"><div><div><h3 class="title"><a name="arch.overview.hbasehdfs"></a>9.1.3.&nbsp;What Is The Difference Between HBase and Hadoop/HDFS?</h3></div></div></div><p><a class="link" href="http://hadoop.apache.org/hdfs/" target="_top">HDFS</a> is a distributed file system that is well suited for the storage of large files.
          Its documentation states that it is not, however, a general purpose file system, and does not provide fast individual record lookups in files.
          HBase, on the other hand, is built on top of HDFS and provides fast record lookups (and updates) for large tables.
          This can sometimes be a point of conceptual confusion.  HBase internally puts your data in indexed "StoreFiles" that exist
          on HDFS for high-speed lookups.  See the <a class="xref" href="#datamodel" title="Chapter&nbsp;5.&nbsp;Data Model">Chapter&nbsp;5, <i>Data Model</i></a> and the rest of this chapter for more information on how HBase achieves its goals.
         </p></div></div><div class="section" title="9.2.&nbsp;Catalog Tables"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="arch.catalog"></a>9.2.&nbsp;Catalog Tables</h2></div></div></div><p>The catalog tables -ROOT- and .META. exist as HBase tables.  They are filtered out
	  of the HBase shell's <code class="code">list</code> command, but they are in fact tables just like any other.
     </p><div class="section" title="9.2.1.&nbsp;ROOT"><div class="titlepage"><div><div><h3 class="title"><a name="arch.catalog.root"></a>9.2.1.&nbsp;ROOT</h3></div></div></div><p>-ROOT- keeps track of where the .META. table is.  The -ROOT- table structure is as follows:
       </p><p>Key:
            </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">.META. region key (<code class="code">.META.,,1</code>)</li></ul></div><p>
       </p><p>Values:
            </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><code class="code">info:regioninfo</code> (serialized <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HRegionInfo.html" target="_top">HRegionInfo</a>
               instance of .META.)</li><li class="listitem"><code class="code">info:server</code> (server:port of the RegionServer holding .META.)</li><li class="listitem"><code class="code">info:serverstartcode</code> (start-time of the RegionServer process holding .META.)</li></ul></div><p>
       </p></div><div class="section" title="9.2.2.&nbsp;META"><div class="titlepage"><div><div><h3 class="title"><a name="arch.catalog.meta"></a>9.2.2.&nbsp;META</h3></div></div></div><p>The .META. table keeps a list of all regions in the system. The .META. table structure is as follows:
       </p><p>Key:
            </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Region key of the format (<code class="code">[table],[region start key],[region id]</code>)</li></ul></div><p>
       </p><p>Values:
            </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><code class="code">info:regioninfo</code> (serialized <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HRegionInfo.html" target="_top">
              HRegionInfo</a> instance for this region)
              </li><li class="listitem"><code class="code">info:server</code> (server:port of the RegionServer containing this region)</li><li class="listitem"><code class="code">info:serverstartcode</code> (start-time of the RegionServer process containing this region)</li></ul></div><p>
       </p><p>When a table is in the process of splitting two other columns will be created, <code class="code">info:splitA</code> and <code class="code">info:splitB</code>
       which represent the two daughter regions.  The values for these columns are also serialized HRegionInfo instances.
       After the region has been split eventually this row will be deleted.
       </p><p>Notes on HRegionInfo:  the empty key is used to denote table start and table end.  A region with an empty start key
       is the first region in a table.  If region has both an empty start and an empty end key, it's the only region in the table
       </p><p>In the (hopefully unlikely) event that programmatic processing of catalog metadata is required, see the
         <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/util/Writables.html#getHRegionInfo%28byte[]%29" target="_top">Writables</a> utility.
       </p></div><div class="section" title="9.2.3.&nbsp;Startup Sequencing"><div class="titlepage"><div><div><h3 class="title"><a name="arch.catalog.startup"></a>9.2.3.&nbsp;Startup Sequencing</h3></div></div></div><p>The META location is set in ROOT first.  Then META is updated with server and startcode values.
	    </p><p>For information on region-RegionServer assignment, see <a class="xref" href="#regions.arch.assignment" title="9.7.2.&nbsp;Region-RegionServer Assignment">Section&nbsp;9.7.2, &#8220;Region-RegionServer Assignment&#8221;</a>.
	    </p></div></div><div class="section" title="9.3.&nbsp;Client"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="client"></a>9.3.&nbsp;Client</h2></div></div></div><p>The HBase client
         <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html" target="_top">HTable</a>
         is responsible for finding RegionServers that are serving the
         particular row range of interest.  It does this by querying
         the <code class="code">.META.</code> and <code class="code">-ROOT-</code> catalog tables
         (TODO: Explain).  After locating the required
         region(s), the client <span class="emphasis"><em>directly</em></span> contacts
         the RegionServer serving that region (i.e., it does not go
         through the master) and issues the read or write request.
         This information is cached in the client so that subsequent requests
         need not go through the lookup process.  Should a region be reassigned
         either by the master load balancer or because a RegionServer has died,
         the client will requery the catalog tables to determine the new
         location of the user region.
    </p><p>See <a class="xref" href="#master.runtime" title="9.5.2.&nbsp;Runtime Impact">Section&nbsp;9.5.2, &#8220;Runtime Impact&#8221;</a> for more information about the impact of the Master on HBase Client
    communication.
    </p><p>Administrative functions are handled through <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HBaseAdmin.html" target="_top">HBaseAdmin</a>
    </p><div class="section" title="9.3.1.&nbsp;Connections"><div class="titlepage"><div><div><h3 class="title"><a name="client.connections"></a>9.3.1.&nbsp;Connections</h3></div></div></div><p>For connection configuration information, see <a class="xref" href="#client_dependencies" title="2.3.4.&nbsp;Client configuration and dependencies connecting to an HBase cluster">Section&nbsp;2.3.4, &#8220;Client configuration and dependencies connecting to an HBase cluster&#8221;</a>.
         </p><p><span class="emphasis"><em><a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html" target="_top">HTable</a>
                 instances are not thread-safe</em></span>.  Only one thread use an instance of HTable at any given
             time.  When creating HTable instances, it is advisable to use the same <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HBaseConfiguration" target="_top">HBaseConfiguration</a>
instance.  This will ensure sharing of ZooKeeper and socket instances to the RegionServers
which is usually what you want.  For example, this is preferred:
		</p><pre class="programlisting">HBaseConfiguration conf = HBaseConfiguration.create();
HTable table1 = new HTable(conf, "myTable");
HTable table2 = new HTable(conf, "myTable");</pre><p>
		as opposed to this:
        </p><pre class="programlisting">HBaseConfiguration conf1 = HBaseConfiguration.create();
HTable table1 = new HTable(conf1, "myTable");
HBaseConfiguration conf2 = HBaseConfiguration.create();
HTable table2 = new HTable(conf2, "myTable");</pre><p>
        For more information about how connections are handled in the HBase client,
        see <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HConnectionManager.html" target="_top">HConnectionManager</a>.
          </p><div class="section" title="9.3.1.1.&nbsp;Connection Pooling"><div class="titlepage"><div><div><h4 class="title"><a name="client.connection.pooling"></a>9.3.1.1.&nbsp;Connection Pooling</h4></div></div></div><p>For applications which require high-end multithreaded access (e.g., web-servers or application servers that may serve many application threads
            in a single JVM), one solution is <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTablePool.html" target="_top">HTablePool</a>.
            But as written currently, it is difficult to control client resource consumption when using HTablePool.
            </p><p>
                Another solution is to precreate an <code class="classname">HConnection</code> using
                </p><pre class="programlisting">// Create a connection to the cluster.
HConnection connection = HConnectionManager.createConnection(Configuration);
HTableInterface table = connection.getTable("myTable");
// use table as needed, the table returned is lightweight
table.close();
// use the connection for other access to the cluster
connection.close();</pre><p>
                Constructing HTableInterface implementation is very lightweight and resources are controlled/shared if you go this route.
            </p></div></div><div class="section" title="9.3.2.&nbsp;WriteBuffer and Batch Methods"><div class="titlepage"><div><div><h3 class="title"><a name="client.writebuffer"></a>9.3.2.&nbsp;WriteBuffer and Batch Methods</h3></div></div></div><p>If <a class="xref" href="#perf.hbase.client.autoflush" title="12.8.4.&nbsp;HBase Client: AutoFlush">Section&nbsp;12.8.4, &#8220;HBase Client:  AutoFlush&#8221;</a> is turned off on
               <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html" target="_top">HTable</a>,
               <code class="classname">Put</code>s are sent to RegionServers when the writebuffer
               is filled.  The writebuffer is 2MB by default.  Before an HTable instance is
               discarded, either <code class="methodname">close()</code> or
               <code class="methodname">flushCommits()</code> should be invoked so Puts
               will not be lost.
	      </p><p>Note: <code class="code">htable.delete(Delete);</code> does not go in the writebuffer!  This only applies to Puts.
	      </p><p>For additional information on write durability, review the <a class="link" href="../acid-semantics.html" target="_top">ACID semantics</a> page.
	      </p><p>For fine-grained control of batching of
           <code class="classname">Put</code>s or <code class="classname">Delete</code>s,
           see the <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html#batch%28java.util.List%29" target="_top">batch</a> methods on HTable.
	   </p></div><div class="section" title="9.3.3.&nbsp;External Clients"><div class="titlepage"><div><div><h3 class="title"><a name="client.external"></a>9.3.3.&nbsp;External Clients</h3></div></div></div><p>Information on non-Java clients and custom protocols is covered in <a class="xref" href="#external_apis" title="Chapter&nbsp;10.&nbsp;Apache HBase External APIs">Chapter&nbsp;10, <i>Apache HBase External APIs</i></a>
           </p></div><div class="section" title="9.3.4.&nbsp;RowLocks"><div class="titlepage"><div><div><h3 class="title"><a name="client.rowlocks"></a>9.3.4.&nbsp;RowLocks</h3></div></div></div><p><a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html#lockRow%28byte[]%29" target="_top">RowLocks</a> are still
           in the client API <span class="emphasis"><em>however</em></span> they are discouraged because if not managed properly these can
           lock up the RegionServers.
           </p><p>There is an oustanding ticket <a class="link" href="https://issues.apache.org/jira/browse/HBASE-2332" target="_top">HBASE-2332</a> to
           remove this feature from the client.
           </p></div></div><div class="section" title="9.4.&nbsp;Client Request Filters"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="client.filter"></a>9.4.&nbsp;Client Request Filters</h2></div></div></div><p><a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Get.html" target="_top">Get</a> and <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Scan.html" target="_top">Scan</a> instances can be
       optionally configured with <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/Filter.html" target="_top">filters</a> which are applied on the RegionServer.
      </p><p>Filters can be confusing because there are many different types, and it is best to approach them by understanding the groups
      of Filter functionality.
      </p><div class="section" title="9.4.1.&nbsp;Structural"><div class="titlepage"><div><div><h3 class="title"><a name="client.filter.structural"></a>9.4.1.&nbsp;Structural</h3></div></div></div><p>Structural Filters contain other Filters.</p><div class="section" title="9.4.1.1.&nbsp;FilterList"><div class="titlepage"><div><div><h4 class="title"><a name="client.filter.structural.fl"></a>9.4.1.1.&nbsp;FilterList</h4></div></div></div><p><a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/FilterList.html" target="_top">FilterList</a>
          represents a list of Filters with a relationship of <code class="code">FilterList.Operator.MUST_PASS_ALL</code> or
          <code class="code">FilterList.Operator.MUST_PASS_ONE</code> between the Filters.  The following example shows an 'or' between two
          Filters (checking for either 'my value' or 'my other value' on the same attribute).
</p><pre class="programlisting">
FilterList list = new FilterList(FilterList.Operator.MUST_PASS_ONE);
SingleColumnValueFilter filter1 = new SingleColumnValueFilter(
	cf,
	column,
	CompareOp.EQUAL,
	Bytes.toBytes("my value")
	);
list.add(filter1);
SingleColumnValueFilter filter2 = new SingleColumnValueFilter(
	cf,
	column,
	CompareOp.EQUAL,
	Bytes.toBytes("my other value")
	);
list.add(filter2);
scan.setFilter(list);
</pre><p>
          </p></div></div><div class="section" title="9.4.2.&nbsp;Column Value"><div class="titlepage"><div><div><h3 class="title"><a name="client.filter.cv"></a>9.4.2.&nbsp;Column Value</h3></div></div></div><div class="section" title="9.4.2.1.&nbsp;SingleColumnValueFilter"><div class="titlepage"><div><div><h4 class="title"><a name="client.filter.cv.scvf"></a>9.4.2.1.&nbsp;SingleColumnValueFilter</h4></div></div></div><p><a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/SingleColumnValueFilter.html" target="_top">SingleColumnValueFilter</a>
          can be used to test column values for equivalence (<code class="code"><a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/CompareFilter.CompareOp.html" target="_top">CompareOp.EQUAL</a>
          </code>), inequality (<code class="code">CompareOp.NOT_EQUAL</code>), or ranges
          (e.g., <code class="code">CompareOp.GREATER</code>).  The folowing is example of testing equivalence a column to a String value "my value"...
</p><pre class="programlisting">
SingleColumnValueFilter filter = new SingleColumnValueFilter(
	cf,
	column,
	CompareOp.EQUAL,
	Bytes.toBytes("my value")
	);
scan.setFilter(filter);
</pre><p>
          </p></div></div><div class="section" title="9.4.3.&nbsp;Column Value Comparators"><div class="titlepage"><div><div><h3 class="title"><a name="client.filter.cvp"></a>9.4.3.&nbsp;Column Value Comparators</h3></div></div></div><p>There are several Comparator classes in the Filter package that deserve special mention.
        These Comparators are used in concert with other Filters, such as  <a class="xref" href="#client.filter.cv.scvf" title="9.4.2.1.&nbsp;SingleColumnValueFilter">Section&nbsp;9.4.2.1, &#8220;SingleColumnValueFilter&#8221;</a>.
        </p><div class="section" title="9.4.3.1.&nbsp;RegexStringComparator"><div class="titlepage"><div><div><h4 class="title"><a name="client.filter.cvp.rcs"></a>9.4.3.1.&nbsp;RegexStringComparator</h4></div></div></div><p><a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/RegexStringComparator.html" target="_top">RegexStringComparator</a>
          supports regular expressions for value comparisons.
</p><pre class="programlisting">
RegexStringComparator comp = new RegexStringComparator("my.");   // any value that starts with 'my'
SingleColumnValueFilter filter = new SingleColumnValueFilter(
	cf,
	column,
	CompareOp.EQUAL,
	comp
	);
scan.setFilter(filter);
</pre><p>
          See the Oracle JavaDoc for <a class="link" href="http://download.oracle.com/javase/6/docs/api/java/util/regex/Pattern.html" target="_top">supported RegEx patterns in Java</a>.
          </p></div><div class="section" title="9.4.3.2.&nbsp;SubstringComparator"><div class="titlepage"><div><div><h4 class="title"><a name="client.filter.cvp.rcs"></a>9.4.3.2.&nbsp;SubstringComparator</h4></div></div></div><p><a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/SubstringComparator.html" target="_top">SubstringComparator</a>
          can be used to determine if a given substring exists in a value.  The comparison is case-insensitive.
          </p><pre class="programlisting">
SubstringComparator comp = new SubstringComparator("y val");   // looking for 'my value'
SingleColumnValueFilter filter = new SingleColumnValueFilter(
	cf,
	column,
	CompareOp.EQUAL,
	comp
	);
scan.setFilter(filter);
</pre></div><div class="section" title="9.4.3.3.&nbsp;BinaryPrefixComparator"><div class="titlepage"><div><div><h4 class="title"><a name="client.filter.cvp.bfp"></a>9.4.3.3.&nbsp;BinaryPrefixComparator</h4></div></div></div><p>See <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/BinaryPrefixComparator.html" target="_top">BinaryPrefixComparator</a>.</p></div><div class="section" title="9.4.3.4.&nbsp;BinaryComparator"><div class="titlepage"><div><div><h4 class="title"><a name="client.filter.cvp.bc"></a>9.4.3.4.&nbsp;BinaryComparator</h4></div></div></div><p>See <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/BinaryComparator.html" target="_top">BinaryComparator</a>.</p></div></div><div class="section" title="9.4.4.&nbsp;KeyValue Metadata"><div class="titlepage"><div><div><h3 class="title"><a name="client.filter.kvm"></a>9.4.4.&nbsp;KeyValue Metadata</h3></div></div></div><p>As HBase stores data internally as KeyValue pairs, KeyValue Metadata Filters evaluate the existence of keys (i.e., ColumnFamily:Column qualifiers)
        for a row, as opposed to values the previous section.
        </p><div class="section" title="9.4.4.1.&nbsp;FamilyFilter"><div class="titlepage"><div><div><h4 class="title"><a name="client.filter.kvm.ff"></a>9.4.4.1.&nbsp;FamilyFilter</h4></div></div></div><p><a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/FamilyFilter.html" target="_top">FamilyFilter</a> can be used
          to filter on the ColumnFamily.  It is generally a better idea to select ColumnFamilies in the Scan than to do it with a Filter.</p></div><div class="section" title="9.4.4.2.&nbsp;QualifierFilter"><div class="titlepage"><div><div><h4 class="title"><a name="client.filter.kvm.qf"></a>9.4.4.2.&nbsp;QualifierFilter</h4></div></div></div><p><a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/QualifierFilter.html" target="_top">QualifierFilter</a> can be used
          to filter based on Column (aka Qualifier) name.
          </p></div><div class="section" title="9.4.4.3.&nbsp;ColumnPrefixFilter"><div class="titlepage"><div><div><h4 class="title"><a name="client.filter.kvm.cpf"></a>9.4.4.3.&nbsp;ColumnPrefixFilter</h4></div></div></div><p><a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/ColumnPrefixFilter.html" target="_top">ColumnPrefixFilter</a> can be used
          to filter based on the lead portion of Column (aka Qualifier) names.
          </p><p>A ColumnPrefixFilter seeks ahead to the first column matching the prefix in each row and for each involved column family. It can be used to efficiently
	 	  get a subset of the columns in very wide rows.
	      </p><p>Note: The same column qualifier can be used in different column families. This filter returns all matching columns.
          </p><p>Example: Find all columns in a row and family that start with "abc"
</p><pre class="programlisting">
HTableInterface t = ...;
byte[] row = ...;
byte[] family = ...;
byte[] prefix = Bytes.toBytes("abc");
Scan scan = new Scan(row, row); // (optional) limit to one row
scan.addFamily(family); // (optional) limit to one family
Filter f = new ColumnPrefixFilter(prefix);
scan.setFilter(f);
scan.setBatch(10); // set this if there could be many columns returned
ResultScanner rs = t.getScanner(scan);
for (Result r = rs.next(); r != null; r = rs.next()) {
  for (KeyValue kv : r.raw()) {
    // each kv represents a column
  }
}
rs.close();
</pre><p>
</p></div><div class="section" title="9.4.4.4.&nbsp;MultipleColumnPrefixFilter"><div class="titlepage"><div><div><h4 class="title"><a name="client.filter.kvm.mcpf"></a>9.4.4.4.&nbsp;MultipleColumnPrefixFilter</h4></div></div></div><p><a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/MultipleColumnPrefixFilter.html" target="_top">MultipleColumnPrefixFilter</a> behaves like ColumnPrefixFilter
          but allows specifying multiple prefixes.
          </p><p>Like ColumnPrefixFilter, MultipleColumnPrefixFilter efficiently seeks ahead to the first column matching the lowest prefix and also seeks past ranges of columns between prefixes.
	      It can be used to efficiently get discontinuous sets of columns from very wide rows.
		  </p><p>Example: Find all columns in a row and family that start with "abc" or "xyz"
</p><pre class="programlisting">
HTableInterface t = ...;
byte[] row = ...;
byte[] family = ...;
byte[][] prefixes = new byte[][] {Bytes.toBytes("abc"), Bytes.toBytes("xyz")};
Scan scan = new Scan(row, row); // (optional) limit to one row
scan.addFamily(family); // (optional) limit to one family
Filter f = new MultipleColumnPrefixFilter(prefixes);
scan.setFilter(f);
scan.setBatch(10); // set this if there could be many columns returned
ResultScanner rs = t.getScanner(scan);
for (Result r = rs.next(); r != null; r = rs.next()) {
  for (KeyValue kv : r.raw()) {
    // each kv represents a column
  }
}
rs.close();
</pre><p>
</p></div><div class="section" title="9.4.4.5.&nbsp;ColumnRangeFilter"><div class="titlepage"><div><div><h4 class="title"><a name="client.filter.kvm.crf "></a>9.4.4.5.&nbsp;ColumnRangeFilter</h4></div></div></div><p>A <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/ColumnRangeFilter.html" target="_top">ColumnRangeFilter</a> allows efficient intra row scanning.
            </p><p>A ColumnRangeFilter can seek ahead to the first matching column for each involved column family. It can be used to efficiently
			get a 'slice' of the columns of a very wide row.
			 i.e. you have a million columns in a row but you only want to look at columns bbbb-bbdd.
            </p><p>Note: The same column qualifier can be used in different column families. This filter returns all matching columns.
            </p><p>Example: Find all columns in a row and family between "bbbb" (inclusive) and "bbdd" (inclusive)
</p><pre class="programlisting">
HTableInterface t = ...;
byte[] row = ...;
byte[] family = ...;
byte[] startColumn = Bytes.toBytes("bbbb");
byte[] endColumn = Bytes.toBytes("bbdd");
Scan scan = new Scan(row, row); // (optional) limit to one row
scan.addFamily(family); // (optional) limit to one family
Filter f = new ColumnRangeFilter(startColumn, true, endColumn, true);
scan.setFilter(f);
scan.setBatch(10); // set this if there could be many columns returned
ResultScanner rs = t.getScanner(scan);
for (Result r = rs.next(); r != null; r = rs.next()) {
  for (KeyValue kv : r.raw()) {
    // each kv represents a column
  }
}
rs.close();
</pre><p>
</p><p>Note:  Introduced in HBase 0.92</p></div></div><div class="section" title="9.4.5.&nbsp;RowKey"><div class="titlepage"><div><div><h3 class="title"><a name="client.filter.row"></a>9.4.5.&nbsp;RowKey</h3></div></div></div><div class="section" title="9.4.5.1.&nbsp;RowFilter"><div class="titlepage"><div><div><h4 class="title"><a name="client.filter.row.rf"></a>9.4.5.1.&nbsp;RowFilter</h4></div></div></div><p>It is generally a better idea to use the startRow/stopRow methods on Scan for row selection, however
          <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/RowFilter.html" target="_top">RowFilter</a> can also be used.</p></div></div><div class="section" title="9.4.6.&nbsp;Utility"><div class="titlepage"><div><div><h3 class="title"><a name="client.filter.utility"></a>9.4.6.&nbsp;Utility</h3></div></div></div><div class="section" title="9.4.6.1.&nbsp;FirstKeyOnlyFilter"><div class="titlepage"><div><div><h4 class="title"><a name="client.filter.utility.fkof"></a>9.4.6.1.&nbsp;FirstKeyOnlyFilter</h4></div></div></div><p>This is primarily used for rowcount jobs.
          See <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/FirstKeyOnlyFilter.html" target="_top">FirstKeyOnlyFilter</a>.</p></div></div></div><div class="section" title="9.5.&nbsp;Master"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="master"></a>9.5.&nbsp;Master</h2></div></div></div><p><code class="code">HMaster</code> is the implementation of the Master Server.  The Master server
       is responsible for monitoring all RegionServer instances in the cluster, and is
       the interface for all metadata changes.  In a distributed cluster, the Master typically runs on the <a class="xref" href="#arch.hdfs.nn" title="9.9.1.&nbsp;NameNode">Section&nbsp;9.9.1, &#8220;NameNode&#8221;</a><sup>[<a name="d366e6789" href="#ftn.d366e6789" class="footnote">24</a>]</sup>
       </p><div class="section" title="9.5.1.&nbsp;Startup Behavior"><div class="titlepage"><div><div><h3 class="title"><a name="master.startup"></a>9.5.1.&nbsp;Startup Behavior</h3></div></div></div><p>If run in a multi-Master environment, all Masters compete to run the cluster.  If the active
         Master loses its lease in ZooKeeper (or the Master shuts down), then then the remaining Masters jostle to
         take over the Master role.
         </p></div><div class="section" title="9.5.2.&nbsp;Runtime Impact"><div class="titlepage"><div><div><h3 class="title"><a name="master.runtime"></a>9.5.2.&nbsp;Runtime Impact</h3></div></div></div><p>A common dist-list question is what happens to an HBase cluster when the Master goes down.  Because the
         HBase client talks directly to the RegionServers, the cluster can still function in a "steady
         state."  Additionally, per <a class="xref" href="#arch.catalog" title="9.2.&nbsp;Catalog Tables">Section&nbsp;9.2, &#8220;Catalog Tables&#8221;</a> ROOT and META exist as HBase tables (i.e., are
         not resident in the Master).  However, the Master controls critical functions such as RegionServer failover and
         completing region splits.  So while the cluster can still run <span class="emphasis"><em>for a time</em></span> without the Master,
         the Master should be restarted as soon as possible.
         </p></div><div class="section" title="9.5.3.&nbsp;Interface"><div class="titlepage"><div><div><h3 class="title"><a name="master.api"></a>9.5.3.&nbsp;Interface</h3></div></div></div><p>The methods exposed by <code class="code">HMasterInterface</code> are primarily metadata-oriented methods:
         </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Table (createTable, modifyTable, removeTable, enable, disable)
            </li><li class="listitem">ColumnFamily (addColumn, modifyColumn, removeColumn)
            </li><li class="listitem">Region (move, assign, unassign)
            </li></ul></div><p>
         For example, when the <code class="code">HBaseAdmin</code> method <code class="code">disableTable</code> is invoked, it is serviced by the Master server.
         </p></div><div class="section" title="9.5.4.&nbsp;Processes"><div class="titlepage"><div><div><h3 class="title"><a name="master.processes"></a>9.5.4.&nbsp;Processes</h3></div></div></div><p>The Master runs several background threads:
         </p><div class="section" title="9.5.4.1.&nbsp;LoadBalancer"><div class="titlepage"><div><div><h4 class="title"><a name="master.processes.loadbalancer"></a>9.5.4.1.&nbsp;LoadBalancer</h4></div></div></div><p>Periodically, and when there are no regions in transition,
             a load balancer will run and move regions around to balance the cluster's load.
             See <a class="xref" href="#balancer_config" title="2.5.3.1.&nbsp;Balancer">Section&nbsp;2.5.3.1, &#8220;Balancer&#8221;</a> for configuring this property.</p><p>See <a class="xref" href="#regions.arch.assignment" title="9.7.2.&nbsp;Region-RegionServer Assignment">Section&nbsp;9.7.2, &#8220;Region-RegionServer Assignment&#8221;</a> for more information on region assignment.
             </p></div><div class="section" title="9.5.4.2.&nbsp;CatalogJanitor"><div class="titlepage"><div><div><h4 class="title"><a name="master.processes.catalog"></a>9.5.4.2.&nbsp;CatalogJanitor</h4></div></div></div><p>Periodically checks and cleans up the .META. table.  See <a class="xref" href="#arch.catalog.meta" title="9.2.2.&nbsp;META">Section&nbsp;9.2.2, &#8220;META&#8221;</a> for more information on META.</p></div></div></div><div class="section" title="9.6.&nbsp;RegionServer"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="regionserver.arch"></a>9.6.&nbsp;RegionServer</h2></div></div></div><p><code class="code">HRegionServer</code> is the RegionServer implementation.  It is responsible for serving and managing regions.
       In a distributed cluster, a RegionServer runs on a <a class="xref" href="#arch.hdfs.dn" title="9.9.2.&nbsp;DataNode">Section&nbsp;9.9.2, &#8220;DataNode&#8221;</a>.
       </p><div class="section" title="9.6.1.&nbsp;Interface"><div class="titlepage"><div><div><h3 class="title"><a name="regionserver.arch.api"></a>9.6.1.&nbsp;Interface</h3></div></div></div><p>The methods exposed by <code class="code">HRegionRegionInterface</code> contain both data-oriented and region-maintenance methods:
         </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Data (get, put, delete, next, etc.)
            </li><li class="listitem">Region (splitRegion, compactRegion, etc.)
            </li></ul></div><p>
         For example, when the <code class="code">HBaseAdmin</code> method <code class="code">majorCompact</code> is invoked on a table, the client is actually iterating through
         all regions for the specified table and requesting a major compaction directly to each region.
         </p></div><div class="section" title="9.6.2.&nbsp;Processes"><div class="titlepage"><div><div><h3 class="title"><a name="regionserver.arch.processes"></a>9.6.2.&nbsp;Processes</h3></div></div></div><p>The RegionServer runs a variety of background threads:</p><div class="section" title="9.6.2.1.&nbsp;CompactSplitThread"><div class="titlepage"><div><div><h4 class="title"><a name="regionserver.arch.processes.compactsplit"></a>9.6.2.1.&nbsp;CompactSplitThread</h4></div></div></div><p>Checks for splits and handle minor compactions.</p></div><div class="section" title="9.6.2.2.&nbsp;MajorCompactionChecker"><div class="titlepage"><div><div><h4 class="title"><a name="regionserver.arch.processes.majorcompact"></a>9.6.2.2.&nbsp;MajorCompactionChecker</h4></div></div></div><p>Checks for major compactions.</p></div><div class="section" title="9.6.2.3.&nbsp;MemStoreFlusher"><div class="titlepage"><div><div><h4 class="title"><a name="regionserver.arch.processes.memstore"></a>9.6.2.3.&nbsp;MemStoreFlusher</h4></div></div></div><p>Periodically flushes in-memory writes in the MemStore to StoreFiles.</p></div><div class="section" title="9.6.2.4.&nbsp;LogRoller"><div class="titlepage"><div><div><h4 class="title"><a name="regionserver.arch.processes.log"></a>9.6.2.4.&nbsp;LogRoller</h4></div></div></div><p>Periodically checks the RegionServer's HLog.</p></div></div><div class="section" title="9.6.3.&nbsp;Coprocessors"><div class="titlepage"><div><div><h3 class="title"><a name="coprocessors"></a>9.6.3.&nbsp;Coprocessors</h3></div></div></div><p>Coprocessors were added in 0.92.  There is a thorough <a class="link" href="https://blogs.apache.org/hbase/entry/coprocessor_introduction" target="_top">Blog Overview of CoProcessors</a>
         posted.  Documentation will eventually move to this reference guide, but the blog is the most current information available at this time.
         </p></div><div class="section" title="9.6.4.&nbsp;Block Cache"><div class="titlepage"><div><div><h3 class="title"><a name="block.cache"></a>9.6.4.&nbsp;Block Cache</h3></div></div></div><div class="section" title="9.6.4.1.&nbsp;Design"><div class="titlepage"><div><div><h4 class="title"><a name="block.cache.design"></a>9.6.4.1.&nbsp;Design</h4></div></div></div><p>The Block Cache is an LRU cache that contains three levels of block priority to allow for scan-resistance and in-memory ColumnFamilies:
        </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Single access priority: The first time a block is loaded from HDFS it normally has this priority and it will be part of the first group to be considered
            during evictions. The advantage is that scanned blocks are more likely to get evicted than blocks that are getting more usage.
            </li><li class="listitem">Mutli access priority: If a block in the previous priority group is accessed again, it upgrades to this priority. It is thus part of the second group
            considered during evictions.
            </li><li class="listitem">In-memory access priority: If the block's family was configured to be "in-memory", it will be part of this priority disregarding the number of times it
            was accessed. Catalog tables are configured like this. This group is the last one considered during evictions.
            </li></ul></div><p>
        For more information, see the <a class="link" href="http://hbase.apache.org/xref/org/apache/hadoop/hbase/io/hfile/LruBlockCache.html" target="_top">LruBlockCache source</a>
        </p></div><div class="section" title="9.6.4.2.&nbsp;Usage"><div class="titlepage"><div><div><h4 class="title"><a name="block.cache.usage"></a>9.6.4.2.&nbsp;Usage</h4></div></div></div><p>Block caching is enabled by default for all the user tables which means that any read operation will load the LRU cache. This might be good for a large number of use cases,
        but further tunings are usually required in order to achieve better performance. An important concept is the
        <a class="link" href="http://en.wikipedia.org/wiki/Working_set_size" target="_top">working set size</a>, or WSS, which is: "the amount of memory needed to compute the answer to a problem".
        For a website, this would be the data that's needed to answer the queries over a short amount of time.
        </p><p>The way to calculate how much memory is available in HBase for caching is:
        </p><pre class="programlisting">
            number of region servers * heap size * hfile.block.cache.size * 0.85
        </pre><p>The default value for the block cache is 0.25 which represents 25% of the available heap. The last value (85%) is the default acceptable loading factor in the LRU cache after
        which eviction is started. The reason it is included in this equation is that it would be unrealistic to say that it is possible to use 100% of the available memory since this would
        make the process blocking from the point where it loads new blocks. Here are some examples:
        </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">One region server with the default heap size (1GB) and the default block cache size will have 217MB of block cache available.
            </li><li class="listitem">20 region servers with the heap size set to 8GB and a default block cache size will have 34GB of block cache.
            </li><li class="listitem">100 region servers with the heap size set to 24GB and a block cache size of 0.5 will have about 1TB of block cache.
            </li></ul></div><p>Your data isn't the only resident of the block cache, here are others that you may have to take into account:
        </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Catalog tables: The -ROOT- and .META. tables are forced into the block cache and have the in-memory priority which means that they are harder to evict. The former never uses
            more than a few hundreds of bytes while the latter can occupy a few MBs (depending on the number of regions).
            </li><li class="listitem">HFiles indexes: HFile is the file format that HBase uses to store data in HDFS and it contains a multi-layered index in order seek to the data without having to read the whole file.
            The size of those indexes is a factor of the block size (64KB by default), the size of your keys and the amount of data you are storing. For big data sets it's not unusual to see numbers around
            1GB per region server, although not all of it will be in cache because the LRU will evict indexes that aren't used.
            </li><li class="listitem">Keys: Taking into account only the values that are being stored is missing half the picture since every value is stored along with its keys
            (row key, family, qualifier, and timestamp). See <a class="xref" href="#keysize" title="6.3.2.&nbsp;Try to minimize row and column sizes">Section&nbsp;6.3.2, &#8220;Try to minimize row and column sizes&#8221;</a>.
            </li><li class="listitem">Bloom filters: Just like the HFile indexes, those data structures (when enabled) are stored in the LRU.
            </li></ul></div><p>Currently the recommended way to measure HFile indexes and bloom filters sizes is to look at the region server web UI and checkout the relevant metrics. For keys,
        sampling can be done by using the HFile command line tool and look for the average key size metric.
        </p><p>It's generally bad to use block caching when the WSS doesn't fit in memory. This is the case when you have for example 40GB available across all your region servers' block caches
        but you need to process 1TB of data. One of the reasons is that the churn generated by the evictions will trigger more garbage collections unnecessarily. Here are two use cases:
        </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Fully random reading pattern: This is a case where you almost never access the same row twice within a short amount of time such that the chance of hitting a cached block is close
            to 0. Setting block caching on such a table is a waste of memory and CPU cycles, more so that it will generate more garbage to pick up by the JVM. For more information on monitoring GC,
            see <a class="xref" href="#trouble.log.gc" title="13.2.3.&nbsp;JVM Garbage Collection Logs">Section&nbsp;13.2.3, &#8220;JVM Garbage Collection Logs&#8221;</a>.
            </li><li class="listitem">Mapping a table: In a typical MapReduce job that takes a table in input, every row will be read only once so there's no need to put them into the block cache. The Scan object has
            the option of turning this off via the setCaching method (set it to false). You can still keep block caching turned on on this table if you need fast random read access. An example would be
            counting the number of rows in a table that serves live traffic, caching every block of that table would create massive churn and would surely evict data that's currently in use.
            </li></ul></div></div></div><div class="section" title="9.6.5.&nbsp;Write Ahead Log (WAL)"><div class="titlepage"><div><div><h3 class="title"><a name="wal"></a>9.6.5.&nbsp;Write Ahead Log (WAL)</h3></div></div></div><div class="section" title="9.6.5.1.&nbsp;Purpose"><div class="titlepage"><div><div><h4 class="title"><a name="purpose.wal"></a>9.6.5.1.&nbsp;Purpose</h4></div></div></div><p>Each RegionServer adds updates (Puts, Deletes) to its write-ahead log (WAL)
            first, and then to the <a class="xref" href="#store.memstore" title="9.7.6.1.&nbsp;MemStore">Section&nbsp;9.7.6.1, &#8220;MemStore&#8221;</a> for the affected <a class="xref" href="#store" title="9.7.6.&nbsp;Store">Section&nbsp;9.7.6, &#8220;Store&#8221;</a>.
        This ensures that HBase has durable writes. Without WAL, there is the possibility of data loss in the case of a RegionServer failure
        before each MemStore is flushed and new StoreFiles are written.  <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/regionserver/wal/HLog.html" target="_top">HLog</a>
        is the HBase WAL implementation, and there is one HLog instance per RegionServer.
       </p>The WAL is in HDFS in <code class="filename">/hbase/.logs/</code> with subdirectories per region.
       <p>
        For more general information about the concept of write ahead logs, see the Wikipedia
        <a class="link" href="http://en.wikipedia.org/wiki/Write-ahead_logging" target="_top">Write-Ahead Log</a> article.
       </p></div><div class="section" title="9.6.5.2.&nbsp;WAL Flushing"><div class="titlepage"><div><div><h4 class="title"><a name="wal_flush"></a>9.6.5.2.&nbsp;WAL Flushing</h4></div></div></div><p>TODO (describe).
          </p></div><div class="section" title="9.6.5.3.&nbsp;WAL Splitting"><div class="titlepage"><div><div><h4 class="title"><a name="wal_splitting"></a>9.6.5.3.&nbsp;WAL Splitting</h4></div></div></div><div class="section" title="9.6.5.3.1.&nbsp;How edits are recovered from a crashed RegionServer"><div class="titlepage"><div><div><h5 class="title"><a name="d366e7015"></a>9.6.5.3.1.&nbsp;How edits are recovered from a crashed RegionServer</h5></div></div></div><p>When a RegionServer crashes, it will lose its ephemeral lease in
         ZooKeeper...TODO</p></div><div class="section" title="9.6.5.3.2.&nbsp;hbase.hlog.split.skip.errors"><div class="titlepage"><div><div><h5 class="title"><a name="d366e7020"></a>9.6.5.3.2.&nbsp;<code class="varname">hbase.hlog.split.skip.errors</code></h5></div></div></div><p>When set to <code class="constant">true</code>, any error
        encountered splitting will be logged, the problematic WAL will be
        moved into the <code class="filename">.corrupt</code> directory under the hbase
        <code class="varname">rootdir</code>, and processing will continue. If set to
        <code class="constant">false</code>, the default, the exception will be propagated and the
        split logged as failed.<sup>[<a name="d366e7038" href="#ftn.d366e7038" class="footnote">25</a>]</sup></p></div><div class="section" title="9.6.5.3.3.&nbsp;How EOFExceptions are treated when splitting a crashed RegionServers' WALs"><div class="titlepage"><div><div><h5 class="title"><a name="d366e7044"></a>9.6.5.3.3.&nbsp;How EOFExceptions are treated when splitting a crashed
        RegionServers' WALs</h5></div></div></div><p>If we get an EOF while splitting logs, we proceed with the split
        even when <code class="varname">hbase.hlog.split.skip.errors</code> ==
        <code class="constant">false</code>. An EOF while reading the last log in the
        set of files to split is near-guaranteed since the RegionServer likely
        crashed mid-write of a record. But we'll continue even if we got an
        EOF reading other than the last file in the set.<sup>[<a name="d366e7055" href="#ftn.d366e7055" class="footnote">26</a>]</sup></p></div></div></div></div><div class="section" title="9.7.&nbsp;Regions"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="regions.arch"></a>9.7.&nbsp;Regions</h2></div></div></div><p>Regions are the basic element of availability and
     distribution for tables, and are comprised of a Store per Column Family. The heirarchy of objects
     is as follows:
</p><pre class="programlisting">
<code class="filename">Table</code>       (HBase table)
    <code class="filename">Region</code>       (Regions for the table)
         <code class="filename">Store</code>          (Store per ColumnFamily for each Region for the table)
              <code class="filename">MemStore</code>           (MemStore for each Store for each Region for the table)
              <code class="filename">StoreFile</code>          (StoreFiles for each Store for each Region for the table)
                    <code class="filename">Block</code>             (Blocks within a StoreFile within a Store for each Region for the table)
 </pre><p>
     For a description of what HBase files look like when written to HDFS, see <a class="xref" href="#trouble.namenode.hbase.objects" title="13.7.2.&nbsp;Browsing HDFS for HBase Objects">Section&nbsp;13.7.2, &#8220;Browsing HDFS for HBase Objects&#8221;</a>.
            </p><div class="section" title="9.7.1.&nbsp;"><div class="titlepage"></div><p> In general, HBase is designed to run with a small (20-200) number of relatively large (5-20Gb) regions per server. The considerations for this are as follows:</p><div class="section" title="9.7.1.1.&nbsp;Why cannot I have too many regions?"><div class="titlepage"><div><div><h4 class="title"><a name="too_many_regions"></a>9.7.1.1.&nbsp;Why cannot I have too many regions?</h4></div></div></div><p>
              Typically you want to keep your region count low on HBase for numerous reasons.
              Usually right around 100 regions per RegionServer has yielded the best results.
              Here are some of the reasons below for keeping region count low:
              </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
                          MSLAB requires 2mb per memstore (that's 2mb per family per region).
                          1000 regions that have 2 families each is 3.9GB of heap used, and it's not even storing data yet. NB: the 2MB value is configurable.
                  </p></li><li class="listitem"><p>If you fill all the regions at somewhat the same rate, the global memory usage makes it that it forces tiny
                          flushes when you have too many regions which in turn generates compactions.
                          Rewriting the same data tens of times is the last thing you want.
                          An example is filling 1000 regions (with one family) equally and let's consider a lower bound for global memstore
                          usage of 5GB (the region server would have a big heap).
                          Once it reaches 5GB it will force flush the biggest region,
                          at that point they should almost all have about 5MB of data so
                          it would flush that amount. 5MB inserted later, it would flush another
                          region that will now have a bit over 5MB of data, and so on.
                          This is currently the main limiting factor for the number of regions; see <a class="xref" href="#ops.capacity.regions.count" title="15.9.2.1.&nbsp;Number of regions per RS - upper bound">Section&nbsp;15.9.2.1, &#8220;Number of regions per RS - upper bound&#8221;</a>
                          for detailed formula.
                  </p></li><li class="listitem"><p>The master as is is allergic to tons of regions, and will
                          take a lot of time assigning them and moving them around in batches.
                          The reason is that it's heavy on ZK usage, and it's not very async
                          at the moment (could really be improved -- and has been imporoved a bunch
                          in 0.96 hbase).
                  </p></li><li class="listitem"><p>
                          In older versions of HBase (pre-v2 hfile, 0.90 and previous), tons of regions
                          on a few RS can cause the store file index to rise, increasing heap usage and potentially
                          creating memory pressure or OOME on the RSs
                  </p></li></ol></div><p>
      </p></div><p>Another issue is the effect of the number of regions on mapreduce jobs; it is typical to have one mapper per HBase region.
          Thus, hosting only 5 regions per RS may not be enough to get sufficient number of tasks for a mapreduce job, while 1000 regions will generate far too many tasks.
      </p><p>See <a class="xref" href="#ops.capacity.regions" title="15.9.2.&nbsp;Determining region count and size">Section&nbsp;15.9.2, &#8220;Determining region count and size&#8221;</a> for configuration guidelines.</p></div><div class="section" title="9.7.2.&nbsp;Region-RegionServer Assignment"><div class="titlepage"><div><div><h3 class="title"><a name="regions.arch.assignment"></a>9.7.2.&nbsp;Region-RegionServer Assignment</h3></div></div></div><p>This section describes how Regions are assigned to RegionServers.
         </p><div class="section" title="9.7.2.1.&nbsp;Startup"><div class="titlepage"><div><div><h4 class="title"><a name="regions.arch.assignment.startup"></a>9.7.2.1.&nbsp;Startup</h4></div></div></div><p>When HBase starts regions are assigned as follows (short version):
            </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">The Master invokes the <code class="code">AssignmentManager</code> upon startup.
              </li><li class="listitem">The <code class="code">AssignmentManager</code> looks at the existing region assignments in META.
              </li><li class="listitem">If the region assignment is still valid (i.e., if the RegionServer is still online)
                then the assignment is kept.
              </li><li class="listitem">If the assignment is invalid, then the <code class="code">LoadBalancerFactory</code> is invoked to assign the
                region.  The <code class="code">DefaultLoadBalancer</code> will randomly assign the region to a RegionServer.
              </li><li class="listitem">META is updated with the RegionServer assignment (if needed) and the RegionServer start codes
              (start time of the RegionServer process) upon region opening by the RegionServer.
              </li></ol></div><p>
          </p></div><div class="section" title="9.7.2.2.&nbsp;Failover"><div class="titlepage"><div><div><h4 class="title"><a name="regions.arch.assignment.failover"></a>9.7.2.2.&nbsp;Failover</h4></div></div></div><p>When a RegionServer fails (short version):
            </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">The regions immediately become unavailable because the RegionServer is down.
              </li><li class="listitem">The Master will detect that the RegionServer has failed.
              </li><li class="listitem">The region assignments will be considered invalid and will be re-assigned just
                like the startup sequence.
              </li></ol></div><p>
           </p></div><div class="section" title="9.7.2.3.&nbsp;Region Load Balancing"><div class="titlepage"><div><div><h4 class="title"><a name="regions.arch.balancer"></a>9.7.2.3.&nbsp;Region Load Balancing</h4></div></div></div><p>
          Regions can be periodically moved by the <a class="xref" href="#master.processes.loadbalancer" title="9.5.4.1.&nbsp;LoadBalancer">Section&nbsp;9.5.4.1, &#8220;LoadBalancer&#8221;</a>.
          </p></div></div><div class="section" title="9.7.3.&nbsp;Region-RegionServer Locality"><div class="titlepage"><div><div><h3 class="title"><a name="regions.arch.locality"></a>9.7.3.&nbsp;Region-RegionServer Locality</h3></div></div></div><p>Over time, Region-RegionServer locality is achieved via HDFS block replication.
          The HDFS client does the following by default when choosing locations to write replicas:
           </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">First replica is written to local node
             </li><li class="listitem">Second replica is written to a random node on another rack
             </li><li class="listitem">Third replica is written on the same rack as the second, but on a different node chosen randomly
             </li><li class="listitem">Subsequent replicas are written on random nodes on the cluster
<sup>[<a name="d366e7188" href="#ftn.d366e7188" class="footnote">27</a>]</sup></li></ol></div><p>
          Thus, HBase eventually achieves locality for a region after a flush or a compaction.
          In a RegionServer failover situation a RegionServer may be assigned regions with non-local
          StoreFiles (because none of the replicas are local), however as new data is written
          in the region, or the table is compacted and StoreFiles are re-written, they will become "local"
          to the RegionServer.
        </p><p>For more information, see <span class="emphasis"><em>Replica Placement: The First Baby Steps</em></span> on this page: <a class="link" href="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html" target="_top">HDFS Architecture</a>
        and also Lars George's blog on <a class="link" href="http://www.larsgeorge.com/2010/05/hbase-file-locality-in-hdfs.html" target="_top">HBase and HDFS locality</a>.
        </p></div><div class="section" title="9.7.4.&nbsp;Region Splits"><div class="titlepage"><div><div><h3 class="title"><a name="arch.region.splits"></a>9.7.4.&nbsp;Region Splits</h3></div></div></div><p>Regions split when they reach a configured threshold.
        Below we treat the topic in short.  For a longer exposition,
        see <a class="link" href="http://hortonworks.com/blog/apache-hbase-region-splitting-and-merging/" target="_top">Apache HBase Region Splitting and Merging</a>
        by our Enis Soztutar.
        </p><p>Splits run unaided on the RegionServer; i.e. the Master does not
        participate. The RegionServer splits a region, offlines the split
        region and then adds the daughter regions to META, opens daughters on
        the parent's hosting RegionServer and then reports the split to the
        Master. See <a class="xref" href="#disable.splitting" title="2.5.2.7.&nbsp;Managed Splitting">Section&nbsp;2.5.2.7, &#8220;Managed Splitting&#8221;</a> for how to manually manage
        splits (and for why you might do this)</p><div class="section" title="9.7.4.1.&nbsp;Custom Split Policies"><div class="titlepage"><div><div><h4 class="title"><a name="d366e7220"></a>9.7.4.1.&nbsp;Custom Split Policies</h4></div></div></div><p>The default split policy can be overwritten using a custom <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/regionserver/RegionSplitPolicy.html" target="_top">RegionSplitPolicy</a> (HBase 0.94+).
          Typically a custom split policy should extend HBase's default split policy: <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/regionserver/ConstantSizeRegionSplitPolicy.html" target="_top">ConstantSizeRegionSplitPolicy</a>.
          </p><p>The policy can set globally through the HBaseConfiguration used or on a per table basis:
</p><pre class="programlisting">
HTableDescriptor myHtd = ...;
myHtd.setValue(HTableDescriptor.SPLIT_POLICY, MyCustomSplitPolicy.class.getName());
</pre><p>
          </p></div></div><div class="section" title="9.7.5.&nbsp;Online Region Merges"><div class="titlepage"><div><div><h3 class="title"><a name="d366e7236"></a>9.7.5.&nbsp;Online Region Merges</h3></div></div></div><p>Both Master and Regionserver participate in the event of online region merges.
        Client sends merge RPC to master, then master moves the regions together to the
        same regionserver where the more heavily loaded region resided, finally master
        send merge request to this regionserver and regionserver run the region merges.
        Similar with process of region splits, region merges run as a local transaction
        on the regionserver, offlines the regions and then merges two regions on the file
        system, atomically delete merging regions from META and add merged region to the META,
        opens merged region on the regionserver and reports the merge to Master at last.
        </p><p>An example of region merges in the hbase shell
          </p><pre class="programlisting">$ hbase&gt; merge_region 'ENCODED_REGIONNAME', 'ENCODED_REGIONNAME'
          hbase&gt; merge_region 'ENCODED_REGIONNAME', 'ENCODED_REGIONNAME', true
          </pre><p>
          It's an asynchronous operation and call returns immediately without waiting merge completed.
          Passing 'true' as the optional third parameter will force a merge ('force' merges regardless
          else merge will fail unless passed adjacent regions. 'force' is for expert use only)
        </p></div><div class="section" title="9.7.6.&nbsp;Store"><div class="titlepage"><div><div><h3 class="title"><a name="store"></a>9.7.6.&nbsp;Store</h3></div></div></div><p>A Store hosts a MemStore and 0 or more StoreFiles (HFiles). A Store corresponds to a column family for a table for a given region.
          </p><div class="section" title="9.7.6.1.&nbsp;MemStore"><div class="titlepage"><div><div><h4 class="title"><a name="store.memstore"></a>9.7.6.1.&nbsp;MemStore</h4></div></div></div><p>The MemStore holds in-memory modifications to the Store.  Modifications are KeyValues.
       When asked to flush, current memstore is moved to snapshot and is cleared.
       HBase continues to serve edits out of new memstore and backing snapshot until flusher reports in that the
       flush succeeded. At this point the snapshot is let go.</p></div><div class="section" title="9.7.6.2.&nbsp;StoreFile (HFile)"><div class="titlepage"><div><div><h4 class="title"><a name="hfile"></a>9.7.6.2.&nbsp;StoreFile (HFile)</h4></div></div></div><p>StoreFiles are where your data lives.
      </p><div class="section" title="9.7.6.2.1.&nbsp;HFile Format"><div class="titlepage"><div><div><h5 class="title"><a name="d366e7261"></a>9.7.6.2.1.&nbsp;HFile Format</h5></div></div></div><p>The <span class="emphasis"><em>hfile</em></span> file format is based on
              the SSTable file described in the <a class="link" href="http://research.google.com/archive/bigtable.html" target="_top">BigTable [2006]</a> paper and on
              Hadoop's <a class="link" href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/io/file/tfile/TFile.html" target="_top">tfile</a>
              (The unit test suite and the compression harness were taken directly from tfile).
              Schubert Zhang's blog post on HFile: A Block-Indexed File Format to Store Sorted Key-Value Pairs makes for a thorough introduction to HBase's hfile.  Matteo Bertozzi has also put up a
              helpful description, <a class="link" href="http://th30z.blogspot.com/2011/02/hbase-io-hfile.html?spref=tw" target="_top">HBase I/O: HFile</a>.
          </p><p>For more information, see the <a class="link" href="http://hbase.apache.org/xref/org/apache/hadoop/hbase/io/hfile/HFile.html" target="_top">HFile source code</a>.
          Also see <a class="xref" href="#hfilev2" title="Appendix&nbsp;E.&nbsp;HFile format version 2">Appendix&nbsp;E, <i>HFile format version 2</i></a> for information about the HFile v2 format that was included in 0.92.
          </p></div><div class="section" title="9.7.6.2.2.&nbsp;HFile Tool"><div class="titlepage"><div><div><h5 class="title"><a name="hfile_tool"></a>9.7.6.2.2.&nbsp;HFile Tool</h5></div></div></div><p>To view a textualized version of hfile content, you can do use
        the <code class="classname">org.apache.hadoop.hbase.io.hfile.HFile
        </code>tool. Type the following to see usage:</p><pre class="programlisting"><code class="code">$ ${HBASE_HOME}/bin/hbase org.apache.hadoop.hbase.io.hfile.HFile </code> </pre><p>For
        example, to view the content of the file
        <code class="filename">hdfs://10.81.47.41:8020/hbase/TEST/1418428042/DSMP/4759508618286845475</code>,
        type the following:</p><pre class="programlisting"> <code class="code">$ ${HBASE_HOME}/bin/hbase org.apache.hadoop.hbase.io.hfile.HFile -v -f hdfs://10.81.47.41:8020/hbase/TEST/1418428042/DSMP/4759508618286845475 </code> </pre><p>If
        you leave off the option -v to see just a summary on the hfile. See
        usage for other things to do with the <code class="classname">HFile</code>
        tool.</p></div><div class="section" title="9.7.6.2.3.&nbsp;StoreFile Directory Structure on HDFS"><div class="titlepage"><div><div><h5 class="title"><a name="store.file.dir"></a>9.7.6.2.3.&nbsp;StoreFile Directory Structure on HDFS</h5></div></div></div><p>For more information of what StoreFiles look like on HDFS with respect to the directory structure, see <a class="xref" href="#trouble.namenode.hbase.objects" title="13.7.2.&nbsp;Browsing HDFS for HBase Objects">Section&nbsp;13.7.2, &#8220;Browsing HDFS for HBase Objects&#8221;</a>.
        </p></div></div><div class="section" title="9.7.6.3.&nbsp;Blocks"><div class="titlepage"><div><div><h4 class="title"><a name="hfile.blocks"></a>9.7.6.3.&nbsp;Blocks</h4></div></div></div><p>StoreFiles are composed of blocks.  The blocksize is configured on a per-ColumnFamily basis.
        </p><p>Compression happens at the block level within StoreFiles.  For more information on compression, see <a class="xref" href="#compression" title="Appendix&nbsp;C.&nbsp;Compression In HBase">Appendix&nbsp;C, <i>Compression In HBase</i></a>.
        </p><p>For more information on blocks, see the <a class="link" href="http://hbase.apache.org/xref/org/apache/hadoop/hbase/io/hfile/HFileBlock.html" target="_top">HFileBlock source code</a>.
        </p></div><div class="section" title="9.7.6.4.&nbsp;KeyValue"><div class="titlepage"><div><div><h4 class="title"><a name="keyvalue"></a>9.7.6.4.&nbsp;KeyValue</h4></div></div></div><p>The KeyValue class is the heart of data storage in HBase.  KeyValue wraps a byte array and takes offsets and lengths into passed array
         at where to start interpreting the content as KeyValue.
        </p><p>The KeyValue format inside a byte array is:
           </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">keylength</li><li class="listitem">valuelength</li><li class="listitem">key</li><li class="listitem">value</li></ul></div><p>
        </p><p>The Key is further decomposed as:
           </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">rowlength</li><li class="listitem">row (i.e., the rowkey)</li><li class="listitem">columnfamilylength</li><li class="listitem">columnfamily</li><li class="listitem">columnqualifier</li><li class="listitem">timestamp</li><li class="listitem">keytype (e.g., Put, Delete, DeleteColumn, DeleteFamily)</li></ul></div><p>
        </p><p>KeyValue instances are <span class="emphasis"><em>not</em></span> split across blocks.
         For example, if there is an 8 MB KeyValue, even if the block-size is 64kb this KeyValue will be read
         in as a coherent block.  For more information, see the <a class="link" href="http://hbase.apache.org/xref/org/apache/hadoop/hbase/KeyValue.html" target="_top">KeyValue source code</a>.
        </p><div class="section" title="9.7.6.4.1.&nbsp;Example"><div class="titlepage"><div><div><h5 class="title"><a name="keyvalue.example"></a>9.7.6.4.1.&nbsp;Example</h5></div></div></div><p>To emphasize the points above, examine what happens with two Puts for two different columns for the same row:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Put #1:  <code class="code">rowkey=row1, cf:attr1=value1</code></li><li class="listitem">Put #2:  <code class="code">rowkey=row1, cf:attr2=value2</code></li></ul></div><p>Even though these are for the same row, a KeyValue is created for each column:</p><p>Key portion for Put #1:
           </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">rowlength <code class="code">------------&gt; 4</code></li><li class="listitem">row <code class="code">-----------------&gt; row1</code></li><li class="listitem">columnfamilylength <code class="code">---&gt; 2</code></li><li class="listitem">columnfamily <code class="code">--------&gt; cf</code></li><li class="listitem">columnqualifier <code class="code">------&gt; attr1</code></li><li class="listitem">timestamp <code class="code">-----------&gt; server time of Put</code></li><li class="listitem">keytype <code class="code">-------------&gt; Put</code></li></ul></div><p>
          </p><p>Key portion for Put #2:
           </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">rowlength <code class="code">------------&gt; 4</code></li><li class="listitem">row <code class="code">-----------------&gt; row1</code></li><li class="listitem">columnfamilylength <code class="code">---&gt; 2</code></li><li class="listitem">columnfamily <code class="code">--------&gt; cf</code></li><li class="listitem">columnqualifier <code class="code">------&gt; attr2</code></li><li class="listitem">timestamp <code class="code">-----------&gt; server time of Put</code></li><li class="listitem">keytype <code class="code">-------------&gt; Put</code></li></ul></div><p>
           
          </p></div><p>It is critical to understand that the rowkey, ColumnFamily, and column (aka columnqualifier) are embedded within
       the KeyValue instance.  The longer these identifiers are, the bigger the KeyValue is.</p></div><div class="section" title="9.7.6.5.&nbsp;Compaction"><div class="titlepage"><div><div><h4 class="title"><a name="compaction"></a>9.7.6.5.&nbsp;Compaction</h4></div></div></div><p>There are two types of compactions:  minor and major.  Minor compactions will usually pick up a couple of the smaller adjacent
         StoreFiles and rewrite them as one.  Minors do not drop deletes or expired cells, only major compactions do this.  Sometimes a minor compaction
         will pick up all the StoreFiles in the Store and in this case it actually promotes itself to being a major compaction.
         </p><p>After a major compaction runs there will be a single StoreFile per Store, and this will help performance usually.  Caution:  major compactions rewrite all of the Stores data and on a loaded system, this may not be tenable;
             major compactions will usually have to be done manually on large systems.  See <a class="xref" href="#managed.compactions" title="2.5.2.8.&nbsp;Managed Compactions">Section&nbsp;2.5.2.8, &#8220;Managed Compactions&#8221;</a>.
        </p><p>Compactions will <span class="emphasis"><em>not</em></span> perform region merges.  See <a class="xref" href="#ops.regionmgt.merge" title="15.2.2.&nbsp;Merge">Section&nbsp;15.2.2, &#8220;Merge&#8221;</a> for more information on region merging.
        </p><div class="section" title="9.7.6.5.1.&nbsp;Compaction File Selection"><div class="titlepage"><div><div><h5 class="title"><a name="compaction.file.selection"></a>9.7.6.5.1.&nbsp;Compaction File Selection</h5></div></div></div><p>To understand the core algorithm for StoreFile selection, there is some ASCII-art in the <a class="link" href="http://hbase.apache.org/xref/org/apache/hadoop/hbase/regionserver/Store.html#836" target="_top">Store source code</a> that
          will serve as useful reference.  It has been copied below:
</p><pre class="programlisting">
/* normal skew:
 *
 *         older ----&gt; newer
 *     _
 *    | |   _
 *    | |  | |   _
 *  --|-|- |-|- |-|---_-------_-------  minCompactSize
 *    | |  | |  | |  | |  _  | |
 *    | |  | |  | |  | | | | | |
 *    | |  | |  | |  | | | | | |
 */
</pre><p>
          Important knobs:
          </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><code class="code">hbase.store.compaction.ratio</code> Ratio used in compaction
            file selection algorithm (default 1.2f). </li><li class="listitem"><code class="code">hbase.hstore.compaction.min</code> (.90 hbase.hstore.compactionThreshold) (files) Minimum number
            of StoreFiles per Store to be selected for a compaction to occur (default 2).</li><li class="listitem"><code class="code">hbase.hstore.compaction.max</code> (files) Maximum number of StoreFiles to compact per minor compaction (default 10).</li><li class="listitem"><code class="code">hbase.hstore.compaction.min.size</code> (bytes)
            Any StoreFile smaller than this setting with automatically be a candidate for compaction.  Defaults to
            <code class="code">hbase.hregion.memstore.flush.size</code> (128 mb). </li><li class="listitem"><code class="code">hbase.hstore.compaction.max.size</code> (.92) (bytes)
            Any StoreFile larger than this setting with automatically be excluded from compaction (default Long.MAX_VALUE). </li></ul></div><p>
          </p><p>The minor compaction StoreFile selection logic is size based, and selects a file for compaction when the file
           &lt;= sum(smaller_files) * <code class="code">hbase.hstore.compaction.ratio</code>.
          </p></div><div class="section" title="9.7.6.5.2.&nbsp;Minor Compaction File Selection - Example #1 (Basic Example)"><div class="titlepage"><div><div><h5 class="title"><a name="compaction.file.selection.example1"></a>9.7.6.5.2.&nbsp;Minor Compaction File Selection - Example #1 (Basic Example)</h5></div></div></div><p>This example mirrors an example from the unit test <code class="code">TestCompactSelection</code>.
          </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><code class="code">hbase.store.compaction.ratio</code> = 1.0f </li><li class="listitem"><code class="code">hbase.hstore.compaction.min</code> = 3 (files) </li><li class="listitem"><code class="code">hbase.hstore.compaction.max</code> = 5 (files) </li><li class="listitem"><code class="code">hbase.hstore.compaction.min.size</code> = 10 (bytes) </li><li class="listitem"><code class="code">hbase.hstore.compaction.max.size</code> = 1000 (bytes) </li></ul></div><p>
          The following StoreFiles exist: 100, 50, 23, 12, and 12 bytes apiece (oldest to newest).
          With the above parameters, the files that would be selected for minor compaction are 23, 12, and 12.
          </p><p>Why?
          </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">100 --&gt;  No, because sum(50, 23, 12, 12) * 1.0 = 97. </li><li class="listitem">50 --&gt;  No, because sum(23, 12, 12) * 1.0 = 47. </li><li class="listitem">23 --&gt;  Yes, because sum(12, 12) * 1.0 = 24. </li><li class="listitem">12 --&gt;  Yes, because the previous file has been included, and because this
          does not exceed the the max-file limit of 5  </li><li class="listitem">12 --&gt;  Yes, because the previous file had been included, and because this
          does not exceed the the max-file limit of 5.</li></ul></div><p>
          </p></div><div class="section" title="9.7.6.5.3.&nbsp;Minor Compaction File Selection - Example #2 (Not Enough Files To Compact)"><div class="titlepage"><div><div><h5 class="title"><a name="compaction.file.selection.example2"></a>9.7.6.5.3.&nbsp;Minor Compaction File Selection - Example #2 (Not Enough Files To Compact)</h5></div></div></div><p>This example mirrors an example from the unit test <code class="code">TestCompactSelection</code>.
          </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><code class="code">hbase.store.compaction.ratio</code> = 1.0f </li><li class="listitem"><code class="code">hbase.hstore.compaction.min</code> = 3 (files) </li><li class="listitem"><code class="code">hbase.hstore.compaction.max</code> = 5 (files) </li><li class="listitem"><code class="code">hbase.hstore.compaction.min.size</code> = 10 (bytes) </li><li class="listitem"><code class="code">hbase.hstore.compaction.max.size</code> = 1000 (bytes) </li></ul></div><p>
          </p><p>The following StoreFiles exist: 100, 25, 12, and 12 bytes apiece (oldest to newest).
          With the above parameters, no compaction will be started.
          </p><p>Why?
          </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">100 --&gt; No, because sum(25, 12, 12) * 1.0 = 47</li><li class="listitem">25 --&gt;  No, because sum(12, 12) * 1.0 = 24</li><li class="listitem">12 --&gt;  No. Candidate because sum(12) * 1.0 = 12, there are only 2 files to compact and that is less than the threshold of 3</li><li class="listitem">12 --&gt;  No. Candidate because the previous StoreFile was, but there are not enough files to compact</li></ul></div><p>
          </p></div><div class="section" title="9.7.6.5.4.&nbsp;Minor Compaction File Selection - Example #3 (Limiting Files To Compact)"><div class="titlepage"><div><div><h5 class="title"><a name="compaction.file.selection.example2"></a>9.7.6.5.4.&nbsp;Minor Compaction File Selection - Example #3 (Limiting Files To Compact)</h5></div></div></div><p>This example mirrors an example from the unit test <code class="code">TestCompactSelection</code>.
          </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><code class="code">hbase.store.compaction.ratio</code> = 1.0f </li><li class="listitem"><code class="code">hbase.hstore.compaction.min</code> = 3 (files) </li><li class="listitem"><code class="code">hbase.hstore.compaction.max</code> = 5 (files) </li><li class="listitem"><code class="code">hbase.hstore.compaction.min.size</code> = 10 (bytes) </li><li class="listitem"><code class="code">hbase.hstore.compaction.max.size</code> = 1000 (bytes) </li></ul></div><p>
          The following StoreFiles exist: 7, 6, 5, 4, 3, 2, and 1 bytes apiece (oldest to newest).
          With the above parameters, the files that would be selected for minor compaction are 7, 6, 5, 4, 3.
          </p><p>Why?
          </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">7 --&gt;  Yes, because sum(6, 5, 4, 3, 2, 1) * 1.0 = 21.  Also, 7 is less than the min-size</li><li class="listitem">6 --&gt;  Yes, because sum(5, 4, 3, 2, 1) * 1.0 = 15.  Also, 6 is less than the min-size. </li><li class="listitem">5 --&gt;  Yes, because sum(4, 3, 2, 1) * 1.0 = 10.  Also, 5 is less than the min-size. </li><li class="listitem">4 --&gt;  Yes, because sum(3, 2, 1) * 1.0 = 6.  Also, 4 is less than the min-size. </li><li class="listitem">3 --&gt;  Yes, because sum(2, 1) * 1.0 = 3.  Also, 3 is less than the min-size. </li><li class="listitem">2 --&gt;  No.  Candidate because previous file was selected and 2 is less than the min-size, but the max-number of files to compact has been reached. </li><li class="listitem">1 --&gt;  No.  Candidate because previous file was selected and 1 is less than the min-size, but max-number of files to compact has been reached. </li></ul></div><p>
          </p></div><div class="section" title="9.7.6.5.5.&nbsp;Impact of Key Configuration Options"><div class="titlepage"><div><div><h5 class="title"><a name="compaction.config.impact"></a>9.7.6.5.5.&nbsp;Impact of Key Configuration Options</h5></div></div></div><p><code class="code">hbase.store.compaction.ratio</code>.  A large ratio (e.g., 10) will produce a single giant file.  Conversely, a value of .25 will
          produce behavior similar to the BigTable compaction algorithm - resulting in 4 StoreFiles.
          </p><p><code class="code">hbase.hstore.compaction.min.size</code>.  Because
          this limit represents the "automatic include" limit for all StoreFiles smaller than this value, this value may need to
          be adjusted downwards in write-heavy environments where many 1 or 2 mb StoreFiles are being flushed, because every file
          will be targeted for compaction and the resulting files may still be under the min-size and require further compaction, etc.
          </p></div><div class="section" title="9.7.6.5.6.&nbsp;Experimental: stripe compactions"><div class="titlepage"><div><div><h5 class="title"><a name="ops.stripe"></a>9.7.6.5.6.&nbsp;Experimental: stripe compactions</h5></div></div></div><p>
Stripe compactions is an experimental feature added in HBase 0.98 which aims to improve compactions for large regions or non-uniformly distributed row keys. In order to achieve smaller and/or more granular compactions, the store files within a region are maintained separately for several row-key sub-ranges, or "stripes", of the region. The division is not visible to the higher levels of the system, so externally each region functions as before.
</p><p>
This feature is fully compatible with default compactions - it can be enabled for existing tables, and the table will continue to operate normally if it's disabled later.
</p></div><div class="section" title="9.7.6.5.7.&nbsp;When to use"><div class="titlepage"><div><div><h5 class="title"><a name="ops.stripe.when"></a>9.7.6.5.7.&nbsp;When to use</h5></div></div></div><p>You might want to consider using this feature if you have:
</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
large regions (in that case, you can get the positive effect of much smaller regions without additional memstore and region management overhead); or
</li><li class="listitem">
non-uniform row keys, e.g. time dimension in a key (in that case, only the stripes receiving the new keys will keep compacting - old data will not compact as much, or at all).
</li></ul></div><p>
</p><p>
According to perf testing performed, in these case the read performance can improve somewhat, and the read and write performance variability due to compactions is greatly reduced. There's overall perf improvement on large, non-uniform row key regions (hash-prefixed timestamp key) over long term. All of these performance gains are best realized when table is already large. In future, the perf improvement might also extend to region splits.
</p><div class="section" title="9.7.6.5.7.1.&nbsp;How to enable"><div class="titlepage"><div><div><h6 class="title"><a name="ops.stripe.enable"></a>9.7.6.5.7.1.&nbsp;How to enable</h6></div></div></div><p>
To use stripe compactions for a table or a column family, you should set its  <code class="varname">hbase.hstore.engine.class</code> to <code class="varname">org.apache.hadoop.hbase.regionserver.StripeStoreEngine</code>. Due to the nature of compactions, you also need to set the blocking file count to a high number (100 is a good default, which is 10 times the normal default of 10). If changing the existing table, you should do it when it is disabled. Examples:
</p><pre class="programlisting">
alter 'orders_table', CONFIGURATION =&gt; {'hbase.hstore.engine.class' =&gt; 'org.apache.hadoop.hbase.regionserver.StripeStoreEngine', 'hbase.hstore.blockingStoreFiles' =&gt; '100'}

alter 'orders_table', {NAME =&gt; 'blobs_cf', CONFIGURATION =&gt; {'hbase.hstore.engine.class' =&gt; 'org.apache.hadoop.hbase.regionserver.StripeStoreEngine', 'hbase.hstore.blockingStoreFiles' =&gt; '100'}}

create 'orders_table', 'blobs_cf', CONFIGURATION =&gt; {'hbase.hstore.engine.class' =&gt; 'org.apache.hadoop.hbase.regionserver.StripeStoreEngine', 'hbase.hstore.blockingStoreFiles' =&gt; '100'}
</pre><p>
</p><p>
Then, you can configure the other options if needed (see below) and enable the table.
To switch back to default compactions, set <code class="varname">hbase.hstore.engine.class</code> to nil to unset it; or set it explicitly to "<code class="varname">org.apache.hadoop.hbase.regionserver.DefaultStoreEngine</code>" (this also needs to be done on a disabled table).
</p><p>
When you enable a large table after changing the store engine either way, a major compaction will likely be performed on most regions. This is not a problem with new tables.
</p></div><div class="section" title="9.7.6.5.7.2.&nbsp;How to configure"><div class="titlepage"><div><div><h6 class="title"><a name="ops.stripe.config"></a>9.7.6.5.7.2.&nbsp;How to configure</h6></div></div></div><p>
All of the settings described below are best set on table/cf level (with the table disabled first, for the settings to apply), similar to the above, e.g.
</p><pre class="programlisting">
alter 'orders_table', CONFIGURATION =&gt; {'key' =&gt; 'value', ..., 'key' =&gt; 'value'}}
</pre><p>
</p><div class="section" title="9.7.6.5.7.2.1.&nbsp;Region and stripe sizing"><div class="titlepage"><div><div><h6 class="title"><a name="ops.stripe.config.sizing"></a>9.7.6.5.7.2.1.&nbsp;Region and stripe sizing</h6></div></div></div><p>
Based on your region sizing, you might want to also change your stripe sizing. By default, your new regions will start with one stripe. When the stripe is too big (16 memstore flushes size), on next compaction it will be split into two stripes. Stripe splitting will continue in a similar manner as the region grows, until the region itself is big enough to split (region split will work the same as with default compactions).
</p><p>
You can improve this pattern for your data. You should generally aim at stripe size of at least 1Gb, and about 8-12 stripes for uniform row keys - so, for example if your regions are 30 Gb, 12x2.5Gb stripes might be a good idea.
</p><p>
The settings are as follows:
</p><div class="table"><a name="d366e7739"></a><p class="title"><b>Table&nbsp;9.1.&nbsp;</b></p><div class="table-contents"><table border="1"><colgroup><col align="left" class="c1"><col align="left" class="c2"></colgroup><thead><tr><th align="left">Setting</th><th align="left">Notes</th></tr></thead><tbody><tr><td align="left">
<code class="varname">hbase.store.stripe.initialStripeCount</code>
</td><td align="left">
Initial stripe count to create. You can use it as follows:
<div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
for relatively uniform row keys, if you know the approximate target number of stripes from the above, you can avoid some splitting overhead by starting w/several stripes (2, 5, 10...). Note that if the early data is not representative of overall row key distribution, this will not be as efficient.
</li><li class="listitem">
for existing tables with lots of data, you can use this to pre-split stripes.
</li><li class="listitem">
for e.g. hash-prefixed sequential keys, with more than one hash prefix per region, you know that some pre-splitting makes sense.
</li></ul></div>
</td></tr><tr><td align="left">
<code class="varname">hbase.store.stripe.sizeToSplit</code>
</td><td align="left">
Maximum stripe size before it's split. You can use this in conjunction with the next setting to control target stripe size (sizeToSplit = splitPartsCount * target stripe size), according to the above sizing considerations.
</td></tr><tr><td align="left">
<code class="varname">hbase.store.stripe.splitPartCount</code>
</td><td align="left">
The number of new stripes to create when splitting one. The default is 2, and is good for most cases. For non-uniform row keys, you might experiment with increasing the number somewhat (3-4), to isolate the arriving updates into narrower slice of the region with just one split instead of several.
</td></tr></tbody></table></div></div><p><br class="table-break">
</p></div><div class="section" title="9.7.6.5.7.2.2.&nbsp;Memstore sizing"><div class="titlepage"><div><div><h6 class="title"><a name="ops.stripe.config.memstore"></a>9.7.6.5.7.2.2.&nbsp;Memstore sizing</h6></div></div></div><p>
By default, the flush creates several files from one memstore, according to existing stripe boundaries and row keys to flush. This approach minimizes write amplification, but can be undesirable if memstore is small and there are many stripes (the files will be too small).
</p><p>
In such cases, you can set <code class="varname">hbase.store.stripe.compaction.flushToL0</code> to true. This will cause  flush to create a single file instead; when at least <code class="varname">hbase.store.stripe.compaction.minFilesL0</code> such files (by default, 4) accumulate, they will be compacted into striped files.</p></div><div class="section" title="9.7.6.5.7.2.3.&nbsp;Normal compaction configuration"><div class="titlepage"><div><div><h6 class="title"><a name="ops.stripe.config.compact"></a>9.7.6.5.7.2.3.&nbsp;Normal compaction configuration</h6></div></div></div><p>
All the settings that apply to normal compactions (file size limits, etc.) apply to stripe compactions. The exception are min and max number of files, which are set to higher values by default because the files in stripes are smaller. To control these for stripe compactions, use <code class="varname">hbase.store.stripe.compaction.minFiles</code> and <code class="varname">.maxFiles</code>.
</p></div></div></div></div></div></div><div class="section" title="9.8.&nbsp;Bulk Loading"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="arch.bulk.load"></a>9.8.&nbsp;Bulk Loading</h2></div></div></div><div class="section" title="9.8.1.&nbsp;Overview"><div class="titlepage"><div><div><h3 class="title"><a name="arch.bulk.load.overview"></a>9.8.1.&nbsp;Overview</h3></div></div></div><p>
        HBase includes several methods of loading data into tables.
        The most straightforward method is to either use the <code class="code">TableOutputFormat</code>
        class from a MapReduce job, or use the normal client APIs; however,
        these are not always the most efficient methods.
      </p><p>
        The bulk load feature uses a MapReduce job to output table data in HBase's internal
        data format, and then directly loads the generated StoreFiles into a running
        cluster. Using bulk load will use less CPU and network resources than
        simply using the HBase API.
      </p></div><div class="section" title="9.8.2.&nbsp;Bulk Load Limitations"><div class="titlepage"><div><div><h3 class="title"><a name="arch.bulk.load.limitations"></a>9.8.2.&nbsp;Bulk Load Limitations</h3></div></div></div><p>As bulk loading bypasses the write path, the WAL doesn&#8217;t get written to as part of the process.
            Replication works by reading the WAL files so it won&#8217;t see the bulk loaded data &#8211; and the same goes for the edits that use Put.setWriteToWAL(true).
            One way to handle that is to ship the raw files or the HFiles to the other cluster and do the other processing there.</p></div><div class="section" title="9.8.3.&nbsp;Bulk Load Architecture"><div class="titlepage"><div><div><h3 class="title"><a name="arch.bulk.load.arch"></a>9.8.3.&nbsp;Bulk Load Architecture</h3></div></div></div><p>
        The HBase bulk load process consists of two main steps.
      </p><div class="section" title="9.8.3.1.&nbsp;Preparing data via a MapReduce job"><div class="titlepage"><div><div><h4 class="title"><a name="arch.bulk.load.prep"></a>9.8.3.1.&nbsp;Preparing data via a MapReduce job</h4></div></div></div><p>
          The first step of a bulk load is to generate HBase data files (StoreFiles) from
          a MapReduce job using <code class="code">HFileOutputFormat</code>. This output format writes
          out data in HBase's internal storage format so that they can be
          later loaded very efficiently into the cluster.
        </p><p>
          In order to function efficiently, <code class="code">HFileOutputFormat</code> must be
          configured such that each output HFile fits within a single region.
          In order to do this, jobs whose output will be bulk loaded into HBase
          use Hadoop's <code class="code">TotalOrderPartitioner</code> class to partition the map output
          into disjoint ranges of the key space, corresponding to the key
          ranges of the regions in the table.
        </p><p>
          <code class="code">HFileOutputFormat</code> includes a convenience function,
          <code class="code">configureIncrementalLoad()</code>, which automatically sets up
          a <code class="code">TotalOrderPartitioner</code> based on the current region boundaries of a
          table.
        </p></div><div class="section" title="9.8.3.2.&nbsp;Completing the data load"><div class="titlepage"><div><div><h4 class="title"><a name="arch.bulk.load.complete"></a>9.8.3.2.&nbsp;Completing the data load</h4></div></div></div><p>
          After the data has been prepared using
          <code class="code">HFileOutputFormat</code>, it is loaded into the cluster using
          <code class="code">completebulkload</code>. This command line tool iterates
          through the prepared data files, and for each one determines the
          region the file belongs to. It then contacts the appropriate Region
          Server which adopts the HFile, moving it into its storage directory
          and making the data available to clients.
        </p><p>
          If the region boundaries have changed during the course of bulk load
          preparation, or between the preparation and completion steps, the
          <code class="code">completebulkloads</code> utility will automatically split the
          data files into pieces corresponding to the new boundaries. This
          process is not optimally efficient, so users should take care to
          minimize the delay between preparing a bulk load and importing it
          into the cluster, especially if other clients are simultaneously
          loading data through other means.
        </p></div></div><div class="section" title="9.8.4.&nbsp;Importing the prepared data using the completebulkload tool"><div class="titlepage"><div><div><h3 class="title"><a name="arch.bulk.load.import"></a>9.8.4.&nbsp;Importing the prepared data using the completebulkload tool</h3></div></div></div><p>
        After a data import has been prepared, either by using the
        <code class="code">importtsv</code> tool with the
        "<code class="code">importtsv.bulk.output</code>" option or by some other MapReduce
        job using the <code class="code">HFileOutputFormat</code>, the
        <code class="code">completebulkload</code> tool is used to import the data into the
        running cluster.
      </p><p>
        The <code class="code">completebulkload</code> tool simply takes the output path
        where <code class="code">importtsv</code> or your MapReduce job put its results, and
        the table name to import into. For example:
      </p><code class="code">$ hadoop jar hbase-VERSION.jar completebulkload [-c /path/to/hbase/config/hbase-site.xml] /user/todd/myoutput mytable</code><p>
        The <code class="code">-c config-file</code> option can be used to specify a file
        containing the appropriate hbase parameters (e.g., hbase-site.xml) if
        not supplied already on the CLASSPATH (In addition, the CLASSPATH must
        contain the directory that has the zookeeper configuration file if
        zookeeper is NOT managed by HBase).
      </p><p>
        Note: If the target table does not already exist in HBase, this
        tool will create the table automatically.</p><p>
        This tool will run quickly, after which point the new data will be visible in
        the cluster.
      </p></div><div class="section" title="9.8.5.&nbsp;See Also"><div class="titlepage"><div><div><h3 class="title"><a name="arch.bulk.load.also"></a>9.8.5.&nbsp;See Also</h3></div></div></div><p>For more information about the referenced utilities, see <a class="xref" href="#importtsv" title="15.1.11.&nbsp;ImportTsv">Section&nbsp;15.1.11, &#8220;ImportTsv&#8221;</a> and  <a class="xref" href="#completebulkload" title="15.1.12.&nbsp;CompleteBulkLoad">Section&nbsp;15.1.12, &#8220;CompleteBulkLoad&#8221;</a>.
      </p><p>
          See How-to: Use HBase Bulk Loading, and Why
          for a recent blog on current state of bulk loading.
      </p></div><div class="section" title="9.8.6.&nbsp;Advanced Usage"><div class="titlepage"><div><div><h3 class="title"><a name="arch.bulk.load.adv"></a>9.8.6.&nbsp;Advanced Usage</h3></div></div></div><p>
        Although the <code class="code">importtsv</code> tool is useful in many cases, advanced users may
        want to generate data programatically, or import data from other formats. To get
        started doing so, dig into <code class="code">ImportTsv.java</code> and check the JavaDoc for
        HFileOutputFormat.
      </p><p>
        The import step of the bulk load can also be done programatically. See the
        <code class="code">LoadIncrementalHFiles</code> class for more information.
      </p></div></div><div class="section" title="9.9.&nbsp;HDFS"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="arch.hdfs"></a>9.9.&nbsp;HDFS</h2></div></div></div><p>As HBase runs on HDFS (and each StoreFile is written as a file on HDFS),
        it is important to have an understanding of the HDFS Architecture
         especially in terms of how it stores files, handles failovers, and replicates blocks.
       </p><p>See the Hadoop documentation on <a class="link" href="http://hadoop.apache.org/common/docs/current/hdfs_design.html" target="_top">HDFS Architecture</a>
       for more information.
       </p><div class="section" title="9.9.1.&nbsp;NameNode"><div class="titlepage"><div><div><h3 class="title"><a name="arch.hdfs.nn"></a>9.9.1.&nbsp;NameNode</h3></div></div></div><p>The NameNode is responsible for maintaining the filesystem metadata.  See the above HDFS Architecture link
         for more information.
         </p></div><div class="section" title="9.9.2.&nbsp;DataNode"><div class="titlepage"><div><div><h3 class="title"><a name="arch.hdfs.dn"></a>9.9.2.&nbsp;DataNode</h3></div></div></div><p>The DataNodes are responsible for storing HDFS blocks.  See the above HDFS Architecture link
         for more information.
         </p></div></div><div class="footnotes"><br><hr width="100" align="left"><div class="footnote"><p><sup>[<a id="ftn.d366e6789" href="#d366e6789" class="para">24</a>] </sup>J Mohamed Zahoor goes into some more detail on the Master Architecture in this blog posting, <a class="link" href="http://blog.zahoor.in/2012/08/hbase-hmaster-architecture/" target="_top">HBase HMaster Architecture
            </a>.</p></div><div class="footnote"><p><sup>[<a id="ftn.d366e7038" href="#d366e7038" class="para">25</a>] </sup>See <a class="link" href="https://issues.apache.org/jira/browse/HBASE-2958" target="_top">HBASE-2958
            When hbase.hlog.split.skip.errors is set to false, we fail the
            split but thats it</a>. We need to do more than just fail split
            if this flag is set.</p></div><div class="footnote"><p><sup>[<a id="ftn.d366e7055" href="#d366e7055" class="para">26</a>] </sup>For background, see <a class="link" href="https://issues.apache.org/jira/browse/HBASE-2643" target="_top">HBASE-2643
            Figure how to deal with eof splitting logs</a></p></div><div class="footnote"><p><sup>[<a id="ftn.d366e7188" href="#d366e7188" class="para">27</a>] </sup>See <span class="emphasis"><em>Replica Placement: The First Baby Steps</em></span> on this page: <a class="link" href="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html" target="_top">HDFS Architecture</a></p></div></div></div><div class="chapter" title="Chapter&nbsp;10.&nbsp;Apache HBase External APIs"><div class="titlepage"><div><div><h2 class="title"><a name="external_apis"></a>Chapter&nbsp;10.&nbsp;Apache HBase External APIs</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#nonjava.jvm">10.1. Non-Java Languages Talking to the JVM</a></span></dt><dt><span class="section"><a href="#rest">10.2. REST</a></span></dt><dt><span class="section"><a href="#thrift">10.3. Thrift</a></span></dt><dd><dl><dt><span class="section"><a href="#thrift.filter-language">10.3.1. Filter Language</a></span></dt></dl></dd><dt><span class="section"><a href="#c">10.4. C/C++ Apache HBase Client</a></span></dt></dl></div>
  This chapter will cover access to Apache HBase either through non-Java languages, or through custom protocols.

  <div class="section" title="10.1.&nbsp;Non-Java Languages Talking to the JVM"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="nonjava.jvm"></a>10.1.&nbsp;Non-Java Languages Talking to the JVM</h2></div></div></div><p>Currently the documentation on this topic in the
      <a class="link" href="http://wiki.apache.org/hadoop/Hbase" target="_top">Apache HBase Wiki</a>.
      See also the <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/thrift/package-summary.html#package_description" target="_top">Thrift API Javadoc</a>.
    </p></div><div class="section" title="10.2.&nbsp;REST"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="rest"></a>10.2.&nbsp;REST</h2></div></div></div><p>Currently most of the documentation on REST exists in the
        <a class="link" href="http://wiki.apache.org/hadoop/Hbase/Stargate" target="_top">Apache HBase Wiki on REST</a> (The REST gateway used to be
        called 'Stargate').  There are also a nice set of blogs on <a class="link" href="http://blog.cloudera.com/blog/2013/03/how-to-use-the-apache-hbase-rest-interface-part-1/" target="_top">How-to: Use the Apache HBase REST Interface</a>
        by Jesse Anderson.
    </p></div><div class="section" title="10.3.&nbsp;Thrift"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="thrift"></a>10.3.&nbsp;Thrift</h2></div></div></div><p>Currently most of the documentation on Thrift exists in the
      <a class="link" href="http://wiki.apache.org/hadoop/Hbase/ThriftApi" target="_top">Apache HBase Wiki on Thrift</a>.
    </p><div class="section" title="10.3.1.&nbsp;Filter Language"><div class="titlepage"><div><div><h3 class="title"><a name="thrift.filter-language"></a>10.3.1.&nbsp;Filter Language</h3></div></div></div><div class="section" title="10.3.1.1.&nbsp;Use Case"><div class="titlepage"><div><div><h4 class="title"><a name="use-case"></a>10.3.1.1.&nbsp;Use Case</h4></div></div></div><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>this feature was introduced in Apache HBase 0.92</p></div><p>This allows the user to perform server-side filtering when accessing HBase over Thrift (or in the shell -- see the 'scan' help in the shell).
               The user specifies a filter via a string. The string is parsed on the server to construct the filter</p></div><div class="section" title="10.3.1.2.&nbsp;General Filter String Syntax"><div class="titlepage"><div><div><h4 class="title"><a name="general-syntax"></a>10.3.1.2.&nbsp;General Filter String Syntax</h4></div></div></div><p>A simple filter expression is expressed as: <code class="code">&#8220;FilterName (argument, argument, ... , argument)&#8221;</code></p><p>You must specify the name of the filter followed by the argument list in parenthesis. Commas separate the individual arguments</p><p>If the argument represents a string, it should be enclosed in single quotes.</p><p>If it represents a boolean, an integer or a comparison operator like &lt;,
                 &gt;, != etc. it should not be enclosed in quotes</p><p>The filter name must be one word. All ASCII characters are allowed except for whitespace, single quotes and parenthesis.</p><p>The filter&#8217;s arguments can contain any ASCII character. <code class="code">If single quotes are present in the argument, they must be escaped by a
                   preceding single quote</code></p></div><div class="section" title="10.3.1.3.&nbsp;Compound Filters and Operators"><div class="titlepage"><div><div><h4 class="title"><a name="compound-filters-and-operators"></a>10.3.1.3.&nbsp;Compound Filters and Operators</h4></div></div></div><p>Currently, two binary operators &#8211; AND/OR and two unary operators &#8211; WHILE/SKIP are supported.</p><p>Note: the operators are all in uppercase</p><p><span class="bold"><strong>AND</strong></span> &#8211; as the name suggests, if this
                 operator is used, the key-value must pass both the filters</p><p><span class="bold"><strong>OR</strong></span> &#8211; as the name suggests, if this operator
                 is used, the key-value must pass at least one of the filters</p><p><span class="bold"><strong>SKIP</strong></span> &#8211; For a particular row, if any of the
                 key-values don&#8217;t pass the filter condition, the entire row is skipped</p><p><span class="bold"><strong>WHILE</strong></span> - For a particular row, it continues
                 to emit key-values until a key-value is reached that fails the filter condition</p><p><span class="bold"><strong>Compound Filters:</strong></span> Using these operators, a
                 hierarchy of filters can be created. For example: <code class="code">&#8220;(Filter1 AND Filter2) OR (Filter3 AND Filter4)&#8221;</code></p></div><div class="section" title="10.3.1.4.&nbsp;Order of Evaluation"><div class="titlepage"><div><div><h4 class="title"><a name="order-of-evaluation"></a>10.3.1.4.&nbsp;Order of Evaluation</h4></div></div></div><p>Parenthesis have the highest precedence. The SKIP and WHILE operators are next and have the same precedence.The AND operator has the next highest precedence followed by the OR operator.</p><p>For example:</p><p>A filter string of the form:<code class="code">&#8220;Filter1 AND Filter2 OR Filter3&#8221;</code>
                 will be evaluated as:<code class="code">&#8220;(Filter1 AND Filter2) OR Filter3&#8221;</code></p><p>A filter string of the form:<code class="code">&#8220;Filter1 AND SKIP Filter2 OR Filter3&#8221;</code>
                 will be evaluated as:<code class="code">&#8220;(Filter1 AND (SKIP Filter2)) OR Filter3&#8221;</code></p></div><div class="section" title="10.3.1.5.&nbsp;Compare Operator"><div class="titlepage"><div><div><h4 class="title"><a name="compare-operator"></a>10.3.1.5.&nbsp;Compare Operator</h4></div></div></div><p>A compare operator can be any of the following:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>LESS (&lt;)</p></li><li class="listitem"><p>LESS_OR_EQUAL (&lt;=)</p></li><li class="listitem"><p>EQUAL (=)</p></li><li class="listitem"><p>NOT_EQUAL (!=)</p></li><li class="listitem"><p>GREATER_OR_EQUAL (&gt;=)</p></li><li class="listitem"><p>GREATER (&gt;)</p></li><li class="listitem"><p>NO_OP (no operation)</p></li></ol></div><p>The client should use the symbols (&lt;, &lt;=, =, !=, &gt;, &gt;=) to express
                 compare operators.</p></div><div class="section" title="10.3.1.6.&nbsp;Comparator"><div class="titlepage"><div><div><h4 class="title"><a name="comparator"></a>10.3.1.6.&nbsp;Comparator</h4></div></div></div><p>A comparator can be any of the following:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p><span class="bold"><strong>BinaryComparator</strong></span> - This
                     lexicographically compares against the specified byte array using
                     Bytes.compareTo(byte[], byte[])</p></li><li class="listitem"><p><span class="bold"><strong>BinaryPrefixComparator</strong></span> - This
                     lexicographically compares against a specified byte array. It only compares up to
                     the length of this byte array.</p></li><li class="listitem"><p><span class="bold"><strong>RegexStringComparator</strong></span> - This compares
                     against the specified byte array using the given regular expression. Only EQUAL
                     and NOT_EQUAL comparisons are valid with this comparator</p></li><li class="listitem"><p><span class="bold"><strong>SubStringComparator</strong></span> - This tests if
                     the given substring appears in a specified byte array. The comparison is case
                     insensitive. Only EQUAL and NOT_EQUAL comparisons are valid with this
                     comparator</p></li></ol></div><p>The general syntax of a comparator is:<code class="code"> ComparatorType:ComparatorValue</code></p><p>The ComparatorType for the various comparators is as follows:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p><span class="bold"><strong>BinaryComparator</strong></span> - binary</p></li><li class="listitem"><p><span class="bold"><strong>BinaryPrefixComparator</strong></span> - binaryprefix</p></li><li class="listitem"><p><span class="bold"><strong>RegexStringComparator</strong></span> - regexstring</p></li><li class="listitem"><p><span class="bold"><strong>SubStringComparator</strong></span> - substring</p></li></ol></div><p>The ComparatorValue can be any value.</p><p>Example1:<code class="code"> &gt;, 'binary:abc' </code>will match everything that is lexicographically greater than "abc" </p><p>Example2:<code class="code"> =, 'binaryprefix:abc' </code>will match everything whose first 3 characters are lexicographically equal to "abc"</p><p>Example3:<code class="code"> !=, 'regexstring:ab*yz' </code>will match everything that doesn't begin with "ab" and ends with "yz"</p><p>Example4:<code class="code"> =, 'substring:abc123' </code>will match everything that begins with the substring "abc123"</p></div><div class="section" title="10.3.1.7.&nbsp;Example PHP Client Program that uses the Filter Language"><div class="titlepage"><div><div><h4 class="title"><a name="example PHP Client Program"></a>10.3.1.7.&nbsp;Example PHP Client Program that uses the Filter Language</h4></div></div></div><pre class="programlisting">
&lt;? $_SERVER['PHP_ROOT'] = realpath(dirname(__FILE__).'/..');
   require_once $_SERVER['PHP_ROOT'].'/flib/__flib.php';
   flib_init(FLIB_CONTEXT_SCRIPT);
   require_module('storage/hbase');
   $hbase = new HBase('&lt;server_name_running_thrift_server&gt;', &lt;port on which thrift server is running&gt;);
   $hbase-&gt;open();
   $client = $hbase-&gt;getClient();
   $result = $client-&gt;scannerOpenWithFilterString('table_name', "(PrefixFilter ('row2') AND (QualifierFilter (&gt;=, 'binary:xyz'))) AND (TimestampsFilter ( 123, 456))");
   $to_print = $client-&gt;scannerGetList($result,1);
   while ($to_print) {
      print_r($to_print);
      $to_print = $client-&gt;scannerGetList($result,1);
    }
   $client-&gt;scannerClose($result);
?&gt;
        </pre></div><div class="section" title="10.3.1.8.&nbsp;Example Filter Strings"><div class="titlepage"><div><div><h4 class="title"><a name="example-filter-strings"></a>10.3.1.8.&nbsp;Example Filter Strings</h4></div></div></div><p>
          </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p><code class="code">&#8220;PrefixFilter (&#8216;Row&#8217;) AND PageFilter (1) AND FirstKeyOnlyFilter ()&#8221;</code> will return all key-value pairs that match the following conditions:</p><p>1) The row containing the key-value should have prefix &#8220;Row&#8221; </p><p>2) The key-value must be located in the first row of the table </p><p>3) The key-value pair must be the first key-value in the row </p></li></ul></div><p>
        </p><div class="orderedlist"><p>
            </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p><code class="code">&#8220;(RowFilter (=, &#8216;binary:Row 1&#8217;) AND TimeStampsFilter (74689, 89734)) OR
                    ColumnRangeFilter (&#8216;abc&#8217;, true, &#8216;xyz&#8217;, false))&#8221;</code> will return all key-value pairs that match both the following conditions:</p><p>1) The key-value is in a row having row key &#8220;Row 1&#8221; </p><p>2) The key-value must have a timestamp of either 74689 or 89734.</p><p>Or it must match the following condition:</p><p>1) The key-value pair must be in a column that is lexicographically &gt;= abc and &lt; xyz&nbsp;</p></li></ul></div><p>
          </p><ol class="orderedlist" type="1"></ol></div><p>
          </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p><code class="code">&#8220;SKIP ValueFilter (0)&#8221;</code> will skip the entire row if any of the values in the row is not 0</p></li></ul></div><p>
        </p></div><div class="section" title="10.3.1.9.&nbsp;Individual Filter Syntax"><div class="titlepage"><div><div><h4 class="title"><a name="Individual Filter Syntax"></a>10.3.1.9.&nbsp;Individual Filter Syntax</h4></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p><span class="bold"><strong><span class="underline">KeyOnlyFilter</span></strong></span></p><p><span class="bold"><strong>Description:</strong></span> This filter doesn&#8217;t take any
              arguments. It returns only the key component of each key-value. </p><p><span class="bold"><strong>Syntax:</strong></span> KeyOnlyFilter () </p><p><span class="bold"><strong>Example:</strong></span> "KeyOnlyFilter ()"</p></li><li class="listitem"><p><span class="bold"><strong><span class="underline">FirstKeyOnlyFilter</span></strong></span></p><p><span class="bold"><strong>Description:</strong></span> This filter doesn&#8217;t take any
              arguments. It returns only the first key-value from each row. </p><p><span class="bold"><strong>Syntax:</strong></span> FirstKeyOnlyFilter () </p><p><span class="bold"><strong>Example:</strong></span> "FirstKeyOnlyFilter ()" </p></li><li class="listitem"><p><span class="bold"><strong><span class="underline">PrefixFilter</span></strong></span></p><p><span class="bold"><strong>Description:</strong></span> This filter takes one argument &#8211; a prefix of a
              row key. It returns only those key-values present in a row that starts with the
              specified row prefix</p><p><span class="bold"><strong>Syntax:</strong></span> PrefixFilter (&#8216;&lt;row_prefix&gt;&#8217;) </p><p><span class="bold"><strong>Example:</strong></span> "PrefixFilter (&#8216;Row&#8217;)" </p></li><li class="listitem"><p><span class="bold"><strong><span class="underline">
                  ColumnPrefixFilter</span></strong></span></p><p><span class="bold"><strong>Description:</strong></span> This filter takes one argument
              &#8211; a column prefix. It returns only those key-values present in a column that starts
              with the specified column prefix. The column prefix must be of the form: <code class="code">&#8220;qualifier&#8221; </code></p><p><span class="bold"><strong>Syntax:</strong></span>ColumnPrefixFilter(&#8216;&lt;column_prefix&gt;&#8217;)</p><p><span class="bold"><strong>Example:</strong></span> "ColumnPrefixFilter(&#8216;Col&#8217;)"</p></li><li class="listitem"><p><span class="underline"><span class="bold"><strong>MultipleColumnPrefixFilter</strong></span></span></p><p><span class="bold"><strong>Description:</strong></span> This filter takes a list of
              column prefixes. It returns key-values that are present in a column that starts with
              any of the specified column prefixes. Each of the column prefixes must be of the form: <code class="code">&#8220;qualifier&#8221;</code></p><p><span class="bold"><strong>Syntax:</strong></span>MultipleColumnPrefixFilter(&#8216;&lt;column_prefix&gt;&#8217;, &#8216;&lt;column_prefix&gt;&#8217;, &#8230;, &#8216;&lt;column_prefix&gt;&#8217;)</p><p><span class="bold"><strong>Example:</strong></span> "MultipleColumnPrefixFilter(&#8216;Col1&#8217;, &#8216;Col2&#8217;)" </p></li><li class="listitem"><p><span class="bold"><strong><span class="underline">ColumnCountGetFilter</span></strong></span></p><p><span class="bold"><strong>Description:</strong></span> This filter takes one argument
              &#8211; a limit. It returns the first limit number of columns in the table</p><p><span class="bold"><strong>Syntax:</strong></span> ColumnCountGetFilter (&#8216;&lt;limit&gt;&#8217;)</p><p><span class="bold"><strong>Example:</strong></span> "ColumnCountGetFilter (4)"</p></li><li class="listitem"><p><span class="bold"><strong><span class="underline">PageFilter</span></strong></span></p><p><span class="bold"><strong>Description:</strong></span> This filter takes one argument
              &#8211; a page size. It returns page size number of rows from the table. </p><p><span class="bold"><strong>Syntax:</strong></span> PageFilter (&#8216;&lt;page_size&gt;&#8217;)</p><p><span class="bold"><strong>Example:</strong></span> "PageFilter (2)" </p></li><li class="listitem"><p><span class="bold"><strong><span class="underline">ColumnPaginationFilter</span></strong></span></p><p><span class="bold"><strong>Description:</strong></span> This filter takes two
              arguments &#8211; a limit and offset. It returns limit number of columns after offset number
              of columns. It does this for all the rows</p><p><span class="bold"><strong>Syntax:</strong></span> ColumnPaginationFilter(&#8216;&lt;limit&gt;&#8217;, &#8216;&lt;offest&gt;&#8217;) </p><p><span class="bold"><strong>Example:</strong></span> "ColumnPaginationFilter (3, 5)" </p></li><li class="listitem"><p><span class="bold"><strong><span class="underline">InclusiveStopFilter</span></strong></span></p><p><span class="bold"><strong>Description:</strong></span> This filter takes one argument
              &#8211; a row key on which to stop scanning. It returns all key-values present in rows up to
              and including the specified row</p><p><span class="bold"><strong>Syntax:</strong></span> InclusiveStopFilter(&#8216;&lt;stop_row_key&gt;&#8217;) </p><p><span class="bold"><strong>Example:</strong></span> "InclusiveStopFilter ('Row2')" </p></li><li class="listitem"><p><span class="bold"><strong><span class="underline">TimeStampsFilter</span></strong></span></p><p><span class="bold"><strong>Description:</strong></span> This filter takes a list of
              timestamps. It returns those key-values whose timestamps matches any of the specified
              timestamps</p><p> <span class="bold"><strong>Syntax:</strong></span> TimeStampsFilter (&lt;timestamp&gt;, &lt;timestamp&gt;, ... ,&lt;timestamp&gt;) </p><p> <span class="bold"><strong>Example:</strong></span> "TimeStampsFilter (5985489, 48895495, 58489845945)"</p></li><li class="listitem"><p><span class="bold"><strong><span class="underline">RowFilter</span></strong></span></p><p><span class="bold"><strong>Description:</strong></span> This filter takes a compare
              operator and a comparator. It compares each row key with the comparator using the
              compare operator and if the comparison returns true, it returns all the key-values in
              that row</p><p><span class="bold"><strong>Syntax:</strong></span> RowFilter (&lt;compareOp&gt;, &#8216;&lt;row_comparator&gt;&#8217;) </p><p><span class="bold"><strong>Example: </strong></span>"RowFilter (&lt;=, &#8216;xyz)" </p></li><li class="listitem"><p><span class="bold"><strong><span class="underline">Family Filter</span></strong></span></p><p><span class="bold"><strong>Description:</strong></span> This filter takes a compare
              operator and a comparator. It compares each qualifier name with the comparator using
              the compare operator and if the comparison returns true, it returns all the key-values
              in that column</p><p><span class="bold"><strong>Syntax:</strong></span> QualifierFilter (&lt;compareOp&gt;, &#8216;&lt;qualifier_comparator&gt;&#8217;) </p><p><span class="bold"><strong>Example:</strong></span> "QualifierFilter (=, &#8216;Column1&#8217;)"</p></li><li class="listitem"><p><span class="bold"><strong><span class="underline">QualifierFilter</span></strong></span></p><p><span class="bold"><strong>Description:</strong></span> This filter takes a compare
              operator and a comparator. It compares each qualifier name with the comparator using
              the compare operator and if the comparison returns true, it returns all the key-values
              in that column</p><p><span class="bold"><strong>Syntax:</strong></span> QualifierFilter (&lt;compareOp&gt;,&#8216;&lt;qualifier_comparator&gt;&#8217;) </p><p><span class="bold"><strong>Example:</strong></span> "QualifierFilter (=,&#8216;Column1&#8217;)"</p></li><li class="listitem"><p><span class="bold"><strong><span class="underline">ValueFilter</span></strong></span></p><p><span class="bold"><strong>Description:</strong></span> This filter takes a compare operator and a
              comparator. It compares each value with the comparator using the compare operator and
              if the comparison returns true, it returns that key-value</p><p><span class="bold"><strong>Syntax:</strong></span> ValueFilter (&lt;compareOp&gt;,&#8216;&lt;value_comparator&gt;&#8217;) </p><p><span class="bold"><strong>Example:</strong></span> "ValueFilter (!=, &#8216;Value&#8217;)" </p></li><li class="listitem"><p><span class="bold"><strong><span class="underline">DependentColumnFilter</span></strong></span></p><p><span class="bold"><strong>Description:</strong></span> This filter takes two arguments &#8211; a family
              and a qualifier. It tries to locate this column in each row and returns all key-values
              in that row that have the same timestamp. If the row doesn&#8217;t contain the specified
              column &#8211; none of the key-values in that row will be returned.</p><p>The filter can also take an optional boolean argument &#8211; dropDependentColumn. If set to true, the column we were depending on doesn&#8217;t get returned.</p><p>The filter can also take two more additional optional arguments &#8211; a compare operator and a value comparator, which are further checks in addition to the family and qualifier. If the dependent column is found, its value should also pass the value check and then only is its timestamp taken into consideration</p><p><span class="bold"><strong>Syntax:</strong></span> DependentColumnFilter (&#8216;&lt;family&gt;&#8217;, &#8216;&lt;qualifier&gt;&#8217;, &lt;boolean&gt;, &lt;compare operator&gt;, &#8216;&lt;value comparator&#8217;)</p><p><span class="bold"><strong>Syntax:</strong></span> DependentColumnFilter (&#8216;&lt;family&gt;&#8217;, &#8216;&lt;qualifier&gt;&#8217;, &lt;boolean&gt;) </p><p><span class="bold"><strong>Syntax:</strong></span> DependentColumnFilter (&#8216;&lt;family&gt;&#8217;, &#8216;&lt;qualifier&gt;&#8217;) </p><p><span class="bold"><strong>Example:</strong></span> "DependentColumnFilter (&#8216;conf&#8217;, &#8216;blacklist&#8217;, false, &gt;=, &#8216;zebra&#8217;)" </p><p><span class="bold"><strong>Example:</strong></span> "DependentColumnFilter (&#8216;conf&#8217;, 'blacklist', true)"</p><p><span class="bold"><strong>Example:</strong></span> "DependentColumnFilter (&#8216;conf&#8217;, 'blacklist')"</p></li><li class="listitem"><p><span class="bold"><strong><span class="underline">SingleColumnValueFilter</span></strong></span></p><p><span class="bold"><strong>Description:</strong></span> This filter takes a column family, a
              qualifier, a compare operator and a comparator. If the specified column is not found &#8211;
              all the columns of that row will be emitted. If the column is found and the comparison
              with the comparator returns true, all the columns of the row will be emitted. If the
              condition fails, the row will not be emitted. </p><p>This filter also takes two additional optional boolean arguments &#8211; filterIfColumnMissing and setLatestVersionOnly</p><p>If the filterIfColumnMissing flag is set to true the columns of the row will not be emitted if the specified column to check is not found in the row. The default value is false.</p><p>If the setLatestVersionOnly flag is set to false, it will test previous versions (timestamps) too. The default value is true.</p><p>These flags are optional and if you must set neither or both</p><p><span class="bold"><strong>Syntax:</strong></span> SingleColumnValueFilter(&#8216;&lt;family&gt;&#8217;, &#8216;&lt;qualifier&gt;&#8217;, &lt;compare operator&gt;, &#8216;&lt;comparator&gt;&#8217;, &lt;filterIfColumnMissing_boolean&gt;, &lt;latest_version_boolean&gt;) </p><p><span class="bold"><strong>Syntax:</strong></span> SingleColumnValueFilter(&#8216;&lt;family&gt;&#8217;, &#8216;&lt;qualifier&gt;, &lt;compare operator&gt;, &#8216;&lt;comparator&gt;&#8217;) </p><p><span class="bold"><strong>Example:</strong></span> "SingleColumnValueFilter (&#8216;FamilyA&#8217;, &#8216;Column1&#8217;, &lt;=, &#8216;abc&#8217;, true, false)" </p><p><span class="bold"><strong>Example:</strong></span> "SingleColumnValueFilter (&#8216;FamilyA&#8217;, &#8216;Column1&#8217;, &lt;=, &#8216;abc&#8217;)" </p></li><li class="listitem"><p><span class="bold"><strong><span class="underline">SingleColumnValueExcludeFilter</span></strong></span></p><p><span class="bold"><strong>Description:</strong></span> This filter takes the same arguments and
              behaves same as SingleColumnValueFilter &#8211; however, if the column is found and the
              condition passes, all the columns of the row will be emitted except for the tested
              column value. </p><p><span class="bold"><strong>Syntax:</strong></span> SingleColumnValueExcludeFilter('&lt;family&gt;', '&lt;qualifier&gt;', &lt;compare operator&gt;, '&lt;comparator&gt;', &lt;latest_version_boolean&gt;, &lt;filterIfColumnMissing_boolean&gt;)</p><p><span class="bold"><strong>Syntax:</strong></span> SingleColumnValueExcludeFilter('&lt;family&gt;', '&lt;qualifier&gt;', &lt;compare operator&gt;, '&lt;comparator&gt;') </p><p><span class="bold"><strong>Example:</strong></span> "SingleColumnValueExcludeFilter (&#8216;FamilyA&#8217;, &#8216;Column1&#8217;, &#8216;&lt;=&#8217;, &#8216;abc&#8217;, &#8216;false&#8217;, &#8216;true&#8217;)"</p><p><span class="bold"><strong>Example:</strong></span> "SingleColumnValueExcludeFilter (&#8216;FamilyA&#8217;, &#8216;Column1&#8217;, &#8216;&lt;=&#8217;, &#8216;abc&#8217;)" </p></li><li class="listitem"><p><span class="bold"><strong><span class="underline">ColumnRangeFilter</span></strong></span></p><p><span class="bold"><strong>Description:</strong></span> This filter is used for selecting only those
              keys with columns that are between minColumn and maxColumn. It also takes two boolean
              variables to indicate whether to include the minColumn and maxColumn or not.</p><p>If you don&#8217;t want to set the minColumn or the maxColumn &#8211; you can pass in an empty argument.</p><p><span class="bold"><strong>Syntax:</strong></span> ColumnRangeFilter (&#8216;&lt;minColumn&gt;&#8217;, &lt;minColumnInclusive_bool&gt;, &#8216;&lt;maxColumn&gt;&#8217;, &lt;maxColumnInclusive_bool&gt;)</p><p><span class="bold"><strong>Example:</strong></span> "ColumnRangeFilter (&#8216;abc&#8217;, true, &#8216;xyz&#8217;, false)"</p></li></ol></div></div></div></div><div class="section" title="10.4.&nbsp;C/C++ Apache HBase Client"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="c"></a>10.4.&nbsp;C/C++ Apache HBase Client</h2></div></div></div><p>FB's Chip Turner wrote a pure C/C++ client.  <a class="link" href="https://github.com/facebook/native-cpp-hbase-client" target="_top">Check it out</a>.
    </p></div></div><div class="chapter" title="Chapter&nbsp;11.&nbsp;Apache HBase Coprocessors"><div class="titlepage"><div><div><h2 class="title"><a name="cp"></a>Chapter&nbsp;11.&nbsp;Apache HBase Coprocessors</h2></div></div></div><p>The idea of HBase coprocessors was inspired by Google's BigTable coprocessors. The <a class="link" href="https://blogs.apache.org/hbase/entry/coprocessor_introduction" target="_top">Apache HBase Blog on Coprocessor</a> is a very good documentation on that. It has detailed information about the coprocessor framework, terminology, management, and so on.
  </p></div><div class="chapter" title="Chapter&nbsp;12.&nbsp;Apache HBase Performance Tuning"><div class="titlepage"><div><div><h2 class="title"><a name="performance"></a>Chapter&nbsp;12.&nbsp;Apache HBase Performance Tuning</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#perf.os">12.1. Operating System</a></span></dt><dd><dl><dt><span class="section"><a href="#perf.os.ram">12.1.1. Memory</a></span></dt><dt><span class="section"><a href="#perf.os.64">12.1.2. 64-bit</a></span></dt><dt><span class="section"><a href="#perf.os.swap">12.1.3. Swapping</a></span></dt></dl></dd><dt><span class="section"><a href="#perf.network">12.2. Network</a></span></dt><dd><dl><dt><span class="section"><a href="#perf.network.1switch">12.2.1. Single Switch</a></span></dt><dt><span class="section"><a href="#perf.network.2switch">12.2.2. Multiple Switches</a></span></dt><dt><span class="section"><a href="#perf.network.multirack">12.2.3. Multiple Racks</a></span></dt><dt><span class="section"><a href="#perf.network.ints">12.2.4. Network Interfaces</a></span></dt></dl></dd><dt><span class="section"><a href="#jvm">12.3. Java</a></span></dt><dd><dl><dt><span class="section"><a href="#gc">12.3.1. The Garbage Collector and Apache HBase</a></span></dt></dl></dd><dt><span class="section"><a href="#perf.configurations">12.4. HBase Configurations</a></span></dt><dd><dl><dt><span class="section"><a href="#perf.compactions.and.splits">12.4.1. Managing Compactions</a></span></dt><dt><span class="section"><a href="#perf.handlers">12.4.2. <code class="varname">hbase.regionserver.handler.count</code></a></span></dt><dt><span class="section"><a href="#perf.hfile.block.cache.size">12.4.3. <code class="varname">hfile.block.cache.size</code></a></span></dt><dt><span class="section"><a href="#perf.rs.memstore.size">12.4.4. <code class="varname">hbase.regionserver.global.memstore.size</code></a></span></dt><dt><span class="section"><a href="#perf.rs.memstore.size.lower.limit">12.4.5. <code class="varname">hbase.regionserver.global.memstore.size.lower.limit</code></a></span></dt><dt><span class="section"><a href="#perf.hstore.blockingstorefiles">12.4.6. <code class="varname">hbase.hstore.blockingStoreFiles</code></a></span></dt><dt><span class="section"><a href="#perf.hregion.memstore.block.multiplier">12.4.7. <code class="varname">hbase.hregion.memstore.block.multiplier</code></a></span></dt><dt><span class="section"><a href="#hbase.regionserver.checksum.verify">12.4.8. <code class="varname">hbase.regionserver.checksum.verify</code></a></span></dt></dl></dd><dt><span class="section"><a href="#perf.zookeeper">12.5. ZooKeeper</a></span></dt><dt><span class="section"><a href="#perf.schema">12.6. Schema Design</a></span></dt><dd><dl><dt><span class="section"><a href="#perf.number.of.cfs">12.6.1. Number of Column Families</a></span></dt><dt><span class="section"><a href="#perf.schema.keys">12.6.2. Key and Attribute Lengths</a></span></dt><dt><span class="section"><a href="#schema.regionsize">12.6.3. Table RegionSize</a></span></dt><dt><span class="section"><a href="#schema.bloom">12.6.4. Bloom Filters</a></span></dt><dt><span class="section"><a href="#schema.cf.blocksize">12.6.5. ColumnFamily BlockSize</a></span></dt><dt><span class="section"><a href="#cf.in.memory">12.6.6. In-Memory ColumnFamilies</a></span></dt><dt><span class="section"><a href="#perf.compression">12.6.7. Compression</a></span></dt></dl></dd><dt><span class="section"><a href="#perf.general">12.7. HBase General Patterns</a></span></dt><dd><dl><dt><span class="section"><a href="#perf.general.constants">12.7.1. Constants</a></span></dt></dl></dd><dt><span class="section"><a href="#perf.writing">12.8. Writing to HBase</a></span></dt><dd><dl><dt><span class="section"><a href="#perf.batch.loading">12.8.1. Batch Loading</a></span></dt><dt><span class="section"><a href="#precreate.regions">12.8.2. 
    Table Creation: Pre-Creating Regions
    </a></span></dt><dt><span class="section"><a href="#def.log.flush">12.8.3. 
    Table Creation: Deferred Log Flush
    </a></span></dt><dt><span class="section"><a href="#perf.hbase.client.autoflush">12.8.4. HBase Client:  AutoFlush</a></span></dt><dt><span class="section"><a href="#perf.hbase.client.putwal">12.8.5. HBase Client:  Turn off WAL on Puts</a></span></dt><dt><span class="section"><a href="#perf.hbase.client.regiongroup">12.8.6. HBase Client: Group Puts by RegionServer</a></span></dt><dt><span class="section"><a href="#perf.hbase.write.mr.reducer">12.8.7. MapReduce:  Skip The Reducer</a></span></dt><dt><span class="section"><a href="#perf.one.region">12.8.8. Anti-Pattern:  One Hot Region</a></span></dt></dl></dd><dt><span class="section"><a href="#perf.reading">12.9. Reading from HBase</a></span></dt><dd><dl><dt><span class="section"><a href="#perf.hbase.client.caching">12.9.1. Scan Caching</a></span></dt><dt><span class="section"><a href="#perf.hbase.client.selection">12.9.2. Scan Attribute Selection</a></span></dt><dt><span class="section"><a href="#perf.hbase.client.seek">12.9.3. Avoid scan seeks</a></span></dt><dt><span class="section"><a href="#perf.hbase.mr.input">12.9.4. MapReduce - Input Splits</a></span></dt><dt><span class="section"><a href="#perf.hbase.client.scannerclose">12.9.5. Close ResultScanners</a></span></dt><dt><span class="section"><a href="#perf.hbase.client.blockcache">12.9.6. Block Cache</a></span></dt><dt><span class="section"><a href="#perf.hbase.client.rowkeyonly">12.9.7. Optimal Loading of Row Keys</a></span></dt><dt><span class="section"><a href="#perf.hbase.read.dist">12.9.8. Concurrency:  Monitor Data Spread</a></span></dt><dt><span class="section"><a href="#blooms">12.9.9. Bloom Filters</a></span></dt></dl></dd><dt><span class="section"><a href="#perf.deleting">12.10. Deleting from HBase</a></span></dt><dd><dl><dt><span class="section"><a href="#perf.deleting.queue">12.10.1. Using HBase Tables as Queues</a></span></dt><dt><span class="section"><a href="#perf.deleting.rpc">12.10.2. Delete RPC Behavior</a></span></dt></dl></dd><dt><span class="section"><a href="#perf.hdfs">12.11. HDFS</a></span></dt><dd><dl><dt><span class="section"><a href="#perf.hdfs.curr">12.11.1. Current Issues With Low-Latency Reads</a></span></dt><dt><span class="section"><a href="#perf.hdfs.configs.localread">12.11.2. Leveraging local data</a></span></dt><dt><span class="section"><a href="#perf.hdfs.comp">12.11.3. Performance Comparisons of HBase vs. HDFS</a></span></dt></dl></dd><dt><span class="section"><a href="#perf.ec2">12.12. Amazon EC2</a></span></dt><dt><span class="section"><a href="#perf.hbase.mr.cluster">12.13. Collocating HBase and MapReduce</a></span></dt><dt><span class="section"><a href="#perf.casestudy">12.14. Case Studies</a></span></dt></dl></div><div class="section" title="12.1.&nbsp;Operating System"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="perf.os"></a>12.1.&nbsp;Operating System</h2></div></div></div><div class="section" title="12.1.1.&nbsp;Memory"><div class="titlepage"><div><div><h3 class="title"><a name="perf.os.ram"></a>12.1.1.&nbsp;Memory</h3></div></div></div><p>RAM, RAM, RAM.  Don't starve HBase.</p></div><div class="section" title="12.1.2.&nbsp;64-bit"><div class="titlepage"><div><div><h3 class="title"><a name="perf.os.64"></a>12.1.2.&nbsp;64-bit</h3></div></div></div><p>Use a 64-bit platform (and 64-bit JVM).</p></div><div class="section" title="12.1.3.&nbsp;Swapping"><div class="titlepage"><div><div><h3 class="title"><a name="perf.os.swap"></a>12.1.3.&nbsp;Swapping</h3></div></div></div><p>Watch out for swapping.  Set swappiness to 0.</p></div></div><div class="section" title="12.2.&nbsp;Network"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="perf.network"></a>12.2.&nbsp;Network</h2></div></div></div><p>
    Perhaps the most important factor in avoiding network issues degrading Hadoop and HBbase performance is the switching hardware
    that is used, decisions made early in the scope of the project can cause major problems when you double or triple the size of your cluster (or more).
    </p><p>
    Important items to consider:
        </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Switching capacity of the device</li><li class="listitem">Number of systems connected</li><li class="listitem">Uplink capacity</li></ul></div><p>
    </p><div class="section" title="12.2.1.&nbsp;Single Switch"><div class="titlepage"><div><div><h3 class="title"><a name="perf.network.1switch"></a>12.2.1.&nbsp;Single Switch</h3></div></div></div><p>The single most important factor in this configuration is that the switching capacity of the hardware is capable of
      handling the traffic which can be generated by all systems connected to the switch. Some lower priced commodity hardware
      can have a slower switching capacity than could be utilized by a full switch.
      </p></div><div class="section" title="12.2.2.&nbsp;Multiple Switches"><div class="titlepage"><div><div><h3 class="title"><a name="perf.network.2switch"></a>12.2.2.&nbsp;Multiple Switches</h3></div></div></div><p>Multiple switches are a potential pitfall in the architecture.   The most common configuration of lower priced hardware is a
      simple 1Gbps uplink from one switch to another. This often overlooked pinch point can easily become a bottleneck for cluster communication.
      Especially with MapReduce jobs that are both reading and writing a lot of data the communication across this uplink could be saturated.
      </p><p>Mitigation of this issue is fairly simple and can be accomplished in multiple ways:
      </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Use appropriate hardware for the scale of the cluster which you're attempting to build.</li><li class="listitem">Use larger single switch configurations i.e. single 48 port as opposed to 2x 24 port</li><li class="listitem">Configure port trunking for uplinks to utilize multiple interfaces to increase cross switch bandwidth.</li></ul></div><p>
      </p></div><div class="section" title="12.2.3.&nbsp;Multiple Racks"><div class="titlepage"><div><div><h3 class="title"><a name="perf.network.multirack"></a>12.2.3.&nbsp;Multiple Racks</h3></div></div></div><p>Multiple rack configurations carry the same potential issues as multiple switches, and can suffer performance degradation from two main areas:
         </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Poor switch capacity performance</li><li class="listitem">Insufficient uplink to another rack</li></ul></div><p>
      If the the switches in your rack have appropriate switching capacity to handle all the hosts at full speed, the next most likely issue will be caused by homing
      more of your cluster across racks.  The easiest way to avoid issues when spanning multiple racks is to use port trunking to create a bonded uplink to other racks.
      The downside of this method however, is in the overhead of ports that could potentially be used. An example of this is, creating an 8Gbps port channel from rack
      A to rack B, using 8 of your 24 ports to communicate between racks gives you a poor ROI, using too few however can mean you're not getting the most out of your cluster.
      </p><p>Using 10Gbe links between racks will greatly increase performance, and assuming your switches support a 10Gbe uplink or allow for an expansion card will allow you to
      save your ports for machines as opposed to uplinks.
      </p></div><div class="section" title="12.2.4.&nbsp;Network Interfaces"><div class="titlepage"><div><div><h3 class="title"><a name="perf.network.ints"></a>12.2.4.&nbsp;Network Interfaces</h3></div></div></div><p>Are all the network interfaces functioning correctly?  Are you sure?  See the Troubleshooting Case Study in <a class="xref" href="#casestudies.slownode" title="14.3.1.&nbsp;Case Study #1 (Performance Issue On A Single Node)">Section&nbsp;14.3.1, &#8220;Case Study #1 (Performance Issue On A Single Node)&#8221;</a>.
      </p></div></div><div class="section" title="12.3.&nbsp;Java"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="jvm"></a>12.3.&nbsp;Java</h2></div></div></div><div class="section" title="12.3.1.&nbsp;The Garbage Collector and Apache HBase"><div class="titlepage"><div><div><h3 class="title"><a name="gc"></a>12.3.1.&nbsp;The Garbage Collector and Apache HBase</h3></div></div></div><div class="section" title="12.3.1.1.&nbsp;Long GC pauses"><div class="titlepage"><div><div><h4 class="title"><a name="gcpause"></a>12.3.1.1.&nbsp;Long GC pauses</h4></div></div></div><p><a name="mslab"></a>In his presentation, <a class="link" href="http://www.slideshare.net/cloudera/hbase-hug-presentation" target="_top">Avoiding
        Full GCs with MemStore-Local Allocation Buffers</a>, Todd Lipcon
        describes two cases of stop-the-world garbage collections common in
        HBase, especially during loading; CMS failure modes and old generation
        heap fragmentation brought. To address the first, start the CMS
        earlier than default by adding
        <code class="code">-XX:CMSInitiatingOccupancyFraction</code> and setting it down
        from defaults. Start at 60 or 70 percent (The lower you bring down the
        threshold, the more GCing is done, the more CPU used). To address the
        second fragmentation issue, Todd added an experimental facility,
        <a class="indexterm" name="d366e8709"></a>, that
        must be explicitly enabled in Apache HBase 0.90.x (Its defaulted to be on in
        Apache 0.92.x HBase). See <code class="code">hbase.hregion.memstore.mslab.enabled</code>
        to true in your <code class="classname">Configuration</code>. See the cited
        slides for background and detail<sup>[<a name="d366e8719" href="#ftn.d366e8719" class="footnote">28</a>]</sup>.
        Be aware that when enabled, each MemStore instance will occupy at least
        an MSLAB instance of memory.  If you have thousands of regions or lots
        of regions each with many column families, this allocation of MSLAB
        may be responsible for a good portion of your heap allocation and in
        an extreme case cause you to OOME.  Disable MSLAB in this case, or
        lower the amount of memory it uses or float less regions per server.
        </p><p>If you have a write-heavy workload, check out
            <a class="link" href="https://issues.apache.org/jira/browse/HBASE-8163" target="_top">HBASE-8163 MemStoreChunkPool: An improvement for JAVA GC when using MSLAB</a>.
            It describes configurations to lower the amount of young GC during write-heavy loadings.  If you do not have HBASE-8163 installed, and you are
            trying to improve your young GC times, one trick to consider -- courtesy of our Liang Xie -- is to set the GC config <code class="varname">-XX:PretenureSizeThreshold</code> in <code class="filename">hbase-env.sh</code>
            to be just smaller than the size of <code class="varname">hbase.hregion.memstore.mslab.chunksize</code> so MSLAB allocations happen in the
            tenured space directly rather than first in the young gen.  You'd do this because these MSLAB allocations are going to likely make it
            to the old gen anyways and rather than pay the price of a copies between s0 and s1 in eden space followed by the copy up from
            young to old gen after the MSLABs have achieved sufficient tenure, save a bit of YGC churn and allocate in the old gen directly.
            </p><p>For more information about GC logs, see <a class="xref" href="#trouble.log.gc" title="13.2.3.&nbsp;JVM Garbage Collection Logs">Section&nbsp;13.2.3, &#8220;JVM Garbage Collection Logs&#8221;</a>.
        </p></div></div></div><div class="section" title="12.4.&nbsp;HBase Configurations"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="perf.configurations"></a>12.4.&nbsp;HBase Configurations</h2></div></div></div><p>See <a class="xref" href="#recommended_configurations" title="2.5.2.&nbsp;Recommended Configurations">Section&nbsp;2.5.2, &#8220;Recommended Configurations&#8221;</a>.</p><div class="section" title="12.4.1.&nbsp;Managing Compactions"><div class="titlepage"><div><div><h3 class="title"><a name="perf.compactions.and.splits"></a>12.4.1.&nbsp;Managing Compactions</h3></div></div></div><p>For larger systems, managing <a class="link" href="#disable.splitting" title="2.5.2.7.&nbsp;Managed Splitting">compactions and splits</a> may be
      something you want to consider.</p></div><div class="section" title="12.4.2.&nbsp;hbase.regionserver.handler.count"><div class="titlepage"><div><div><h3 class="title"><a name="perf.handlers"></a>12.4.2.&nbsp;<code class="varname">hbase.regionserver.handler.count</code></h3></div></div></div><p>See <a class="xref" href="#hbase.regionserver.handler.count" title="hbase.regionserver.handler.count"><code class="varname">hbase.regionserver.handler.count</code></a>.
	    </p></div><div class="section" title="12.4.3.&nbsp;hfile.block.cache.size"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hfile.block.cache.size"></a>12.4.3.&nbsp;<code class="varname">hfile.block.cache.size</code></h3></div></div></div><p>See <a class="xref" href="#hfile.block.cache.size" title="hfile.block.cache.size"><code class="varname">hfile.block.cache.size</code></a>.
        A memory setting for the RegionServer process.
        </p></div><div class="section" title="12.4.4.&nbsp;hbase.regionserver.global.memstore.size"><div class="titlepage"><div><div><h3 class="title"><a name="perf.rs.memstore.size"></a>12.4.4.&nbsp;<code class="varname">hbase.regionserver.global.memstore.size</code></h3></div></div></div><p>See <a class="xref" href="#">???</a>.
        This memory setting is often adjusted for the RegionServer process depending on needs.
        </p></div><div class="section" title="12.4.5.&nbsp;hbase.regionserver.global.memstore.size.lower.limit"><div class="titlepage"><div><div><h3 class="title"><a name="perf.rs.memstore.size.lower.limit"></a>12.4.5.&nbsp;<code class="varname">hbase.regionserver.global.memstore.size.lower.limit</code></h3></div></div></div><p>See <a class="xref" href="#">???</a>.
        This memory setting is often adjusted for the RegionServer process depending on needs.
        </p></div><div class="section" title="12.4.6.&nbsp;hbase.hstore.blockingStoreFiles"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hstore.blockingstorefiles"></a>12.4.6.&nbsp;<code class="varname">hbase.hstore.blockingStoreFiles</code></h3></div></div></div><p>See <a class="xref" href="#hbase.hstore.blockingStoreFiles" title="hbase.hstore.blockingStoreFiles"><code class="varname">hbase.hstore.blockingStoreFiles</code></a>.
        If there is blocking in the RegionServer logs, increasing this can help.
        </p></div><div class="section" title="12.4.7.&nbsp;hbase.hregion.memstore.block.multiplier"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hregion.memstore.block.multiplier"></a>12.4.7.&nbsp;<code class="varname">hbase.hregion.memstore.block.multiplier</code></h3></div></div></div><p>See <a class="xref" href="#hbase.hregion.memstore.block.multiplier" title="hbase.hregion.memstore.block.multiplier"><code class="varname">hbase.hregion.memstore.block.multiplier</code></a>.
        If there is enough RAM, increasing this can help.
        </p></div><div class="section" title="12.4.8.&nbsp;hbase.regionserver.checksum.verify"><div class="titlepage"><div><div><h3 class="title"><a name="hbase.regionserver.checksum.verify"></a>12.4.8.&nbsp;<code class="varname">hbase.regionserver.checksum.verify</code></h3></div></div></div><p>Have HBase write the checksum into the datablock and save
        having to do the checksum seek whenever you read.</p><p>See <a class="xref" href="#hbase.regionserver.checksum.verify" title="hbase.regionserver.checksum.verify"><code class="varname">hbase.regionserver.checksum.verify</code></a>,
        <a class="xref" href="#hbase.hstore.bytes.per.checksum" title="hbase.hstore.bytes.per.checksum"><code class="varname">hbase.hstore.bytes.per.checksum</code></a> and <a class="xref" href="#hbase.hstore.checksum.algorithm" title="hbase.hstore.checksum.algorithm"><code class="varname">hbase.hstore.checksum.algorithm</code></a>
        For more information see the
        release note on <a class="link" href="https://issues.apache.org/jira/browse/HBASE-5074" target="_top">HBASE-5074 support checksums in HBase block cache</a>.
        </p></div></div><div class="section" title="12.5.&nbsp;ZooKeeper"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="perf.zookeeper"></a>12.5.&nbsp;ZooKeeper</h2></div></div></div><p>See <a class="xref" href="#zookeeper" title="Chapter&nbsp;17.&nbsp;ZooKeeper">Chapter&nbsp;17, <i>ZooKeeper</i></a> for information on configuring ZooKeeper, and see the part
    about having a dedicated disk.
    </p></div><div class="section" title="12.6.&nbsp;Schema Design"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="perf.schema"></a>12.6.&nbsp;Schema Design</h2></div></div></div><div class="section" title="12.6.1.&nbsp;Number of Column Families"><div class="titlepage"><div><div><h3 class="title"><a name="perf.number.of.cfs"></a>12.6.1.&nbsp;Number of Column Families</h3></div></div></div><p>See <a class="xref" href="#number.of.cfs" title="6.2.&nbsp; On the number of column families">Section&nbsp;6.2, &#8220;
      On the number of column families
  &#8221;</a>.</p></div><div class="section" title="12.6.2.&nbsp;Key and Attribute Lengths"><div class="titlepage"><div><div><h3 class="title"><a name="perf.schema.keys"></a>12.6.2.&nbsp;Key and Attribute Lengths</h3></div></div></div><p>See <a class="xref" href="#keysize" title="6.3.2.&nbsp;Try to minimize row and column sizes">Section&nbsp;6.3.2, &#8220;Try to minimize row and column sizes&#8221;</a>.  See also <a class="xref" href="#perf.compression.however" title="12.6.7.1.&nbsp;However...">Section&nbsp;12.6.7.1, &#8220;However...&#8221;</a> for
      compression caveats.</p></div><div class="section" title="12.6.3.&nbsp;Table RegionSize"><div class="titlepage"><div><div><h3 class="title"><a name="schema.regionsize"></a>12.6.3.&nbsp;Table RegionSize</h3></div></div></div><p>The regionsize can be set on a per-table basis via <code class="code">setFileSize</code> on
    <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HTableDescriptor.html" target="_top">HTableDescriptor</a> in the
    event where certain tables require different regionsizes than the configured default regionsize.
    </p><p>See <a class="xref" href="#ops.capacity.regions" title="15.9.2.&nbsp;Determining region count and size">Section&nbsp;15.9.2, &#8220;Determining region count and size&#8221;</a> for more information.
    </p></div><div class="section" title="12.6.4.&nbsp;Bloom Filters"><div class="titlepage"><div><div><h3 class="title"><a name="schema.bloom"></a>12.6.4.&nbsp;Bloom Filters</h3></div></div></div><p>Bloom Filters can be enabled per-ColumnFamily.
        Use <code class="code">HColumnDescriptor.setBloomFilterType(NONE | ROW |
        ROWCOL)</code> to enable blooms per Column Family. Default =
        <code class="varname">NONE</code> for no bloom filters. If
        <code class="varname">ROW</code>, the hash of the row will be added to the bloom
        on each insert. If <code class="varname">ROWCOL</code>, the hash of the row +
        column family name + column family qualifier will be added to the bloom on
        each key insert.</p><p>See <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HColumnDescriptor.html" target="_top">HColumnDescriptor</a> and
    <a class="xref" href="#blooms" title="12.9.9.&nbsp;Bloom Filters">Section&nbsp;12.9.9, &#8220;Bloom Filters&#8221;</a> for more information or this answer up in quora,
<a class="link" href="http://www.quora.com/How-are-bloom-filters-used-in-HBase" target="_top">How are bloom filters used in HBase?</a>.
    </p></div><div class="section" title="12.6.5.&nbsp;ColumnFamily BlockSize"><div class="titlepage"><div><div><h3 class="title"><a name="schema.cf.blocksize"></a>12.6.5.&nbsp;ColumnFamily BlockSize</h3></div></div></div><p>The blocksize can be configured for each ColumnFamily in a table, and this defaults to 64k.  Larger cell values require larger blocksizes.
    There is an inverse relationship between blocksize and the resulting StoreFile indexes (i.e., if the blocksize is doubled then the resulting
    indexes should be roughly halved).
    </p><p>See <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HColumnDescriptor.html" target="_top">HColumnDescriptor</a>
    and <a class="xref" href="#store" title="9.7.6.&nbsp;Store">Section&nbsp;9.7.6, &#8220;Store&#8221;</a>for more information.
    </p></div><div class="section" title="12.6.6.&nbsp;In-Memory ColumnFamilies"><div class="titlepage"><div><div><h3 class="title"><a name="cf.in.memory"></a>12.6.6.&nbsp;In-Memory ColumnFamilies</h3></div></div></div><p>ColumnFamilies can optionally be defined as in-memory.  Data is still persisted to disk, just like any other ColumnFamily.
    In-memory blocks have the highest priority in the <a class="xref" href="#block.cache" title="9.6.4.&nbsp;Block Cache">Section&nbsp;9.6.4, &#8220;Block Cache&#8221;</a>, but it is not a guarantee that the entire table
    will be in memory.
    </p><p>See <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HColumnDescriptor.html" target="_top">HColumnDescriptor</a> for more information.
    </p></div><div class="section" title="12.6.7.&nbsp;Compression"><div class="titlepage"><div><div><h3 class="title"><a name="perf.compression"></a>12.6.7.&nbsp;Compression</h3></div></div></div><p>Production systems should use compression with their ColumnFamily definitions.  See <a class="xref" href="#compression" title="Appendix&nbsp;C.&nbsp;Compression In HBase">Appendix&nbsp;C, <i>Compression In HBase</i></a> for more information.
      </p><div class="section" title="12.6.7.1.&nbsp;However..."><div class="titlepage"><div><div><h4 class="title"><a name="perf.compression.however"></a>12.6.7.1.&nbsp;However...</h4></div></div></div><p>Compression deflates data <span class="emphasis"><em>on disk</em></span>.  When it's in-memory (e.g., in the
         MemStore) or on the wire (e.g., transferring between RegionServer and Client) it's inflated.
         So while using ColumnFamily compression is a best practice, but it's not going to completely eliminate
         the impact of over-sized Keys, over-sized ColumnFamily names, or over-sized Column names.
         </p><p>See <a class="xref" href="#keysize" title="6.3.2.&nbsp;Try to minimize row and column sizes">Section&nbsp;6.3.2, &#8220;Try to minimize row and column sizes&#8221;</a> on for schema design tips, and <a class="xref" href="#keyvalue" title="9.7.6.4.&nbsp;KeyValue">Section&nbsp;9.7.6.4, &#8220;KeyValue&#8221;</a> for more information on HBase stores data internally.
         </p></div></div></div><div class="section" title="12.7.&nbsp;HBase General Patterns"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="perf.general"></a>12.7.&nbsp;HBase General Patterns</h2></div></div></div><div class="section" title="12.7.1.&nbsp;Constants"><div class="titlepage"><div><div><h3 class="title"><a name="perf.general.constants"></a>12.7.1.&nbsp;Constants</h3></div></div></div><p>When people get started with HBase they have a tendency to write code that looks like this:
</p><pre class="programlisting">
Get get = new Get(rowkey);
Result r = htable.get(get);
byte[] b = r.getValue(Bytes.toBytes("cf"), Bytes.toBytes("attr"));  // returns current version of value
</pre><p>
		But especially when inside loops (and MapReduce jobs), converting the columnFamily and column-names
		to byte-arrays repeatedly is surprisingly expensive.
		It's better to use constants for the byte-arrays, like this:
</p><pre class="programlisting">
public static final byte[] CF = "cf".getBytes();
public static final byte[] ATTR = "attr".getBytes();
...
Get get = new Get(rowkey);
Result r = htable.get(get);
byte[] b = r.getValue(CF, ATTR);  // returns current version of value
</pre><p>
      </p></div></div><div class="section" title="12.8.&nbsp;Writing to HBase"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="perf.writing"></a>12.8.&nbsp;Writing to HBase</h2></div></div></div><div class="section" title="12.8.1.&nbsp;Batch Loading"><div class="titlepage"><div><div><h3 class="title"><a name="perf.batch.loading"></a>12.8.1.&nbsp;Batch Loading</h3></div></div></div><p>Use the bulk load tool if you can.  See
        <a class="xref" href="#arch.bulk.load" title="9.8.&nbsp;Bulk Loading">Section&nbsp;9.8, &#8220;Bulk Loading&#8221;</a>.
        Otherwise, pay attention to the below.
      </p></div><div class="section" title="12.8.2.&nbsp; Table Creation: Pre-Creating Regions"><div class="titlepage"><div><div><h3 class="title"><a name="precreate.regions"></a>12.8.2.&nbsp;
    Table Creation: Pre-Creating Regions
    </h3></div></div></div><p>
Tables in HBase are initially created with one region by default.  For bulk imports, this means that all clients will write to the same region
until it is large enough to split and become distributed across the cluster.  A useful pattern to speed up the bulk import process is to pre-create empty regions.
 Be somewhat conservative in this, because too-many regions can actually degrade performance.
</p><p>There are two different approaches to pre-creating splits.  The first approach is to rely on the default <code class="code">HBaseAdmin</code> strategy
	(which is implemented in <code class="code">Bytes.split</code>)...
	</p><pre class="programlisting">
byte[] startKey = ...;   	// your lowest keuy
byte[] endKey = ...;   		// your highest key
int numberOfRegions = ...;	// # of regions to create
admin.createTable(table, startKey, endKey, numberOfRegions);
</pre><p>And the other approach is to define the splits yourself...
	</p><pre class="programlisting">
byte[][] splits = ...;   // create your own splits
admin.createTable(table, splits);
</pre><p>
   See <a class="xref" href="#rowkey.regionsplits" title="6.3.6.&nbsp;Relationship Between RowKeys and Region Splits">Section&nbsp;6.3.6, &#8220;Relationship Between RowKeys and Region Splits&#8221;</a> for issues related to understanding your keyspace and pre-creating regions.
  </p></div><div class="section" title="12.8.3.&nbsp; Table Creation: Deferred Log Flush"><div class="titlepage"><div><div><h3 class="title"><a name="def.log.flush"></a>12.8.3.&nbsp;
    Table Creation: Deferred Log Flush
    </h3></div></div></div><p>
The default behavior for Puts using the Write Ahead Log (WAL) is that <code class="classname">HLog</code> edits will be written immediately.  If deferred log flush is used,
WAL edits are kept in memory until the flush period.  The benefit is aggregated and asynchronous <code class="classname">HLog</code>- writes, but the potential downside is that if
 the RegionServer goes down the yet-to-be-flushed edits are lost.  This is safer, however, than not using WAL at all with Puts.
</p><p>
Deferred log flush can be configured on tables via <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HTableDescriptor.html" target="_top">HTableDescriptor</a>.  The default value of <code class="varname">hbase.regionserver.optionallogflushinterval</code> is 1000ms.
</p></div><div class="section" title="12.8.4.&nbsp;HBase Client: AutoFlush"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.client.autoflush"></a>12.8.4.&nbsp;HBase Client:  AutoFlush</h3></div></div></div><p>When performing a lot of Puts, make sure that setAutoFlush is set
      to false on your <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html" target="_top">HTable</a>
      instance. Otherwise, the Puts will be sent one at a time to the
      RegionServer. Puts added via <code class="code"> htable.add(Put)</code> and <code class="code"> htable.add( &lt;List&gt; Put)</code>
      wind up in the same write buffer. If <code class="code">autoFlush = false</code>,
      these messages are not sent until the write-buffer is filled. To
      explicitly flush the messages, call <code class="methodname">flushCommits</code>.
      Calling <code class="methodname">close</code> on the <code class="classname">HTable</code>
      instance will invoke <code class="methodname">flushCommits</code>.</p></div><div class="section" title="12.8.5.&nbsp;HBase Client: Turn off WAL on Puts"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.client.putwal"></a>12.8.5.&nbsp;HBase Client:  Turn off WAL on Puts</h3></div></div></div><p>A frequently discussed option for increasing throughput on <code class="classname">Put</code>s is to call <code class="code">writeToWAL(false)</code>.  Turning this off means
          that the RegionServer will <span class="emphasis"><em>not</em></span> write the <code class="classname">Put</code> to the Write Ahead Log,
          only into the memstore, HOWEVER the consequence is that if there
          is a RegionServer failure <span class="emphasis"><em>there will be data loss</em></span>.
          If <code class="code">writeToWAL(false)</code> is used, do so with extreme caution.  You may find in actuality that
          it makes little difference if your load is well distributed across the cluster.
      </p><p>In general, it is best to use WAL for Puts, and where loading throughput
          is a concern to use <a class="link" href="#perf.batch.loading" title="12.8.1.&nbsp;Batch Loading">bulk loading</a> techniques instead.
      </p></div><div class="section" title="12.8.6.&nbsp;HBase Client: Group Puts by RegionServer"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.client.regiongroup"></a>12.8.6.&nbsp;HBase Client: Group Puts by RegionServer</h3></div></div></div><p>In addition to using the writeBuffer, grouping <code class="classname">Put</code>s by RegionServer can reduce the number of client RPC calls per writeBuffer flush.
      There is a utility <code class="classname">HTableUtil</code> currently on TRUNK that does this, but you can either copy that or implement your own verison for
      those still on 0.90.x or earlier.
      </p></div><div class="section" title="12.8.7.&nbsp;MapReduce: Skip The Reducer"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.write.mr.reducer"></a>12.8.7.&nbsp;MapReduce:  Skip The Reducer</h3></div></div></div><p>When writing a lot of data to an HBase table from a MR job (e.g., with <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/TableOutputFormat.html" target="_top">TableOutputFormat</a>), and specifically where Puts are being emitted
      from the Mapper, skip the Reducer step.  When a Reducer step is used, all of the output (Puts) from the Mapper will get spooled to disk, then sorted/shuffled to other
      Reducers that will most likely be off-node.  It's far more efficient to just write directly to HBase.
      </p><p>For summary jobs where HBase is used as a source and a sink, then writes will be coming from the Reducer step (e.g., summarize values then write out result).
      This is a different processing problem than from the the above case.
      </p></div><div class="section" title="12.8.8.&nbsp;Anti-Pattern: One Hot Region"><div class="titlepage"><div><div><h3 class="title"><a name="perf.one.region"></a>12.8.8.&nbsp;Anti-Pattern:  One Hot Region</h3></div></div></div><p>If all your data is being written to one region at a time, then re-read the
    section on processing <a class="link" href="#timeseries" title="6.3.1.&nbsp; Monotonically Increasing Row Keys/Timeseries Data">timeseries</a> data.</p><p>Also, if you are pre-splitting regions and all your data is <span class="emphasis"><em>still</em></span> winding up in a single region even though
    your keys aren't monotonically increasing, confirm that your keyspace actually works with the split strategy.  There are a
    variety of reasons that regions may appear "well split" but won't work with your data.   As
    the HBase client communicates directly with the RegionServers, this can be obtained via
    <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html#getRegionLocation%28byte[]%29" target="_top">HTable.getRegionLocation</a>.
    </p><p>See <a class="xref" href="#precreate.regions" title="12.8.2.&nbsp; Table Creation: Pre-Creating Regions">Section&nbsp;12.8.2, &#8220;
    Table Creation: Pre-Creating Regions
    &#8221;</a>, as well as <a class="xref" href="#perf.configurations" title="12.4.&nbsp;HBase Configurations">Section&nbsp;12.4, &#8220;HBase Configurations&#8221;</a> </p></div></div><div class="section" title="12.9.&nbsp;Reading from HBase"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="perf.reading"></a>12.9.&nbsp;Reading from HBase</h2></div></div></div><p>The mailing list can help if you are having performance issues.
    For example, here is a good general thread on what to look at addressing
    read-time issues: <a class="link" href="http://search-hadoop.com/m/qOo2yyHtCC1" target="_top">HBase Random Read latency &gt; 100ms</a></p><div class="section" title="12.9.1.&nbsp;Scan Caching"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.client.caching"></a>12.9.1.&nbsp;Scan Caching</h3></div></div></div><p>If HBase is used as an input source for a MapReduce job, for
      example, make sure that the input <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Scan.html" target="_top">Scan</a>
      instance to the MapReduce job has <code class="methodname">setCaching</code> set to something greater
      than the default (which is 1). Using the default value means that the
      map-task will make call back to the region-server for every record
      processed. Setting this value to 500, for example, will transfer 500
      rows at a time to the client to be processed. There is a cost/benefit to
      have the cache value be large because it costs more in memory for both
      client and RegionServer, so bigger isn't always better.</p><div class="section" title="12.9.1.1.&nbsp;Scan Caching in MapReduce Jobs"><div class="titlepage"><div><div><h4 class="title"><a name="perf.hbase.client.caching.mr"></a>12.9.1.1.&nbsp;Scan Caching in MapReduce Jobs</h4></div></div></div><p>Scan settings in MapReduce jobs deserve special attention.  Timeouts can result (e.g., UnknownScannerException)
        in Map tasks if it takes longer to process a batch of records before the client goes back to the RegionServer for the
        next set of data.  This problem can occur because there is non-trivial processing occuring per row.  If you process
        rows quickly, set caching higher.  If you process rows more slowly (e.g., lots of transformations per row, writes),
        then set caching lower.
        </p><p>Timeouts can also happen in a non-MapReduce use case (i.e., single threaded HBase client doing a Scan), but the
        processing that is often performed in MapReduce jobs tends to exacerbate this issue.
        </p></div></div><div class="section" title="12.9.2.&nbsp;Scan Attribute Selection"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.client.selection"></a>12.9.2.&nbsp;Scan Attribute Selection</h3></div></div></div><p>Whenever a Scan is used to process large numbers of rows (and especially when used
      as a MapReduce source), be aware of which attributes are selected.   If <code class="code">scan.addFamily</code> is called
      then <span class="emphasis"><em>all</em></span> of the attributes in the specified ColumnFamily will be returned to the client.
      If only a small number of the available attributes are to be processed, then only those attributes should be specified
      in the input scan because attribute over-selection is a non-trivial performance penalty over large datasets.
      </p></div><div class="section" title="12.9.3.&nbsp;Avoid scan seeks"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.client.seek"></a>12.9.3.&nbsp;Avoid scan seeks</h3></div></div></div><p>When columns are selected explicitly with <code class="code">scan.addColumn</code>, HBase will schedule seek operations to seek between the
      selected columns. When rows have few columns and each column has only a few versions this can be inefficient. A seek operation is generally
      slower if does not seek at least past 5-10 columns/versions or 512-1024 bytes.</p><p>In order to opportunistically look ahead a few columns/versions to see if the next column/version can be found that
      way before a seek operation is scheduled, a new attribute <code class="code">Scan.HINT_LOOKAHEAD</code> can be set the on Scan object. The following code instructs the
      RegionServer to attempt two iterations of next before a seek is scheduled:</p><pre class="programlisting">
Scan scan = new Scan();
scan.addColumn(...);
scan.setAttribute(Scan.HINT_LOOKAHEAD, Bytes.toBytes(2));
table.getScanner(scan);
</pre></div><div class="section" title="12.9.4.&nbsp;MapReduce - Input Splits"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.mr.input"></a>12.9.4.&nbsp;MapReduce - Input Splits</h3></div></div></div><p>For MapReduce jobs that use HBase tables as a source, if there a pattern where the "slow" map tasks seem to
        have the same Input Split (i.e., the RegionServer serving the data), see the
        Troubleshooting Case Study in <a class="xref" href="#casestudies.slownode" title="14.3.1.&nbsp;Case Study #1 (Performance Issue On A Single Node)">Section&nbsp;14.3.1, &#8220;Case Study #1 (Performance Issue On A Single Node)&#8221;</a>.
        </p></div><div class="section" title="12.9.5.&nbsp;Close ResultScanners"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.client.scannerclose"></a>12.9.5.&nbsp;Close ResultScanners</h3></div></div></div><p>This isn't so much about improving performance but rather
      <span class="emphasis"><em>avoiding</em></span> performance problems. If you forget to
      close <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/ResultScanner.html" target="_top">ResultScanners</a>
      you can cause problems on the RegionServers. Always have ResultScanner
      processing enclosed in try/catch blocks... </p><pre class="programlisting">
Scan scan = new Scan();
// set attrs...
ResultScanner rs = htable.getScanner(scan);
try {
  for (Result r = rs.next(); r != null; r = rs.next()) {
  // process result...
} finally {
  rs.close();  // always close the ResultScanner!
}
htable.close();</pre></div><div class="section" title="12.9.6.&nbsp;Block Cache"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.client.blockcache"></a>12.9.6.&nbsp;Block Cache</h3></div></div></div><p><a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Scan.html" target="_top">Scan</a>
      instances can be set to use the block cache in the RegionServer via the
      <code class="methodname">setCacheBlocks</code> method. For input Scans to MapReduce jobs, this should be
      <code class="varname">false</code>. For frequently accessed rows, it is advisable to use the block
      cache.</p></div><div class="section" title="12.9.7.&nbsp;Optimal Loading of Row Keys"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.client.rowkeyonly"></a>12.9.7.&nbsp;Optimal Loading of Row Keys</h3></div></div></div><p>When performing a table <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Scan.html" target="_top">scan</a>
            where only the row keys are needed (no families, qualifiers, values or timestamps), add a FilterList with a
            <code class="varname">MUST_PASS_ALL</code> operator to the scanner using <code class="methodname">setFilter</code>. The filter list
            should include both a <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/FirstKeyOnlyFilter.html" target="_top">FirstKeyOnlyFilter</a>
            and a <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/KeyOnlyFilter.html" target="_top">KeyOnlyFilter</a>.
            Using this filter combination will result in a worst case scenario of a RegionServer reading a single value from disk
            and minimal network traffic to the client for a single row.
      </p></div><div class="section" title="12.9.8.&nbsp;Concurrency: Monitor Data Spread"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.read.dist"></a>12.9.8.&nbsp;Concurrency:  Monitor Data Spread</h3></div></div></div><p>When performing a high number of concurrent reads, monitor the data spread of the target tables.  If the target table(s) have
      too few regions then the reads could likely be served from too few nodes.  </p><p>See <a class="xref" href="#precreate.regions" title="12.8.2.&nbsp; Table Creation: Pre-Creating Regions">Section&nbsp;12.8.2, &#8220;
    Table Creation: Pre-Creating Regions
    &#8221;</a>, as well as <a class="xref" href="#perf.configurations" title="12.4.&nbsp;HBase Configurations">Section&nbsp;12.4, &#8220;HBase Configurations&#8221;</a> </p></div><div class="section" title="12.9.9.&nbsp;Bloom Filters"><div class="titlepage"><div><div><h3 class="title"><a name="blooms"></a>12.9.9.&nbsp;Bloom Filters</h3></div></div></div><p>Enabling Bloom Filters can save your having to go to disk and
         can help improve read latencies.</p><p><a class="link" href="http://en.wikipedia.org/wiki/Bloom_filter" target="_top">Bloom filters</a> were developed over in <a class="link" href="https://issues.apache.org/jira/browse/HBASE-1200" target="_top">HBase-1200
    Add bloomfilters</a>.<sup>[<a name="d366e9233" href="#ftn.d366e9233" class="footnote">29</a>]</sup><sup>[<a name="d366e9245" href="#ftn.d366e9245" class="footnote">30</a>]</sup></p><p>See also <a class="xref" href="#schema.bloom" title="12.6.4.&nbsp;Bloom Filters">Section&nbsp;12.6.4, &#8220;Bloom Filters&#8221;</a>.
        </p><div class="section" title="12.9.9.1.&nbsp;Bloom StoreFile footprint"><div class="titlepage"><div><div><h4 class="title"><a name="bloom_footprint"></a>12.9.9.1.&nbsp;Bloom StoreFile footprint</h4></div></div></div><p>Bloom filters add an entry to the <code class="classname">StoreFile</code>
      general <code class="classname">FileInfo</code> data structure and then two
      extra entries to the <code class="classname">StoreFile</code> metadata
      section.</p><div class="section" title="12.9.9.1.1.&nbsp;BloomFilter in the StoreFile FileInfo data structure"><div class="titlepage"><div><div><h5 class="title"><a name="d366e9269"></a>12.9.9.1.1.&nbsp;BloomFilter in the <code class="classname">StoreFile</code>
        <code class="classname">FileInfo</code> data structure</h5></div></div></div><p><code class="classname">FileInfo</code> has a
          <code class="varname">BLOOM_FILTER_TYPE</code> entry which is set to
          <code class="varname">NONE</code>, <code class="varname">ROW</code> or
          <code class="varname">ROWCOL.</code></p></div><div class="section" title="12.9.9.1.2.&nbsp;BloomFilter entries in StoreFile metadata"><div class="titlepage"><div><div><h5 class="title"><a name="d366e9293"></a>12.9.9.1.2.&nbsp;BloomFilter entries in <code class="classname">StoreFile</code>
        metadata</h5></div></div></div><p><code class="varname">BLOOM_FILTER_META</code> holds Bloom Size, Hash
          Function used, etc. Its small in size and is cached on
          <code class="classname">StoreFile.Reader</code> load</p><p><code class="varname">BLOOM_FILTER_DATA</code> is the actual bloomfilter
          data. Obtained on-demand. Stored in the LRU cache, if it is enabled
          (Its enabled by default).</p></div></div><div class="section" title="12.9.9.2.&nbsp;Bloom Filter Configuration"><div class="titlepage"><div><div><h4 class="title"><a name="config.bloom"></a>12.9.9.2.&nbsp;Bloom Filter Configuration</h4></div></div></div><div class="section" title="12.9.9.2.1.&nbsp;io.hfile.bloom.enabled global kill switch"><div class="titlepage"><div><div><h5 class="title"><a name="d366e9313"></a>12.9.9.2.1.&nbsp;<code class="varname">io.hfile.bloom.enabled</code> global kill
        switch</h5></div></div></div><p><code class="code">io.hfile.bloom.enabled</code> in
        <code class="classname">Configuration</code> serves as the kill switch in case
        something goes wrong. Default = <code class="varname">true</code>.</p></div><div class="section" title="12.9.9.2.2.&nbsp;io.hfile.bloom.error.rate"><div class="titlepage"><div><div><h5 class="title"><a name="d366e9328"></a>12.9.9.2.2.&nbsp;<code class="varname">io.hfile.bloom.error.rate</code></h5></div></div></div><p><code class="varname">io.hfile.bloom.error.rate</code> = average false
        positive rate. Default = 1%. Decrease rate by &frac12; (e.g. to .5%) == +1
        bit per bloom entry.</p></div><div class="section" title="12.9.9.2.3.&nbsp;io.hfile.bloom.max.fold"><div class="titlepage"><div><div><h5 class="title"><a name="d366e9336"></a>12.9.9.2.3.&nbsp;<code class="varname">io.hfile.bloom.max.fold</code></h5></div></div></div><p><code class="varname">io.hfile.bloom.max.fold</code> = guaranteed minimum
        fold rate. Most people should leave this alone. Default = 7, or can
        collapse to at least 1/128th of original size. See the
        <span class="emphasis"><em>Development Process</em></span> section of the document <a class="link" href="https://issues.apache.org/jira/secure/attachment/12444007/Bloom_Filters_in_HBase.pdf" target="_top">BloomFilters
        in HBase</a> for more on what this option means.</p></div></div></div></div><div class="section" title="12.10.&nbsp;Deleting from HBase"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="perf.deleting"></a>12.10.&nbsp;Deleting from HBase</h2></div></div></div><div class="section" title="12.10.1.&nbsp;Using HBase Tables as Queues"><div class="titlepage"><div><div><h3 class="title"><a name="perf.deleting.queue"></a>12.10.1.&nbsp;Using HBase Tables as Queues</h3></div></div></div><p>HBase tables are sometimes used as queues.  In this case, special care must be taken to regularly perform major compactions on tables used in
       this manner.  As is documented in <a class="xref" href="#datamodel" title="Chapter&nbsp;5.&nbsp;Data Model">Chapter&nbsp;5, <i>Data Model</i></a>, marking rows as deleted creates additional StoreFiles which then need to be processed
       on reads.  Tombstones only get cleaned up with major compactions.
       </p><p>See also <a class="xref" href="#compaction" title="9.7.6.5.&nbsp;Compaction">Section&nbsp;9.7.6.5, &#8220;Compaction&#8221;</a> and <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HBaseAdmin.html#majorCompact%28java.lang.String%29" target="_top">HBaseAdmin.majorCompact</a>.
       </p></div><div class="section" title="12.10.2.&nbsp;Delete RPC Behavior"><div class="titlepage"><div><div><h3 class="title"><a name="perf.deleting.rpc"></a>12.10.2.&nbsp;Delete RPC Behavior</h3></div></div></div><p>Be aware that <code class="code">htable.delete(Delete)</code> doesn't use the writeBuffer.  It will execute an RegionServer RPC with each invocation.
       For a large number of deletes, consider <code class="code">htable.delete(List)</code>.
       </p><p>See <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html#delete%28org.apache.hadoop.hbase.client.Delete%29" target="_top">http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html#delete%28org.apache.hadoop.hbase.client.Delete%29</a>
       </p></div></div><div class="section" title="12.11.&nbsp;HDFS"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="perf.hdfs"></a>12.11.&nbsp;HDFS</h2></div></div></div><p>Because HBase runs on <a class="xref" href="#arch.hdfs" title="9.9.&nbsp;HDFS">Section&nbsp;9.9, &#8220;HDFS&#8221;</a> it is important to understand how it works and how it affects
   HBase.
   </p><div class="section" title="12.11.1.&nbsp;Current Issues With Low-Latency Reads"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hdfs.curr"></a>12.11.1.&nbsp;Current Issues With Low-Latency Reads</h3></div></div></div><p>The original use-case for HDFS was batch processing.  As such, there low-latency reads were historically not a priority.
      With the increased adoption of Apache HBase this is changing, and several improvements are already in development.
      See the
      <a class="link" href="https://issues.apache.org/jira/browse/HDFS-1599" target="_top">Umbrella Jira Ticket for HDFS Improvements for HBase</a>.
      </p></div><div class="section" title="12.11.2.&nbsp;Leveraging local data"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hdfs.configs.localread"></a>12.11.2.&nbsp;Leveraging local data</h3></div></div></div><p>Since Hadoop 1.0.0 (also 0.22.1, 0.23.1, CDH3u3 and HDP 1.0) via
<a class="link" href="https://issues.apache.org/jira/browse/HDFS-2246" target="_top">HDFS-2246</a>,
it is possible for the DFSClient to take a "short circuit" and
read directly from disk instead of going through the DataNode when the
data is local. What this means for HBase is that the RegionServers can
read directly off their machine's disks instead of having to open a
socket to talk to the DataNode, the former being generally much
faster<sup>[<a name="d366e9408" href="#ftn.d366e9408" class="footnote">31</a>]</sup>.
Also see <a class="link" href="http://search-hadoop.com/m/zV6dKrLCVh1" target="_top">HBase, mail # dev - read short circuit</a> thread for
more discussion around short circuit reads.
</p><p>To enable "short circuit" reads, it will depend on your version of Hadoop.
    The original shortcircuit read patch was much improved upon in Hadoop 2 in
    <a class="link" href="https://issues.apache.org/jira/browse/HDFS-347" target="_top">HDFS-347</a>.
    See <a class="link" href="http://blog.cloudera.com/blog/2013/08/how-improved-short-circuit-local-reads-bring-better-performance-and-security-to-hadoop/" target="_top">http://blog.cloudera.com/blog/2013/08/how-improved-short-circuit-local-reads-bring-better-performance-and-security-to-hadoop/</a> for details
    on the difference between the old and new implementations.  See
    <a class="link" href="http://archive.cloudera.com/cdh4/cdh/4/hadoop/hadoop-project-dist/hadoop-hdfs/ShortCircuitLocalReads.html" target="_top">Hadoop shortcircuit reads configuration page</a>
    for how to enable the later version of shortcircuit.
</p><p>If you are running on an old Hadoop, one that is without
    <a class="link" href="https://issues.apache.org/jira/browse/HDFS-347" target="_top">HDFS-347</a> but that
    has
<a class="link" href="https://issues.apache.org/jira/browse/HDFS-2246" target="_top">HDFS-2246</a>,
you must set two configurations.
First, the hdfs-site.xml needs to be amended. Set
the property  <code class="varname">dfs.block.local-path-access.user</code>
to be the <span class="emphasis"><em>only</em></span> user that can use the shortcut.
This has to be the user that started HBase.  Then in hbase-site.xml,
set <code class="varname">dfs.client.read.shortcircuit</code> to be <code class="varname">true</code>
</p><p>
    For optimal performance when short-circuit reads are enabled, it is recommended that HDFS checksums are disabled.
    To maintain data integrity with HDFS checksums disabled, HBase can be configured to write its own checksums into
    its datablocks and verify against these. See <a class="xref" href="#hbase.regionserver.checksum.verify" title="hbase.regionserver.checksum.verify"><code class="varname">hbase.regionserver.checksum.verify</code></a>. When both
    local short-circuit reads and hbase level checksums are enabled, you SHOULD NOT disable configuration parameter
    "dfs.client.read.shortcircuit.skip.checksum", which will cause skipping checksum on non-hfile reads. HBase already
    manages that setting under the covers.
</p><p>
The DataNodes need to be restarted in order to pick up the new
configuration. Be aware that if a process started under another
username than the one configured here also has the shortcircuit
enabled, it will get an Exception regarding an unauthorized access but
the data will still be read.
</p><div class="note" title="dfs.client.read.shortcircuit.buffer.size" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a name="dfs.client.read.shortcircuit.buffer.size"></a>dfs.client.read.shortcircuit.buffer.size</h3><p>The default for this value is too high when running on a highly trafficed HBase.  Set it down from its
        1M default down to 128k or so.  Put this configuration in the HBase configs (its a HDFS client-side configuration).
        The Hadoop DFSClient in HBase will allocate a direct byte buffer of this size for <span class="emphasis"><em>each</em></span>
    block it has open; given HBase keeps its HDFS files open all the time, this can add up quickly.</p></div></div><div class="section" title="12.11.3.&nbsp;Performance Comparisons of HBase vs. HDFS"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hdfs.comp"></a>12.11.3.&nbsp;Performance Comparisons of HBase vs. HDFS</h3></div></div></div><p>A fairly common question on the dist-list is why HBase isn't as performant as HDFS files in a batch context (e.g., as
     a MapReduce source or sink).  The short answer is that HBase is doing a lot more than HDFS (e.g., reading the KeyValues,
     returning the most current row or specified timestamps, etc.), and as such HBase is 4-5 times slower than HDFS in this
     processing context.  There is room for improvement and this gap will, over time, be reduced, but HDFS
      will always be faster in this use-case.
     </p></div></div><div class="section" title="12.12.&nbsp;Amazon EC2"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="perf.ec2"></a>12.12.&nbsp;Amazon EC2</h2></div></div></div><p>Performance questions are common on Amazon EC2 environments because it is a shared environment.  You will
   not see the same throughput as a dedicated server.  In terms of running tests on EC2, run them several times for the same
   reason (i.e., it's a shared environment and you don't know what else is happening on the server).
   </p><p>If you are running on EC2 and post performance questions on the dist-list, please state this fact up-front that
    because EC2 issues are practically a separate class of performance issues.
   </p></div><div class="section" title="12.13.&nbsp;Collocating HBase and MapReduce"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="perf.hbase.mr.cluster"></a>12.13.&nbsp;Collocating HBase and MapReduce</h2></div></div></div><p>It is often recommended to have different clusters for HBase and MapReduce. A better qualification of this is:
          don't collocate a HBase that serves live requests with a heavy MR workload. OLTP and OLAP-optimized systems have
          conflicting requirements and one will lose to the other, usually the former. For example, short latency-sensitive
          disk reads will have to wait in line behind longer reads that are trying to squeeze out as much throughput as
          possible. MR jobs that write to HBase will also generate flushes and compactions, which will in turn invalidate
          blocks in the <a class="xref" href="#block.cache" title="9.6.4.&nbsp;Block Cache">Section&nbsp;9.6.4, &#8220;Block Cache&#8221;</a>.
    </p><p>If you need to process the data from your live HBase cluster in MR, you can ship the deltas with <a class="xref" href="#">???</a>
          or use replication to get the new data in real time on the OLAP cluster. In the worst case, if you really need to
          collocate both, set MR to use less Map and Reduce slots than you'd normally configure, possibly just one.
    </p><p>When HBase is used for OLAP operations, it's preferable to set it up in a hardened way like configuring the ZooKeeper session
          timeout higher and giving more memory to the MemStores (the argument being that the Block Cache won't be used much
          since the workloads are usually long scans).
    </p></div><div class="section" title="12.14.&nbsp;Case Studies"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="perf.casestudy"></a>12.14.&nbsp;Case Studies</h2></div></div></div><p>For Performance and Troubleshooting Case Studies, see <a class="xref" href="#casestudies" title="Chapter&nbsp;14.&nbsp;Apache HBase Case Studies">Chapter&nbsp;14, <i>Apache HBase Case Studies</i></a>.
      </p></div><div class="footnotes"><br><hr width="100" align="left"><div class="footnote"><p><sup>[<a id="ftn.d366e8719" href="#d366e8719" class="para">28</a>] </sup>The latest jvms do better
        regards fragmentation so make sure you are running a recent release.
        Read down in the message,
        <a class="link" href="http://osdir.com/ml/hotspot-gc-use/2011-11/msg00002.html" target="_top">Identifying concurrent mode failures caused by fragmentation</a>.</p></div><div class="footnote"><p><sup>[<a id="ftn.d366e9233" href="#d366e9233" class="para">29</a>] </sup>For description of the development process -- why static blooms
        rather than dynamic -- and for an overview of the unique properties
        that pertain to blooms in HBase, as well as possible future
        directions, see the <span class="emphasis"><em>Development Process</em></span> section
        of the document <a class="link" href="https://issues.apache.org/jira/secure/attachment/12444007/Bloom_Filters_in_HBase.pdf" target="_top">BloomFilters
        in HBase</a> attached to <a class="link" href="https://issues.apache.org/jira/browse/HBASE-1200" target="_top">HBase-1200</a>.</p></div><div class="footnote"><p><sup>[<a id="ftn.d366e9245" href="#d366e9245" class="para">30</a>] </sup>The bloom filters described here are actually version two of
        blooms in HBase. In versions up to 0.19.x, HBase had a dynamic bloom
        option based on work done by the <a class="link" href="http://www.one-lab.org" target="_top">European Commission One-Lab
        Project 034819</a>. The core of the HBase bloom work was later
        pulled up into Hadoop to implement org.apache.hadoop.io.BloomMapFile.
        Version 1 of HBase blooms never worked that well. Version 2 is a
        rewrite from scratch though again it starts with the one-lab
        work.</p></div><div class="footnote"><p><sup>[<a id="ftn.d366e9408" href="#d366e9408" class="para">31</a>] </sup>See JD's <a class="link" href="http://files.meetup.com/1350427/hug_ebay_jdcryans.pdf" target="_top">Performance Talk</a></p></div></div></div><div class="chapter" title="Chapter&nbsp;13.&nbsp;Troubleshooting and Debugging Apache HBase"><div class="titlepage"><div><div><h2 class="title"><a name="trouble"></a>Chapter&nbsp;13.&nbsp;Troubleshooting and Debugging Apache HBase</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#trouble.general">13.1. General Guidelines</a></span></dt><dt><span class="section"><a href="#trouble.log">13.2. Logs</a></span></dt><dd><dl><dt><span class="section"><a href="#trouble.log.locations">13.2.1. Log Locations</a></span></dt><dt><span class="section"><a href="#trouble.log.levels">13.2.2. Log Levels</a></span></dt><dt><span class="section"><a href="#trouble.log.gc">13.2.3. JVM Garbage Collection Logs</a></span></dt></dl></dd><dt><span class="section"><a href="#trouble.resources">13.3. Resources</a></span></dt><dd><dl><dt><span class="section"><a href="#trouble.resources.searchhadoop">13.3.1. search-hadoop.com</a></span></dt><dt><span class="section"><a href="#trouble.resources.lists">13.3.2. Mailing Lists</a></span></dt><dt><span class="section"><a href="#trouble.resources.irc">13.3.3. IRC</a></span></dt><dt><span class="section"><a href="#trouble.resources.jira">13.3.4. JIRA</a></span></dt></dl></dd><dt><span class="section"><a href="#trouble.tools">13.4. Tools</a></span></dt><dd><dl><dt><span class="section"><a href="#trouble.tools.builtin">13.4.1. Builtin Tools</a></span></dt><dt><span class="section"><a href="#trouble.tools.external">13.4.2. External Tools</a></span></dt></dl></dd><dt><span class="section"><a href="#trouble.client">13.5. Client</a></span></dt><dd><dl><dt><span class="section"><a href="#trouble.client.scantimeout">13.5.1. ScannerTimeoutException or UnknownScannerException</a></span></dt><dt><span class="section"><a href="#trouble.client.lease.exception">13.5.2. <code class="classname">LeaseException</code> when calling <code class="classname">Scanner.next</code></a></span></dt><dt><span class="section"><a href="#trouble.client.scarylogs">13.5.3. Shell or client application throws lots of scary exceptions during normal operation</a></span></dt><dt><span class="section"><a href="#trouble.client.longpauseswithcompression">13.5.4. Long Client Pauses With Compression</a></span></dt><dt><span class="section"><a href="#trouble.client.zookeeper">13.5.5. ZooKeeper Client Connection Errors</a></span></dt><dt><span class="section"><a href="#trouble.client.oome.directmemory.leak">13.5.6. Client running out of memory though heap size seems to be stable (but the off-heap/direct heap keeps growing)</a></span></dt><dt><span class="section"><a href="#trouble.client.slowdown.admin">13.5.7. Client Slowdown When Calling Admin Methods (flush, compact, etc.)</a></span></dt><dt><span class="section"><a href="#trouble.client.security.rpc">13.5.8. Secure Client Cannot Connect ([Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)])</a></span></dt></dl></dd><dt><span class="section"><a href="#trouble.mapreduce">13.6. MapReduce</a></span></dt><dd><dl><dt><span class="section"><a href="#trouble.mapreduce.local">13.6.1. You Think You're On The Cluster, But You're Actually Local</a></span></dt></dl></dd><dt><span class="section"><a href="#trouble.namenode">13.7. NameNode</a></span></dt><dd><dl><dt><span class="section"><a href="#trouble.namenode.disk">13.7.1. HDFS Utilization of Tables and Regions</a></span></dt><dt><span class="section"><a href="#trouble.namenode.hbase.objects">13.7.2. Browsing HDFS for HBase Objects</a></span></dt></dl></dd><dt><span class="section"><a href="#trouble.network">13.8. Network</a></span></dt><dd><dl><dt><span class="section"><a href="#trouble.network.spikes">13.8.1. Network Spikes</a></span></dt><dt><span class="section"><a href="#trouble.network.loopback">13.8.2. Loopback IP</a></span></dt><dt><span class="section"><a href="#trouble.network.ints">13.8.3. Network Interfaces</a></span></dt></dl></dd><dt><span class="section"><a href="#trouble.rs">13.9. RegionServer</a></span></dt><dd><dl><dt><span class="section"><a href="#trouble.rs.startup">13.9.1. Startup Errors</a></span></dt><dt><span class="section"><a href="#trouble.rs.runtime">13.9.2. Runtime Errors</a></span></dt><dt><span class="section"><a href="#trouble.rs.shutdown">13.9.3. Shutdown Errors</a></span></dt></dl></dd><dt><span class="section"><a href="#trouble.master">13.10. Master</a></span></dt><dd><dl><dt><span class="section"><a href="#trouble.master.startup">13.10.1. Startup Errors</a></span></dt><dt><span class="section"><a href="#trouble.master.shutdown">13.10.2. Shutdown Errors</a></span></dt></dl></dd><dt><span class="section"><a href="#trouble.zookeeper">13.11. ZooKeeper</a></span></dt><dd><dl><dt><span class="section"><a href="#trouble.zookeeper.startup">13.11.1. Startup Errors</a></span></dt><dt><span class="section"><a href="#trouble.zookeeper.general">13.11.2. ZooKeeper, The Cluster Canary</a></span></dt></dl></dd><dt><span class="section"><a href="#trouble.ec2">13.12. Amazon EC2</a></span></dt><dd><dl><dt><span class="section"><a href="#trouble.ec2.zookeeper">13.12.1. ZooKeeper does not seem to work on Amazon EC2</a></span></dt><dt><span class="section"><a href="#trouble.ec2.instability">13.12.2. Instability on Amazon EC2</a></span></dt><dt><span class="section"><a href="#trouble.ec2.connection">13.12.3. Remote Java Connection into EC2 Cluster Not Working</a></span></dt></dl></dd><dt><span class="section"><a href="#trouble.versions">13.13. HBase and Hadoop version issues</a></span></dt><dd><dl><dt><span class="section"><a href="#trouble.versions.205">13.13.1. <code class="code">NoClassDefFoundError</code> when trying to run 0.90.x on hadoop-0.20.205.x (or hadoop-1.0.x)</a></span></dt><dt><span class="section"><a href="#trouble.wrong.version">13.13.2. ...cannot communicate with client version...</a></span></dt></dl></dd><dt><span class="section"><a href="#trouble.tests">13.14. Running unit or integration tests</a></span></dt><dd><dl><dt><span class="section"><a href="#trouble.HDFS-2556">13.14.1. Runtime exceptions from MiniDFSCluster when running tests</a></span></dt></dl></dd><dt><span class="section"><a href="#trouble.casestudy">13.15. Case Studies</a></span></dt><dt><span class="section"><a href="#trouble.crypto">13.16. Cryptographic Features</a></span></dt><dd><dl><dt><span class="section"><a href="#trouble.crypto.HBASE-10132">13.16.1. sun.security.pkcs11.wrapper.PKCS11Exception: CKR_ARGUMENTS_BAD</a></span></dt></dl></dd></dl></div><div class="section" title="13.1.&nbsp;General Guidelines"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="trouble.general"></a>13.1.&nbsp;General Guidelines</h2></div></div></div><p>
          Always start with the master log (TODO: Which lines?).
          Normally it&#8217;s just printing the same lines over and over again.
          If not, then there&#8217;s an issue.
          Google or <a class="link" href="http://search-hadoop.com" target="_top">search-hadoop.com</a>
          should return some hits for those exceptions you&#8217;re seeing.
      </p><p>
          An error rarely comes alone in Apache HBase, usually when something gets screwed up what will
          follow may be hundreds of exceptions and stack traces coming from all over the place.
          The best way to approach this type of problem is to walk the log up to where it all
          began, for example one trick with RegionServers is that they will print some
          metrics when aborting so grepping for <span class="emphasis"><em>Dump</em></span>
          should get you around the start of the problem.
      </p><p>
          RegionServer suicides are &#8220;normal&#8221;, as this is what they do when something goes wrong.
          For example, if ulimit and xcievers (the two most important initial settings, see <a class="xref" href="#ulimit" title="2.1.2.5.&nbsp; ulimit and nproc">Section&nbsp;2.1.2.5, &#8220;
          <code class="varname">ulimit</code>
            and
          <code class="varname">nproc</code>
        &#8221;</a>)
          aren&#8217;t changed, it will make it impossible at some point for DataNodes to create new threads
          that from the HBase point of view is seen as if HDFS was gone. Think about what would happen if your
          MySQL database was suddenly unable to access files on your local file system, well it&#8217;s the same with
          HBase and HDFS. Another very common reason to see RegionServers committing seppuku is when they enter
          prolonged garbage collection pauses that last longer than the default ZooKeeper session timeout.
          For more information on GC pauses, see the
          <a class="link" href="http://www.cloudera.com/blog/2011/02/avoiding-full-gcs-in-hbase-with-memstore-local-allocation-buffers-part-1/" target="_top">3 part blog post</a>  by Todd Lipcon
          and <a class="xref" href="#gcpause" title="12.3.1.1.&nbsp;Long GC pauses">Section&nbsp;12.3.1.1, &#8220;Long GC pauses&#8221;</a> above.
      </p></div><div class="section" title="13.2.&nbsp;Logs"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="trouble.log"></a>13.2.&nbsp;Logs</h2></div></div></div><p>
      The key process logs are as follows...   (replace &lt;user&gt; with the user that started the service, and &lt;hostname&gt; for the machine name)
      </p><p>
      NameNode:  <code class="filename">$HADOOP_HOME/logs/hadoop-&lt;user&gt;-namenode-&lt;hostname&gt;.log</code>
      </p><p>
      DataNode:  <code class="filename">$HADOOP_HOME/logs/hadoop-&lt;user&gt;-datanode-&lt;hostname&gt;.log</code>
      </p><p>
      JobTracker:  <code class="filename">$HADOOP_HOME/logs/hadoop-&lt;user&gt;-jobtracker-&lt;hostname&gt;.log</code>
      </p><p>
      TaskTracker:  <code class="filename">$HADOOP_HOME/logs/hadoop-&lt;user&gt;-tasktracker-&lt;hostname&gt;.log</code>
      </p><p>
      HMaster:  <code class="filename">$HBASE_HOME/logs/hbase-&lt;user&gt;-master-&lt;hostname&gt;.log</code>
      </p><p>
      RegionServer:  <code class="filename">$HBASE_HOME/logs/hbase-&lt;user&gt;-regionserver-&lt;hostname&gt;.log</code>
      </p><p>
      ZooKeeper:  <code class="filename">TODO</code>
      </p><div class="section" title="13.2.1.&nbsp;Log Locations"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.log.locations"></a>13.2.1.&nbsp;Log Locations</h3></div></div></div><p>For stand-alone deployments the logs are obviously going to be on a single machine, however this is a development configuration only.
        Production deployments need to run on a cluster.</p><div class="section" title="13.2.1.1.&nbsp;NameNode"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.log.locations.namenode"></a>13.2.1.1.&nbsp;NameNode</h4></div></div></div><p>The NameNode log is on the NameNode server.  The HBase Master is typically run on the NameNode server, and well as ZooKeeper.</p><p>For smaller clusters the JobTracker is typically run on the NameNode server as well.</p></div><div class="section" title="13.2.1.2.&nbsp;DataNode"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.log.locations.datanode"></a>13.2.1.2.&nbsp;DataNode</h4></div></div></div><p>Each DataNode server will have a DataNode log for HDFS, as well as a RegionServer log for HBase.</p><p>Additionally, each DataNode server will also have a TaskTracker log for MapReduce task execution.</p></div></div><div class="section" title="13.2.2.&nbsp;Log Levels"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.log.levels"></a>13.2.2.&nbsp;Log Levels</h3></div></div></div><div class="section" title="13.2.2.1.&nbsp;Enabling RPC-level logging"><div class="titlepage"><div><div><h4 class="title"><a name="rpc.logging"></a>13.2.2.1.&nbsp;Enabling RPC-level logging</h4></div></div></div><p>Enabling the RPC-level logging on a RegionServer can often given
           insight on timings at the server.  Once enabled, the amount of log
           spewed is voluminous.  It is not recommended that you leave this
           logging on for more than short bursts of time.  To enable RPC-level
           logging, browse to the RegionServer UI and click on
           <span class="emphasis"><em>Log Level</em></span>.  Set the log level to <code class="varname">DEBUG</code> for the package
           <code class="classname">org.apache.hadoop.ipc</code> (Thats right, for
           <code class="classname">hadoop.ipc</code>, NOT, <code class="classname">hbase.ipc</code>).  Then tail the RegionServers log.  Analyze.</p><p>To disable, set the logging level back to <code class="varname">INFO</code> level.
           </p></div></div><div class="section" title="13.2.3.&nbsp;JVM Garbage Collection Logs"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.log.gc"></a>13.2.3.&nbsp;JVM Garbage Collection Logs</h3></div></div></div><p>HBase is memory intensive, and using the default GC you can see long pauses in all threads including the <span class="emphasis"><em>Juliet Pause</em></span> aka "GC of Death".
           To help debug this or confirm this is happening GC logging can be turned on in the Java virtual machine.
          </p><p>
          To enable, in <code class="filename">hbase-env.sh</code>, uncomment one of the below lines :
          </p><pre class="programlisting">
# This enables basic gc logging to the .out file.
# export SERVER_GC_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps"

# This enables basic gc logging to its own file.
# export SERVER_GC_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:&lt;FILE-PATH&gt;"

# This enables basic GC logging to its own file with automatic log rolling. Only applies to jdk 1.6.0_34+ and 1.7.0_2+.
# export SERVER_GC_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:&lt;FILE-PATH&gt; -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=1 -XX:GCLogFileSize=512M"

# If &lt;FILE-PATH&gt; is not replaced, the log file(.gc) would be generated in the HBASE_LOG_DIR.
          </pre><p>
          </p><p>
           At this point you should see logs like so:
          </p><pre class="programlisting">
64898.952: [GC [1 CMS-initial-mark: 2811538K(3055704K)] 2812179K(3061272K), 0.0007360 secs] [Times: user=0.00 sys=0.00, real=0.00 secs]
64898.953: [CMS-concurrent-mark-start]
64898.971: [GC 64898.971: [ParNew: 5567K-&gt;576K(5568K), 0.0101110 secs] 2817105K-&gt;2812715K(3061272K), 0.0102200 secs] [Times: user=0.07 sys=0.00, real=0.01 secs]
          </pre><p>
          </p><p>
           In this section, the first line indicates a 0.0007360 second pause for the CMS to initially mark. This pauses the entire VM, all threads for that period of time.
            </p><p>
           The third line indicates a "minor GC", which pauses the VM for 0.0101110 seconds - aka 10 milliseconds. It has reduced the "ParNew" from about 5.5m to 576k.
           Later on in this cycle we see:
           </p><pre class="programlisting">
64901.445: [CMS-concurrent-mark: 1.542/2.492 secs] [Times: user=10.49 sys=0.33, real=2.49 secs]
64901.445: [CMS-concurrent-preclean-start]
64901.453: [GC 64901.453: [ParNew: 5505K-&gt;573K(5568K), 0.0062440 secs] 2868746K-&gt;2864292K(3061272K), 0.0063360 secs] [Times: user=0.05 sys=0.00, real=0.01 secs]
64901.476: [GC 64901.476: [ParNew: 5563K-&gt;575K(5568K), 0.0072510 secs] 2869283K-&gt;2864837K(3061272K), 0.0073320 secs] [Times: user=0.05 sys=0.01, real=0.01 secs]
64901.500: [GC 64901.500: [ParNew: 5517K-&gt;573K(5568K), 0.0120390 secs] 2869780K-&gt;2865267K(3061272K), 0.0121150 secs] [Times: user=0.09 sys=0.00, real=0.01 secs]
64901.529: [GC 64901.529: [ParNew: 5507K-&gt;569K(5568K), 0.0086240 secs] 2870200K-&gt;2865742K(3061272K), 0.0087180 secs] [Times: user=0.05 sys=0.00, real=0.01 secs]
64901.554: [GC 64901.555: [ParNew: 5516K-&gt;575K(5568K), 0.0107130 secs] 2870689K-&gt;2866291K(3061272K), 0.0107820 secs] [Times: user=0.06 sys=0.00, real=0.01 secs]
64901.578: [CMS-concurrent-preclean: 0.070/0.133 secs] [Times: user=0.48 sys=0.01, real=0.14 secs]
64901.578: [CMS-concurrent-abortable-preclean-start]
64901.584: [GC 64901.584: [ParNew: 5504K-&gt;571K(5568K), 0.0087270 secs] 2871220K-&gt;2866830K(3061272K), 0.0088220 secs] [Times: user=0.05 sys=0.00, real=0.01 secs]
64901.609: [GC 64901.609: [ParNew: 5512K-&gt;569K(5568K), 0.0063370 secs] 2871771K-&gt;2867322K(3061272K), 0.0064230 secs] [Times: user=0.06 sys=0.00, real=0.01 secs]
64901.615: [CMS-concurrent-abortable-preclean: 0.007/0.037 secs] [Times: user=0.13 sys=0.00, real=0.03 secs]
64901.616: [GC[YG occupancy: 645 K (5568 K)]64901.616: [Rescan (parallel) , 0.0020210 secs]64901.618: [weak refs processing, 0.0027950 secs] [1 CMS-remark: 2866753K(3055704K)] 2867399K(3061272K), 0.0049380 secs] [Times: user=0.00 sys=0.01, real=0.01 secs]
64901.621: [CMS-concurrent-sweep-start]
            </pre><p>
            </p><p>
            The first line indicates that the CMS concurrent mark (finding garbage) has taken 2.4 seconds. But this is a _concurrent_ 2.4 seconds, Java has not been paused at any point in time.
            </p><p>
            There are a few more minor GCs, then there is a pause at the 2nd last line:
            </p><pre class="programlisting">
64901.616: [GC[YG occupancy: 645 K (5568 K)]64901.616: [Rescan (parallel) , 0.0020210 secs]64901.618: [weak refs processing, 0.0027950 secs] [1 CMS-remark: 2866753K(3055704K)] 2867399K(3061272K), 0.0049380 secs] [Times: user=0.00 sys=0.01, real=0.01 secs]
            </pre><p>
            </p><p>
            The pause here is 0.0049380 seconds (aka 4.9 milliseconds) to 'remark' the heap.
            </p><p>
            At this point the sweep starts, and you can watch the heap size go down:
            </p><pre class="programlisting">
64901.637: [GC 64901.637: [ParNew: 5501K-&gt;569K(5568K), 0.0097350 secs] 2871958K-&gt;2867441K(3061272K), 0.0098370 secs] [Times: user=0.05 sys=0.00, real=0.01 secs]
...  lines removed ...
64904.936: [GC 64904.936: [ParNew: 5532K-&gt;568K(5568K), 0.0070720 secs] 1365024K-&gt;1360689K(3061272K), 0.0071930 secs] [Times: user=0.05 sys=0.00, real=0.01 secs]
64904.953: [CMS-concurrent-sweep: 2.030/3.332 secs] [Times: user=9.57 sys=0.26, real=3.33 secs]
            </pre><p>
            At this point, the CMS sweep took 3.332 seconds, and heap went from about ~ 2.8 GB to 1.3 GB (approximate).
            </p><p>
            The key points here is to keep all these pauses low. CMS pauses are always low, but if your ParNew starts growing, you can see minor GC pauses approach 100ms, exceed 100ms and hit as high at 400ms.
            </p><p>
            This can be due to the size of the ParNew, which should be relatively small. If your ParNew is very large after running HBase for a while, in one example a ParNew was about 150MB, then you might have to constrain the size of ParNew (The larger it is, the longer the collections take but if its too small, objects are promoted to old gen too quickly). In the below we constrain new gen size to 64m.
            </p><p>
             Add the below line in <code class="filename">hbase-env.sh</code>:
            </p><pre class="programlisting">
export SERVER_GC_OPTS="$SERVER_GC_OPTS -XX:NewSize=64m -XX:MaxNewSize=64m"
            </pre><p>
            </p><p>
            Similarly, to enable GC logging for client processes, uncomment one of the below lines in <code class="filename">hbase-env.sh</code>:
            </p><pre class="programlisting">
# This enables basic gc logging to the .out file.
# export CLIENT_GC_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps"

# This enables basic gc logging to its own file.
# export CLIENT_GC_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:&lt;FILE-PATH&gt;"

# This enables basic GC logging to its own file with automatic log rolling. Only applies to jdk 1.6.0_34+ and 1.7.0_2+.
# export CLIENT_GC_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:&lt;FILE-PATH&gt; -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=1 -XX:GCLogFileSize=512M"

# If &lt;FILE-PATH&gt; is not replaced, the log file(.gc) would be generated in the HBASE_LOG_DIR .
            </pre><p>
            </p><p>
            For more information on GC pauses, see the <a class="link" href="http://www.cloudera.com/blog/2011/02/avoiding-full-gcs-in-hbase-with-memstore-local-allocation-buffers-part-1/" target="_top">3 part blog post</a>  by Todd Lipcon
            and <a class="xref" href="#gcpause" title="12.3.1.1.&nbsp;Long GC pauses">Section&nbsp;12.3.1.1, &#8220;Long GC pauses&#8221;</a> above.
            </p></div></div><div class="section" title="13.3.&nbsp;Resources"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="trouble.resources"></a>13.3.&nbsp;Resources</h2></div></div></div><div class="section" title="13.3.1.&nbsp;search-hadoop.com"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.resources.searchhadoop"></a>13.3.1.&nbsp;search-hadoop.com</h3></div></div></div><p>
        <a class="link" href="http://search-hadoop.com" target="_top">search-hadoop.com</a> indexes all the mailing lists and is great for historical searches.
        Search here first when you have an issue as its more than likely someone has already had your problem.
        </p></div><div class="section" title="13.3.2.&nbsp;Mailing Lists"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.resources.lists"></a>13.3.2.&nbsp;Mailing Lists</h3></div></div></div><p>Ask a question on the <a class="link" href="http://hbase.apache.org/mail-lists.html" target="_top">Apache HBase mailing lists</a>.
        The 'dev' mailing list is aimed at the community of developers actually building Apache HBase and for features currently under development, and 'user'
        is generally used for questions on released versions of Apache HBase.  Before going to the mailing list, make sure your
        question has not already been answered by searching the mailing list archives first.  Use
        <a class="xref" href="#trouble.resources.searchhadoop" title="13.3.1.&nbsp;search-hadoop.com">Section&nbsp;13.3.1, &#8220;search-hadoop.com&#8221;</a>.
        Take some time crafting your question<sup>[<a name="d366e9696" href="#ftn.d366e9696" class="footnote">32</a>]</sup>; a quality question that includes all context and
        exhibits evidence the author has tried to find answers in the manual and out on lists
        is more likely to get a prompt response.
        </p></div><div class="section" title="13.3.3.&nbsp;IRC"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.resources.irc"></a>13.3.3.&nbsp;IRC</h3></div></div></div><p>#hbase on irc.freenode.net</p></div><div class="section" title="13.3.4.&nbsp;JIRA"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.resources.jira"></a>13.3.4.&nbsp;JIRA</h3></div></div></div><p>
        <a class="link" href="https://issues.apache.org/jira/browse/HBASE" target="_top">JIRA</a> is also really helpful when looking for Hadoop/HBase-specific issues.
        </p></div></div><div class="section" title="13.4.&nbsp;Tools"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="trouble.tools"></a>13.4.&nbsp;Tools</h2></div></div></div><div class="section" title="13.4.1.&nbsp;Builtin Tools"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.tools.builtin"></a>13.4.1.&nbsp;Builtin Tools</h3></div></div></div><div class="section" title="13.4.1.1.&nbsp;Master Web Interface"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.tools.builtin.webmaster"></a>13.4.1.1.&nbsp;Master Web Interface</h4></div></div></div><p>The Master starts a web-interface on port 16010 by default.
	      (Up to and including 0.98 this was port 60010)
              </p><p>The Master web UI lists created tables and their definition (e.g., ColumnFamilies, blocksize, etc.).  Additionally,
              the available RegionServers in the cluster are listed along with selected high-level metrics (requests, number of regions, usedHeap, maxHeap).
              The Master web UI allows navigation to each RegionServer's web UI.
              </p></div><div class="section" title="13.4.1.2.&nbsp;RegionServer Web Interface"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.tools.builtin.webregion"></a>13.4.1.2.&nbsp;RegionServer Web Interface</h4></div></div></div><p>RegionServers starts a web-interface on port 16030 by default.
              (Up to an including 0.98 this was port 60030)
              </p><p>The RegionServer web UI lists online regions and their start/end keys, as well as point-in-time RegionServer metrics (requests, regions, storeFileIndexSize, compactionQueueSize, etc.).
              </p><p>See <a class="xref" href="#hbase_metrics" title="15.4.&nbsp;HBase Metrics">Section&nbsp;15.4, &#8220;HBase Metrics&#8221;</a> for more information in metric definitions.
            </p></div><div class="section" title="13.4.1.3.&nbsp;zkcli"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.tools.builtin.zkcli"></a>13.4.1.3.&nbsp;zkcli</h4></div></div></div><p><code class="code">zkcli</code> is a very useful tool for investigating ZooKeeper-related issues.  To invoke:
</p><pre class="programlisting">
./hbase zkcli -server host:port &lt;cmd&gt; &lt;args&gt;
</pre><p>
              The commands (and arguments) are:
</p><pre class="programlisting">
	connect host:port
	get path [watch]
	ls path [watch]
	set path data [version]
	delquota [-n|-b] path
	quit
	printwatches on|off
	create [-s] [-e] path data acl
	stat path [watch]
	close
	ls2 path [watch]
	history
	listquota path
	setAcl path acl
	getAcl path
	sync path
	redo cmdno
	addauth scheme auth
	delete path [version]
	setquota -n|-b val path
</pre><p>
            </p></div></div><div class="section" title="13.4.2.&nbsp;External Tools"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.tools.external"></a>13.4.2.&nbsp;External Tools</h3></div></div></div><div class="section" title="13.4.2.1.&nbsp;tail"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.tools.tail"></a>13.4.2.1.&nbsp;tail</h4></div></div></div><p>
        <code class="code">tail</code> is the command line tool that lets you look at the end of a file. Add the &#8220;-f&#8221; option and it will refresh when new data is available. It&#8217;s useful when you are wondering what&#8217;s happening, for example, when a cluster is taking a long time to shutdown or startup as you can just fire a new terminal and tail the master log (and maybe a few RegionServers).
        </p></div><div class="section" title="13.4.2.2.&nbsp;top"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.tools.top"></a>13.4.2.2.&nbsp;top</h4></div></div></div><p>
        <code class="code">top</code> is probably one of the most important tool when first trying to see what&#8217;s running on a machine and how the resources are consumed. Here&#8217;s an example from production system:
        </p><pre class="programlisting">
top - 14:46:59 up 39 days, 11:55,  1 user,  load average: 3.75, 3.57, 3.84
Tasks: 309 total,   1 running, 308 sleeping,   0 stopped,   0 zombie
Cpu(s):  4.5%us,  1.6%sy,  0.0%ni, 91.7%id,  1.4%wa,  0.1%hi,  0.6%si,  0.0%st
Mem:  24414432k total, 24296956k used,   117476k free,     7196k buffers
Swap: 16008732k total,	14348k used, 15994384k free, 11106908k cached

  PID USER  	PR  NI  VIRT  RES  SHR S %CPU %MEM	TIME+  COMMAND
15558 hadoop	18  -2 3292m 2.4g 3556 S   79 10.4   6523:52 java
13268 hadoop	18  -2 8967m 8.2g 4104 S   21 35.1   5170:30 java
 8895 hadoop	18  -2 1581m 497m 3420 S   11  2.1   4002:32 java
&#8230;
        </pre><p>
        </p><p>
        Here we can see that the system load average during the last five minutes is 3.75, which very roughly means that on average 3.75 threads were waiting for CPU time during these 5 minutes.  In general, the &#8220;perfect&#8221; utilization equals to the number of cores, under that number the machine is under utilized and over that the machine is over utilized.  This is an important concept, see this article to understand it more: <a class="link" href="http://www.linuxjournal.com/article/9001" target="_top">http://www.linuxjournal.com/article/9001</a>.
        </p><p>
        Apart from load, we can see that the system is using almost all its available RAM but most of it is used for the OS cache (which is good). The swap only has a few KBs in it and this is wanted, high numbers would indicate swapping activity which is the nemesis of performance of Java systems. Another way to detect swapping is when the load average goes through the roof (although this could also be caused by things like a dying disk, among others).
        </p><p>
        The list of processes isn&#8217;t super useful by default, all we know is that 3 java processes are using about 111% of the CPUs. To know which is which, simply type &#8220;c&#8221; and each line will be expanded. Typing &#8220;1&#8221; will give you the detail of how each CPU is used instead of the average for all of them like shown here.
        </p></div><div class="section" title="13.4.2.3.&nbsp;jps"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.tools.jps"></a>13.4.2.3.&nbsp;jps</h4></div></div></div><p>
        <code class="code">jps</code> is shipped with every JDK and gives the java process ids for the current user (if root, then it gives the ids for all users). Example:
        </p><pre class="programlisting">
hadoop@sv4borg12:~$ jps
1322 TaskTracker
17789 HRegionServer
27862 Child
1158 DataNode
25115 HQuorumPeer
2950 Jps
19750 ThriftServer
18776 jmx
        </pre><p>
        In order, we see a:
        </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Hadoop TaskTracker, manages the local Childs</li><li class="listitem">HBase RegionServer, serves regions</li><li class="listitem">Child, its MapReduce task, cannot tell which type exactly</li><li class="listitem">Hadoop TaskTracker, manages the local Childs</li><li class="listitem">Hadoop DataNode, serves blocks</li><li class="listitem">HQuorumPeer, a ZooKeeper ensemble member</li><li class="listitem">Jps, well&#8230; it&#8217;s the current process</li><li class="listitem">ThriftServer, it&#8217;s a special one will be running only if thrift was started</li><li class="listitem">jmx, this is a local process that&#8217;s part of our monitoring platform ( poorly named maybe). You probably don&#8217;t have that.</li></ul></div><p>
        </p><p>
      You can then do stuff like checking out the full command line that started the process:
        </p><pre class="programlisting">
hadoop@sv4borg12:~$ ps aux | grep HRegionServer
hadoop   17789  155 35.2 9067824 8604364 ?     S&lt;l  Mar04 9855:48 /usr/java/jdk1.6.0_14/bin/java -Xmx8000m -XX:+DoEscapeAnalysis -XX:+AggressiveOpts -XX:+UseConcMarkSweepGC -XX:NewSize=64m -XX:MaxNewSize=64m -XX:CMSInitiatingOccupancyFraction=88 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:/export1/hadoop/logs/gc-hbase.log -Dcom.sun.management.jmxremote.port=10102 -Dcom.sun.management.jmxremote.authenticate=true -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.password.file=/home/hadoop/hbase/conf/jmxremote.password -Dcom.sun.management.jmxremote -Dhbase.log.dir=/export1/hadoop/logs -Dhbase.log.file=hbase-hadoop-regionserver-sv4borg12.log -Dhbase.home.dir=/home/hadoop/hbase -Dhbase.id.str=hadoop -Dhbase.root.logger=INFO,DRFA -Djava.library.path=/home/hadoop/hbase/lib/native/Linux-amd64-64 -classpath /home/hadoop/hbase/bin/../conf:[many jars]:/home/hadoop/hadoop/conf org.apache.hadoop.hbase.regionserver.HRegionServer start
        </pre><p>
        </p></div><div class="section" title="13.4.2.4.&nbsp;jstack"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.tools.jstack"></a>13.4.2.4.&nbsp;jstack</h4></div></div></div><p>
        <code class="code">jstack</code> is one of the most important tools when trying to figure out what a java process is doing apart from looking at the logs. It has to be used in conjunction with jps in order to give it a process id. It shows a list of threads, each one has a name, and they appear in the order that they were created (so the top ones are the most recent threads). Here&#8217;s a few example:
        </p><p>
        The main thread of a RegionServer that&#8217;s waiting for something to do from the master:
        </p><pre class="programlisting">
      "regionserver60020" prio=10 tid=0x0000000040ab4000 nid=0x45cf waiting on condition [0x00007f16b6a96000..0x00007f16b6a96a70]
   java.lang.Thread.State: TIMED_WAITING (parking)
        	at sun.misc.Unsafe.park(Native Method)
        	- parking to wait for  &lt;0x00007f16cd5c2f30&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:198)
        	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1963)
        	at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:395)
        	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:647)
        	at java.lang.Thread.run(Thread.java:619)

        	The MemStore flusher thread that is currently flushing to a file:
"regionserver60020.cacheFlusher" daemon prio=10 tid=0x0000000040f4e000 nid=0x45eb in Object.wait() [0x00007f16b5b86000..0x00007f16b5b87af0]
   java.lang.Thread.State: WAITING (on object monitor)
        	at java.lang.Object.wait(Native Method)
        	at java.lang.Object.wait(Object.java:485)
        	at org.apache.hadoop.ipc.Client.call(Client.java:803)
        	- locked &lt;0x00007f16cb14b3a8&gt; (a org.apache.hadoop.ipc.Client$Call)
        	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:221)
        	at $Proxy1.complete(Unknown Source)
        	at sun.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)
        	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        	at java.lang.reflect.Method.invoke(Method.java:597)
        	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
        	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
        	at $Proxy1.complete(Unknown Source)
        	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.closeInternal(DFSClient.java:3390)
        	- locked &lt;0x00007f16cb14b470&gt; (a org.apache.hadoop.hdfs.DFSClient$DFSOutputStream)
        	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.close(DFSClient.java:3304)
        	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:61)
        	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:86)
        	at org.apache.hadoop.hbase.io.hfile.HFile$Writer.close(HFile.java:650)
        	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.close(StoreFile.java:853)
        	at org.apache.hadoop.hbase.regionserver.Store.internalFlushCache(Store.java:467)
        	- locked &lt;0x00007f16d00e6f08&gt; (a java.lang.Object)
        	at org.apache.hadoop.hbase.regionserver.Store.flushCache(Store.java:427)
        	at org.apache.hadoop.hbase.regionserver.Store.access$100(Store.java:80)
        	at org.apache.hadoop.hbase.regionserver.Store$StoreFlusherImpl.flushCache(Store.java:1359)
        	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:907)
        	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:834)
        	at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:786)
        	at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:250)
        	at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:224)
        	at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.run(MemStoreFlusher.java:146)
        </pre><p>
        </p><p>
        	A handler thread that&#8217;s waiting for stuff to do (like put, delete, scan, etc):
        </p><pre class="programlisting">
"IPC Server handler 16 on 60020" daemon prio=10 tid=0x00007f16b011d800 nid=0x4a5e waiting on condition [0x00007f16afefd000..0x00007f16afefd9f0]
   java.lang.Thread.State: WAITING (parking)
        	at sun.misc.Unsafe.park(Native Method)
        	- parking to wait for  &lt;0x00007f16cd3f8dd8&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1013)
        </pre><p>
        </p><p>
              	And one that&#8217;s busy doing an increment of a counter (it&#8217;s in the phase where it&#8217;s trying to create a scanner in order to read the last value):
        </p><pre class="programlisting">
"IPC Server handler 66 on 60020" daemon prio=10 tid=0x00007f16b006e800 nid=0x4a90 runnable [0x00007f16acb77000..0x00007f16acb77cf0]
   java.lang.Thread.State: RUNNABLE
        	at org.apache.hadoop.hbase.regionserver.KeyValueHeap.&lt;init&gt;(KeyValueHeap.java:56)
        	at org.apache.hadoop.hbase.regionserver.StoreScanner.&lt;init&gt;(StoreScanner.java:79)
        	at org.apache.hadoop.hbase.regionserver.Store.getScanner(Store.java:1202)
        	at org.apache.hadoop.hbase.regionserver.HRegion$RegionScanner.&lt;init&gt;(HRegion.java:2209)
        	at org.apache.hadoop.hbase.regionserver.HRegion.instantiateInternalScanner(HRegion.java:1063)
        	at org.apache.hadoop.hbase.regionserver.HRegion.getScanner(HRegion.java:1055)
        	at org.apache.hadoop.hbase.regionserver.HRegion.getScanner(HRegion.java:1039)
        	at org.apache.hadoop.hbase.regionserver.HRegion.getLastIncrement(HRegion.java:2875)
        	at org.apache.hadoop.hbase.regionserver.HRegion.incrementColumnValue(HRegion.java:2978)
        	at org.apache.hadoop.hbase.regionserver.HRegionServer.incrementColumnValue(HRegionServer.java:2433)
        	at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)
        	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        	at java.lang.reflect.Method.invoke(Method.java:597)
        	at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:560)
        	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1027)
        </pre><p>
        </p><p>
        	A thread that receives data from HDFS:
        </p><pre class="programlisting">
"IPC Client (47) connection to sv4borg9/10.4.24.40:9000 from hadoop" daemon prio=10 tid=0x00007f16a02d0000 nid=0x4fa3 runnable [0x00007f16b517d000..0x00007f16b517dbf0]
   java.lang.Thread.State: RUNNABLE
        	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:215)
        	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
        	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
        	- locked &lt;0x00007f17d5b68c00&gt; (a sun.nio.ch.Util$1)
        	- locked &lt;0x00007f17d5b68be8&gt; (a java.util.Collections$UnmodifiableSet)
        	- locked &lt;0x00007f1877959b50&gt; (a sun.nio.ch.EPollSelectorImpl)
        	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
        	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:332)
        	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
        	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:155)
        	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:128)
        	at java.io.FilterInputStream.read(FilterInputStream.java:116)
        	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:304)
        	at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
        	at java.io.BufferedInputStream.read(BufferedInputStream.java:237)
        	- locked &lt;0x00007f1808539178&gt; (a java.io.BufferedInputStream)
        	at java.io.DataInputStream.readInt(DataInputStream.java:370)
        	at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:569)
        	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:477)
          </pre><p>
          </p><p>
           	And here is a master trying to recover a lease after a RegionServer died:
          </p><pre class="programlisting">
"LeaseChecker" daemon prio=10 tid=0x00000000407ef800 nid=0x76cd waiting on condition [0x00007f6d0eae2000..0x00007f6d0eae2a70]
--
   java.lang.Thread.State: WAITING (on object monitor)
        	at java.lang.Object.wait(Native Method)
        	at java.lang.Object.wait(Object.java:485)
        	at org.apache.hadoop.ipc.Client.call(Client.java:726)
        	- locked &lt;0x00007f6d1cd28f80&gt; (a org.apache.hadoop.ipc.Client$Call)
        	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)
        	at $Proxy1.recoverBlock(Unknown Source)
        	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:2636)
        	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.&lt;init&gt;(DFSClient.java:2832)
        	at org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:529)
        	at org.apache.hadoop.hdfs.DistributedFileSystem.append(DistributedFileSystem.java:186)
        	at org.apache.hadoop.fs.FileSystem.append(FileSystem.java:530)
        	at org.apache.hadoop.hbase.util.FSUtils.recoverFileLease(FSUtils.java:619)
        	at org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(HLog.java:1322)
        	at org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(HLog.java:1210)
        	at org.apache.hadoop.hbase.master.HMaster.splitLogAfterStartup(HMaster.java:648)
        	at org.apache.hadoop.hbase.master.HMaster.joinCluster(HMaster.java:572)
        	at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:503)
          </pre><p>
          </p></div><div class="section" title="13.4.2.5.&nbsp;OpenTSDB"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.tools.opentsdb"></a>13.4.2.5.&nbsp;OpenTSDB</h4></div></div></div><p>
          <a class="link" href="http://opentsdb.net" target="_top">OpenTSDB</a> is an excellent alternative to Ganglia as it uses Apache HBase to store all the time series and doesn&#8217;t have to downsample. Monitoring your own HBase cluster that hosts OpenTSDB is a good exercise.
          </p><p>
          Here&#8217;s an example of a cluster that&#8217;s suffering from hundreds of compactions launched almost all around the same time, which severely affects the IO performance:  (TODO:  insert graph plotting compactionQueueSize)
          </p><p>
          It&#8217;s a good practice to build dashboards with all the important graphs per machine and per cluster so that debugging issues can be done with a single quick look. For example, at StumbleUpon there&#8217;s one dashboard per cluster with the most important metrics from both the OS and Apache HBase. You can then go down at the machine level and get even more detailed metrics.
          </p></div><div class="section" title="13.4.2.6.&nbsp;clusterssh+top"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.tools.clustersshtop"></a>13.4.2.6.&nbsp;clusterssh+top</h4></div></div></div><p>
          clusterssh+top, it&#8217;s like a poor man&#8217;s monitoring system and it can be quite useful when you have only a few machines as it&#8217;s very easy to setup. Starting clusterssh will give you one terminal per machine and another terminal in which whatever you type will be retyped in every window. This means that you can type &#8220;top&#8221; once and it will start it for all of your machines at the same time giving you full view of the current state of your cluster. You can also tail all the logs at the same time, edit files, etc.
          </p></div></div></div><div class="section" title="13.5.&nbsp;Client"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="trouble.client"></a>13.5.&nbsp;Client</h2></div></div></div><p>For more information on the HBase client, see <a class="xref" href="#client" title="9.3.&nbsp;Client">Section&nbsp;9.3, &#8220;Client&#8221;</a>.
       </p><div class="section" title="13.5.1.&nbsp;ScannerTimeoutException or UnknownScannerException"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.client.scantimeout"></a>13.5.1.&nbsp;ScannerTimeoutException or UnknownScannerException</h3></div></div></div><p>This is thrown if the time between RPC calls from the client to RegionServer exceeds the scan timeout.
            For example, if <code class="code">Scan.setCaching</code> is set to 500, then there will be an RPC call to fetch the next batch of rows every 500 <code class="code">.next()</code> calls on the ResultScanner
            because data is being transferred in blocks of 500 rows to the client.  Reducing the setCaching value may be an option, but setting this value too low makes for inefficient
            processing on numbers of rows.
            </p><p>See <a class="xref" href="#perf.hbase.client.caching" title="12.9.1.&nbsp;Scan Caching">Section&nbsp;12.9.1, &#8220;Scan Caching&#8221;</a>.
            </p></div><div class="section" title="13.5.2.&nbsp;LeaseException when calling Scanner.next"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.client.lease.exception"></a>13.5.2.&nbsp;<code class="classname">LeaseException</code> when calling <code class="classname">Scanner.next</code></h3></div></div></div><p>
In some situations clients that fetch data from a RegionServer get a LeaseException instead of the usual
<a class="xref" href="#trouble.client.scantimeout" title="13.5.1.&nbsp;ScannerTimeoutException or UnknownScannerException">Section&nbsp;13.5.1, &#8220;ScannerTimeoutException or UnknownScannerException&#8221;</a>.  Usually the source of the exception is
<code class="classname">org.apache.hadoop.hbase.regionserver.Leases.removeLease(Leases.java:230)</code> (line number may vary).
It tends to happen in the context of a slow/freezing RegionServer#next call.
It can be prevented by having <code class="varname">hbase.rpc.timeout</code> &gt; <code class="varname">hbase.regionserver.lease.period</code>.
Harsh J investigated the issue as part of the mailing list thread
<a class="link" href="http://mail-archives.apache.org/mod_mbox/hbase-user/201209.mbox/%3CCAOcnVr3R-LqtKhFsk8Bhrm-YW2i9O6J6Fhjz2h7q6_sxvwd2yw%40mail.gmail.com%3E" target="_top">HBase, mail # user - Lease does not exist exceptions</a>
            </p></div><div class="section" title="13.5.3.&nbsp;Shell or client application throws lots of scary exceptions during normal operation"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.client.scarylogs"></a>13.5.3.&nbsp;Shell or client application throws lots of scary exceptions during normal operation</h3></div></div></div><p>Since 0.20.0 the default log level for <code class="code">org.apache.hadoop.hbase.*</code>is DEBUG. </p><p>
            On your clients, edit <code class="filename">$HBASE_HOME/conf/log4j.properties</code> and change this: <code class="code">log4j.logger.org.apache.hadoop.hbase=DEBUG</code> to this: <code class="code">log4j.logger.org.apache.hadoop.hbase=INFO</code>, or even <code class="code">log4j.logger.org.apache.hadoop.hbase=WARN</code>.
            </p></div><div class="section" title="13.5.4.&nbsp;Long Client Pauses With Compression"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.client.longpauseswithcompression"></a>13.5.4.&nbsp;Long Client Pauses With Compression</h3></div></div></div><p>This is a fairly frequent question on the Apache HBase dist-list.  The scenario is that a client is typically inserting a lot of data into a
            relatively un-optimized HBase cluster.  Compression can exacerbate the pauses, although it is not the source of the problem.</p><p>See <a class="xref" href="#precreate.regions" title="12.8.2.&nbsp; Table Creation: Pre-Creating Regions">Section&nbsp;12.8.2, &#8220;
    Table Creation: Pre-Creating Regions
    &#8221;</a> on the pattern for pre-creating regions and confirm that the table isn't starting with a single region.</p><p>See <a class="xref" href="#perf.configurations" title="12.4.&nbsp;HBase Configurations">Section&nbsp;12.4, &#8220;HBase Configurations&#8221;</a> for cluster configuration, particularly <code class="code">hbase.hstore.blockingStoreFiles</code>, <code class="code">hbase.hregion.memstore.block.multiplier</code>,
            <code class="code">MAX_FILESIZE</code> (region size), and <code class="code">MEMSTORE_FLUSHSIZE.</code>  </p><p>A slightly longer explanation of why pauses can happen is as follows:  Puts are sometimes blocked on the MemStores which are blocked by the flusher thread which is blocked because there are
            too many files to compact because the compactor is given too many small files to compact and has to compact the same data repeatedly.  This situation can occur even with minor compactions.
            Compounding this situation, Apache HBase doesn't compress data in memory.  Thus, the 64MB that lives in the MemStore could become a 6MB file after compression - which results in a smaller StoreFile.  The upside is that
            more data is packed into the same region, but performance is achieved by being able to write larger files - which is why HBase waits until the flushize before writing a new StoreFile.  And smaller StoreFiles
            become targets for compaction.  Without compression the files are much bigger and don't need as much compaction, however this is at the expense of I/O.
            </p><p>
            For additional information, see this thread on <a class="link" href="http://search-hadoop.com/m/WUnLM6ojHm1/Long+client+pauses+with+compression&amp;subj=Long+client+pauses+with+compression" target="_top">Long client pauses with compression</a>.
            </p></div><div class="section" title="13.5.5.&nbsp;ZooKeeper Client Connection Errors"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.client.zookeeper"></a>13.5.5.&nbsp;ZooKeeper Client Connection Errors</h3></div></div></div><p>Errors like this...
</p><pre class="programlisting">
11/07/05 11:26:41 WARN zookeeper.ClientCnxn: Session 0x0 for server null,
 unexpected error, closing socket connection and attempting reconnect
 java.net.ConnectException: Connection refused: no further information
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(Unknown Source)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1078)
 11/07/05 11:26:43 INFO zookeeper.ClientCnxn: Opening socket connection to
 server localhost/127.0.0.1:2181
 11/07/05 11:26:44 WARN zookeeper.ClientCnxn: Session 0x0 for server null,
 unexpected error, closing socket connection and attempting reconnect
 java.net.ConnectException: Connection refused: no further information
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(Unknown Source)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1078)
 11/07/05 11:26:45 INFO zookeeper.ClientCnxn: Opening socket connection to
 server localhost/127.0.0.1:2181
</pre><p>
            ... are either due to ZooKeeper being down, or unreachable due to network issues.
            </p><p>The utility <a class="xref" href="#trouble.tools.builtin.zkcli" title="13.4.1.3.&nbsp;zkcli">Section&nbsp;13.4.1.3, &#8220;zkcli&#8221;</a> may help investigate ZooKeeper issues.
            </p></div><div class="section" title="13.5.6.&nbsp;Client running out of memory though heap size seems to be stable (but the off-heap/direct heap keeps growing)"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.client.oome.directmemory.leak"></a>13.5.6.&nbsp;Client running out of memory though heap size seems to be stable (but the off-heap/direct heap keeps growing)</h3></div></div></div><p>
You are likely running into the issue that is described and worked through in
the mail thread HBase, mail # user - Suspected memory leak
and continued over in HBase, mail # dev - FeedbackRe: Suspected memory leak.
A workaround is passing your client-side JVM a reasonable value for <code class="code">-XX:MaxDirectMemorySize</code>.  By default,
the <code class="varname">MaxDirectMemorySize</code> is equal to your <code class="code">-Xmx</code> max heapsize setting (if <code class="code">-Xmx</code> is set).
Try seting it to something smaller (for example, one user had success setting it to <code class="code">1g</code> when
they had a client-side heap of <code class="code">12g</code>).  If you set it too small, it will bring on <code class="code">FullGCs</code> so keep
it  a bit hefty.  You want to make this setting client-side only especially if you are running the new experiemental
server-side off-heap cache since this feature depends on being able to use big direct buffers (You may have to keep
separate client-side and server-side config dirs).
            </p></div><div class="section" title="13.5.7.&nbsp;Client Slowdown When Calling Admin Methods (flush, compact, etc.)"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.client.slowdown.admin"></a>13.5.7.&nbsp;Client Slowdown When Calling Admin Methods (flush, compact, etc.)</h3></div></div></div><p>
This is a client issue fixed by <a class="link" href="https://issues.apache.org/jira/browse/HBASE-5073" target="_top">HBASE-5073</a> in 0.90.6.
There was a ZooKeeper leak in the client and the client was getting pummeled by ZooKeeper events with each additional
invocation of the admin API.
            </p></div><div class="section" title="13.5.8.&nbsp;Secure Client Cannot Connect ([Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)])"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.client.security.rpc"></a>13.5.8.&nbsp;Secure Client Cannot Connect ([Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)])</h3></div></div></div><p>
There can be several causes that produce this symptom.
           </p><p>
First, check that you have a valid Kerberos ticket. One is required in order to set up communication with a secure Apache HBase cluster. Examine the ticket currently in the credential cache, if any, by running the klist command line utility. If no ticket is listed, you must obtain a ticket by running the kinit command with either a keytab specified, or by interactively entering a password for the desired principal.
           </p><p>
Then, consult the <a class="link" href="http://docs.oracle.com/javase/1.5.0/docs/guide/security/jgss/tutorials/Troubleshooting.html" target="_top">Java Security Guide troubleshooting section</a>. The most common problem addressed there is resolved by setting javax.security.auth.useSubjectCredsOnly system property value to false.
           </p><p>
Because of a change in the format in which MIT Kerberos writes its credentials cache, there is a bug in the Oracle JDK 6 Update 26 and earlier that causes Java to be unable to read the Kerberos credentials cache created by versions of MIT Kerberos 1.8.1 or higher. If you have this problematic combination of components in your environment, to work around this problem, first log in with kinit and then immediately refresh the credential cache with kinit -R. The refresh will rewrite the credential cache without the problematic formatting.
           </p><p>
Finally, depending on your Kerberos configuration, you may need to install the <a class="link" href="http://docs.oracle.com/javase/1.4.2/docs/guide/security/jce/JCERefGuide.html" target="_top">Java Cryptography Extension</a>, or JCE. Insure the JCE jars are on the classpath on both server and client systems.
           </p><p>
You may also need to download the <a class="link" href="http://www.oracle.com/technetwork/java/javase/downloads/jce-6-download-429243.html" target="_top">unlimited strength JCE policy files</a>. Uncompress and extract the downloaded file, and install the policy jars into &lt;java-home&gt;/lib/security.
           </p></div></div><div class="section" title="13.6.&nbsp;MapReduce"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="trouble.mapreduce"></a>13.6.&nbsp;MapReduce</h2></div></div></div><div class="section" title="13.6.1.&nbsp;You Think You're On The Cluster, But You're Actually Local"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.mapreduce.local"></a>13.6.1.&nbsp;You Think You're On The Cluster, But You're Actually Local</h3></div></div></div><p>This following stacktrace happened using <code class="code">ImportTsv</code>, but things like this
        can happen on any job with a mis-configuration.
</p><pre class="programlisting">
    WARN mapred.LocalJobRunner: job_local_0001
java.lang.IllegalArgumentException: Can't read partitions file
       at org.apache.hadoop.hbase.mapreduce.hadoopbackport.TotalOrderPartitioner.setConf(TotalOrderPartitioner.java:111)
       at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:62)
       at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)
       at org.apache.hadoop.mapred.MapTask$NewOutputCollector.&lt;init&gt;(MapTask.java:560)
       at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:639)
       at org.apache.hadoop.mapred.MapTask.run(MapTask.java:323)
       at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:210)
Caused by: java.io.FileNotFoundException: File _partition.lst does not exist.
       at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:383)
       at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:251)
       at org.apache.hadoop.fs.FileSystem.getLength(FileSystem.java:776)
       at org.apache.hadoop.io.SequenceFile$Reader.&lt;init&gt;(SequenceFile.java:1424)
       at org.apache.hadoop.io.SequenceFile$Reader.&lt;init&gt;(SequenceFile.java:1419)
       at org.apache.hadoop.hbase.mapreduce.hadoopbackport.TotalOrderPartitioner.readPartitions(TotalOrderPartitioner.java:296)
</pre><p>
      .. see the critical portion of the stack?  It's...
</p><pre class="programlisting">
       at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:210)
</pre><p>
       LocalJobRunner means the job is running locally, not on the cluster.
      </p><p>To solve this problem, you should run your MR job with your
      <code class="code">HADOOP_CLASSPATH</code> set to include the HBase dependencies.
      The "hbase classpath" utility can be used to do this easily.
      For example (substitute VERSION with your HBase version):
      </p><pre class="programlisting">
          HADOOP_CLASSPATH=`hbase classpath` hadoop jar $HBASE_HOME/hbase-VERSION.jar rowcounter usertable
      </pre><p>
      </p><p>See
      <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/package-summary.html#classpath" target="_top">
      http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/package-summary.html#classpath</a> for more
      information on HBase MapReduce jobs and classpaths.
      </p></div></div><div class="section" title="13.7.&nbsp;NameNode"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="trouble.namenode"></a>13.7.&nbsp;NameNode</h2></div></div></div><p>For more information on the NameNode, see <a class="xref" href="#arch.hdfs" title="9.9.&nbsp;HDFS">Section&nbsp;9.9, &#8220;HDFS&#8221;</a>.
       </p><div class="section" title="13.7.1.&nbsp;HDFS Utilization of Tables and Regions"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.namenode.disk"></a>13.7.1.&nbsp;HDFS Utilization of Tables and Regions</h3></div></div></div><p>To determine how much space HBase is using on HDFS use the <code class="code">hadoop</code> shell commands from the NameNode.  For example... </p><pre class="programlisting">hadoop fs -dus /hbase/</pre><p> ...returns the summarized disk utilization for all HBase objects.  </p><pre class="programlisting">hadoop fs -dus /hbase/myTable</pre><p> ...returns the summarized disk utilization for the HBase table 'myTable'. </p><pre class="programlisting">hadoop fs -du /hbase/myTable</pre><p> ...returns a list of the regions under the HBase table 'myTable' and their disk utilization. </p><p>For more information on HDFS shell commands, see the <a class="link" href="http://hadoop.apache.org/common/docs/current/file_system_shell.html" target="_top">HDFS FileSystem Shell documentation</a>.
            </p></div><div class="section" title="13.7.2.&nbsp;Browsing HDFS for HBase Objects"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.namenode.hbase.objects"></a>13.7.2.&nbsp;Browsing HDFS for HBase Objects</h3></div></div></div><p>Somtimes it will be necessary to explore the HBase objects that exist on HDFS.  These objects could include the WALs (Write Ahead Logs), tables, regions, StoreFiles, etc.
            The easiest way to do this is with the NameNode web application that runs on port 50070.  The NameNode web application will provide links to the all the DataNodes in the cluster so that
            they can be browsed seamlessly. </p><p>The HDFS directory structure of HBase tables in the cluster is...
            </p><pre class="programlisting">
<code class="filename">/hbase</code>
     <code class="filename">/&lt;Table&gt;</code>             (Tables in the cluster)
          <code class="filename">/&lt;Region&gt;</code>           (Regions for the table)
               <code class="filename">/&lt;ColumnFamiy&gt;</code>      (ColumnFamilies for the Region for the table)
                    <code class="filename">/&lt;StoreFile&gt;</code>        (StoreFiles for the ColumnFamily for the Regions for the table)
            </pre><p>
            </p><p>The HDFS directory structure of HBase WAL is..
            </p><pre class="programlisting">
<code class="filename">/hbase</code>
     <code class="filename">/.logs</code>
          <code class="filename">/&lt;RegionServer&gt;</code>    (RegionServers)
               <code class="filename">/&lt;HLog&gt;</code>           (WAL HLog files for the RegionServer)
            </pre><p>
            </p><p>See the <a class="link" href="see http://hadoop.apache.org/common/docs/current/hdfs_user_guide.html" target="_top">HDFS User Guide</a> for other non-shell diagnostic
		    utilities like <code class="code">fsck</code>.
            </p><div class="section" title="13.7.2.1.&nbsp;Zero size HLogs with data in them"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.namenode.0size.hlogs"></a>13.7.2.1.&nbsp;Zero size HLogs with data in them</h4></div></div></div><p>Problem: when getting a listing of all the files in a region server's .logs directory, one file has a size of 0 but it contains data.</p><p>Answer: It's an HDFS quirk. A file that's currently being to will appear to have a size of 0 but once it's closed it will show its true size</p></div><div class="section" title="13.7.2.2.&nbsp;Use Cases"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.namenode.uncompaction"></a>13.7.2.2.&nbsp;Use Cases</h4></div></div></div><p>Two common use-cases for querying HDFS for HBase objects is research the degree of uncompaction of a table.  If there are a large number of StoreFiles for each ColumnFamily it could
              indicate the need for a major compaction.  Additionally, after a major compaction if the resulting StoreFile is "small" it could indicate the need for a reduction of ColumnFamilies for
              the table.
		    </p></div></div></div><div class="section" title="13.8.&nbsp;Network"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="trouble.network"></a>13.8.&nbsp;Network</h2></div></div></div><div class="section" title="13.8.1.&nbsp;Network Spikes"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.network.spikes"></a>13.8.1.&nbsp;Network Spikes</h3></div></div></div><p>If you are seeing periodic network spikes you might want to check the <code class="code">compactionQueues</code> to see if major
        compactions are happening.
        </p><p>See <a class="xref" href="#managed.compactions" title="2.5.2.8.&nbsp;Managed Compactions">Section&nbsp;2.5.2.8, &#8220;Managed Compactions&#8221;</a> for more information on managing compactions.
        </p></div><div class="section" title="13.8.2.&nbsp;Loopback IP"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.network.loopback"></a>13.8.2.&nbsp;Loopback IP</h3></div></div></div><p>HBase expects the loopback IP Address to be 127.0.0.1.  See the Getting Started section on <a class="xref" href="#loopback.ip" title="2.1.2.3.&nbsp;Loopback IP">Section&nbsp;2.1.2.3, &#8220;Loopback IP&#8221;</a>.
        </p></div><div class="section" title="13.8.3.&nbsp;Network Interfaces"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.network.ints"></a>13.8.3.&nbsp;Network Interfaces</h3></div></div></div><p>Are all the network interfaces functioning correctly?  Are you sure?  See the Troubleshooting Case Study in <a class="xref" href="#trouble.casestudy" title="13.15.&nbsp;Case Studies">Section&nbsp;13.15, &#8220;Case Studies&#8221;</a>.
        </p></div></div><div class="section" title="13.9.&nbsp;RegionServer"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="trouble.rs"></a>13.9.&nbsp;RegionServer</h2></div></div></div><p>For more information on the RegionServers, see <a class="xref" href="#regionserver.arch" title="9.6.&nbsp;RegionServer">Section&nbsp;9.6, &#8220;RegionServer&#8221;</a>.
       </p><div class="section" title="13.9.1.&nbsp;Startup Errors"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.rs.startup"></a>13.9.1.&nbsp;Startup Errors</h3></div></div></div><div class="section" title="13.9.1.1.&nbsp;Master Starts, But RegionServers Do Not"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.rs.startup.master-no-region"></a>13.9.1.1.&nbsp;Master Starts, But RegionServers Do Not</h4></div></div></div><p>The Master believes the RegionServers have the IP of 127.0.0.1 - which is localhost and resolves to the master's own localhost.
            </p><p>The RegionServers are erroneously informing the Master that their IP addresses are 127.0.0.1.
            </p><p>Modify <code class="filename">/etc/hosts</code> on the region servers, from...
            </p><pre class="programlisting">
# Do not remove the following line, or various programs
# that require network functionality will fail.
127.0.0.1               fully.qualified.regionservername regionservername  localhost.localdomain localhost
::1             localhost6.localdomain6 localhost6
            </pre><p>
            ... to (removing the master node's name from localhost)...
            </p><pre class="programlisting">
# Do not remove the following line, or various programs
# that require network functionality will fail.
127.0.0.1               localhost.localdomain localhost
::1             localhost6.localdomain6 localhost6
            </pre><p>
            </p></div><div class="section" title="13.9.1.2.&nbsp;Compression Link Errors"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.rs.startup.compression"></a>13.9.1.2.&nbsp;Compression Link Errors</h4></div></div></div><p>
            Since compression algorithms such as LZO need to be installed and configured on each cluster this is a frequent source of startup error.  If you see messages like this...
            </p><pre class="programlisting">
11/02/20 01:32:15 ERROR lzo.GPLNativeCodeLoader: Could not load native gpl library
java.lang.UnsatisfiedLinkError: no gplcompression in java.library.path
        at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1734)
        at java.lang.Runtime.loadLibrary0(Runtime.java:823)
        at java.lang.System.loadLibrary(System.java:1028)
            </pre><p>
            .. then there is a path issue with the compression libraries.  See the Configuration section on <a class="link" href="#lzo.compression" title="C.3.&nbsp; LZO">LZO compression configuration</a>.
            </p></div></div><div class="section" title="13.9.2.&nbsp;Runtime Errors"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.rs.runtime"></a>13.9.2.&nbsp;Runtime Errors</h3></div></div></div><div class="section" title="13.9.2.1.&nbsp;RegionServer Hanging"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.rs.runtime.hang"></a>13.9.2.1.&nbsp;RegionServer Hanging</h4></div></div></div><p>
            Are you running an old JVM (&lt; 1.6.0_u21?)?  When you look at a thread dump,
            does it look like threads are BLOCKED but no one holds the lock all are
            blocked on?  See <a class="link" href="https://issues.apache.org/jira/browse/HBASE-3622" target="_top">HBASE 3622 Deadlock in HBaseServer (JVM bug?)</a>.
            Adding <code class="code">-XX:+UseMembar</code> to the HBase <code class="varname">HBASE_OPTS</code> in <code class="filename">conf/hbase-env.sh</code>
            may fix it.
            </p><p>Also, are you using <a class="xref" href="#client.rowlocks" title="9.3.4.&nbsp;RowLocks">Section&nbsp;9.3.4, &#8220;RowLocks&#8221;</a>?  These are discouraged because they can lock up the
            RegionServers if not managed properly.
            </p></div><div class="section" title="13.9.2.2.&nbsp;java.io.IOException...(Too many open files)"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.rs.runtime.filehandles"></a>13.9.2.2.&nbsp;java.io.IOException...(Too many open files)</h4></div></div></div><p>
           If you see log messages like this...
</p><pre class="programlisting">
2010-09-13 01:24:17,336 WARN org.apache.hadoop.hdfs.server.datanode.DataNode:
Disk-related IOException in BlockReceiver constructor. Cause is java.io.IOException: Too many open files
        at java.io.UnixFileSystem.createFileExclusively(Native Method)
        at java.io.File.createNewFile(File.java:883)
</pre><p>
           ... see the Getting Started section on <a class="link" href="#ulimit" title="2.1.2.5.&nbsp; ulimit and nproc">ulimit and nproc configuration</a>.
           </p></div><div class="section" title="13.9.2.3.&nbsp;xceiverCount 258 exceeds the limit of concurrent xcievers 256"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.rs.runtime.xceivers"></a>13.9.2.3.&nbsp;xceiverCount 258 exceeds the limit of concurrent xcievers 256</h4></div></div></div><p>
           This typically shows up in the DataNode logs.
           </p><p>
           See the Getting Started section on <a class="link" href="#dfs.datanode.max.xcievers" title="2.1.3.5.&nbsp;dfs.datanode.max.xcievers">xceivers configuration</a>.
           </p></div><div class="section" title="13.9.2.4.&nbsp;System instability, and the presence of &#34;java.lang.OutOfMemoryError: unable to create new native thread in exceptions&#34; HDFS DataNode logs or that of any system daemon"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.rs.runtime.oom-nt"></a>13.9.2.4.&nbsp;System instability, and the presence of "java.lang.OutOfMemoryError: unable to create new native thread in exceptions" HDFS DataNode logs or that of any system daemon</h4></div></div></div><p>
           See the Getting Started section on <a class="link" href="#ulimit" title="2.1.2.5.&nbsp; ulimit and nproc">ulimit and nproc configuration</a>.  The default on recent Linux
           distributions is 1024 - which is far too low for HBase.
           </p></div><div class="section" title="13.9.2.5.&nbsp;DFS instability and/or RegionServer lease timeouts"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.rs.runtime.gc"></a>13.9.2.5.&nbsp;DFS instability and/or RegionServer lease timeouts</h4></div></div></div><p>
           If you see warning messages like this...
           </p><pre class="programlisting">
2009-02-24 10:01:33,516 WARN org.apache.hadoop.hbase.util.Sleeper: We slept xxx ms, ten times longer than scheduled: 10000
2009-02-24 10:01:33,516 WARN org.apache.hadoop.hbase.util.Sleeper: We slept xxx ms, ten times longer than scheduled: 15000
2009-02-24 10:01:36,472 WARN org.apache.hadoop.hbase.regionserver.HRegionServer: unable to report to master for xxx milliseconds - retrying
           </pre><p>
           ... or see full GC compactions then you may be experiencing full GC's.
           </p></div><div class="section" title="13.9.2.6.&nbsp;&#34;No live nodes contain current block&#34; and/or YouAreDeadException"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.rs.runtime.nolivenodes"></a>13.9.2.6.&nbsp;"No live nodes contain current block" and/or YouAreDeadException</h4></div></div></div><p>
           These errors can happen either when running out of OS file handles or in periods of severe network problems where the nodes are unreachable.
           </p><p>
           See the Getting Started section on <a class="link" href="#ulimit" title="2.1.2.5.&nbsp; ulimit and nproc">ulimit and nproc configuration</a> and check your network.
           </p></div><div class="section" title="13.9.2.7.&nbsp;ZooKeeper SessionExpired events"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.rs.runtime.zkexpired"></a>13.9.2.7.&nbsp;ZooKeeper SessionExpired events</h4></div></div></div><p>Master or RegionServers shutting down with messages like those in the logs: </p><pre class="programlisting">
WARN org.apache.zookeeper.ClientCnxn: Exception
closing session 0x278bd16a96000f to sun.nio.ch.SelectionKeyImpl@355811ec
java.io.IOException: TIMED OUT
       at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:906)
WARN org.apache.hadoop.hbase.util.Sleeper: We slept 79410ms, ten times longer than scheduled: 5000
INFO org.apache.zookeeper.ClientCnxn: Attempting connection to server hostname/IP:PORT
INFO org.apache.zookeeper.ClientCnxn: Priming connection to java.nio.channels.SocketChannel[connected local=/IP:PORT remote=hostname/IP:PORT]
INFO org.apache.zookeeper.ClientCnxn: Server connection successful
WARN org.apache.zookeeper.ClientCnxn: Exception closing session 0x278bd16a96000d to sun.nio.ch.SelectionKeyImpl@3544d65e
java.io.IOException: Session Expired
       at org.apache.zookeeper.ClientCnxn$SendThread.readConnectResult(ClientCnxn.java:589)
       at org.apache.zookeeper.ClientCnxn$SendThread.doIO(ClientCnxn.java:709)
       at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:945)
ERROR org.apache.hadoop.hbase.regionserver.HRegionServer: ZooKeeper session expired
           </pre><p>
           The JVM is doing a long running garbage collecting which is pausing every threads (aka "stop the world").
           Since the RegionServer's local ZooKeeper client cannot send heartbeats, the session times out.
           By design, we shut down any node that isn't able to contact the ZooKeeper ensemble after getting a timeout so that it stops serving data that may already be assigned elsewhere.
           </p><p>
            </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Make sure you give plenty of RAM (in <code class="filename">hbase-env.sh</code>), the default of 1GB won't be able to sustain long running imports.</li><li class="listitem">Make sure you don't swap, the JVM never behaves well under swapping.</li><li class="listitem">Make sure you are not CPU starving the RegionServer thread. For example, if you are running a MapReduce job using 6 CPU-intensive tasks on a machine with 4 cores, you are probably starving the RegionServer enough to create longer garbage collection pauses.</li><li class="listitem">Increase the ZooKeeper session timeout</li></ul></div><p>
           If you wish to increase the session timeout, add the following to your <code class="filename">hbase-site.xml</code> to increase the timeout from the default of 60 seconds to 120 seconds.
           </p><pre class="programlisting">
&lt;property&gt;
    &lt;name&gt;zookeeper.session.timeout&lt;/name&gt;
    &lt;value&gt;1200000&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;hbase.zookeeper.property.tickTime&lt;/name&gt;
    &lt;value&gt;6000&lt;/value&gt;
&lt;/property&gt;
            </pre><p>
           </p><p>
           Be aware that setting a higher timeout means that the regions served by a failed RegionServer will take at least
           that amount of time to be transfered to another RegionServer. For a production system serving live requests, we would instead
           recommend setting it lower than 1 minute and over-provision your cluster in order the lower the memory load on each machines (hence having
           less garbage to collect per machine).
           </p><p>
           If this is happening during an upload which only happens once (like initially loading all your data into HBase), consider bulk loading.
           </p>
           See <a class="xref" href="#trouble.zookeeper.general" title="13.11.2.&nbsp;ZooKeeper, The Cluster Canary">Section&nbsp;13.11.2, &#8220;ZooKeeper, The Cluster Canary&#8221;</a> for other general information about ZooKeeper troubleshooting.
        </div><div class="section" title="13.9.2.8.&nbsp;NotServingRegionException"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.rs.runtime.notservingregion"></a>13.9.2.8.&nbsp;NotServingRegionException</h4></div></div></div><p>This exception is "normal" when found in the RegionServer logs at DEBUG level.  This exception is returned back to the client
           and then the client goes back to .META. to find the new location of the moved region.</p><p>However, if the NotServingRegionException is logged ERROR, then the client ran out of retries and something probably wrong.</p></div><div class="section" title="13.9.2.9.&nbsp;Regions listed by domain name, then IP"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.rs.runtime.double_listed_regions"></a>13.9.2.9.&nbsp;Regions listed by domain name, then IP</h4></div></div></div><p>
           Fix your DNS.  In versions of Apache HBase before 0.92.x, reverse DNS needs to give same answer
           as forward lookup. See <a class="link" href="https://issues.apache.org/jira/browse/HBASE-3431" target="_top">HBASE 3431
           RegionServer is not using the name given it by the master; double entry in master listing of servers</a> for gorey details.
          </p></div><div class="section" title="13.9.2.10.&nbsp;Logs flooded with '2011-01-10 12:40:48,407 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor' messages"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.rs.runtime.codecmsgs"></a>13.9.2.10.&nbsp;Logs flooded with '2011-01-10 12:40:48,407 INFO org.apache.hadoop.io.compress.CodecPool: Got
            brand-new compressor' messages</h4></div></div></div><p>We are not using the native versions of compression
                    libraries.  See <a class="link" href="https://issues.apache.org/jira/browse/HBASE-1900" target="_top">HBASE-1900 Put back native support when hadoop 0.21 is released</a>.
                    Copy the native libs from hadoop under hbase lib dir or
                    symlink them into place and the message should go away.
                </p></div><div class="section" title="13.9.2.11.&nbsp;Server handler X on 60020 caught: java.nio.channels.ClosedChannelException"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.rs.runtime.client_went_away"></a>13.9.2.11.&nbsp;Server handler X on 60020 caught: java.nio.channels.ClosedChannelException</h4></div></div></div><p>
           If you see this type of message it means that the region server was trying to read/send data from/to a client but
           it already went away. Typical causes for this are if the client was killed (you see a storm of messages like this when a MapReduce
           job is killed or fails) or if the client receives a SocketTimeoutException. It's harmless, but you should consider digging in
           a bit more if you aren't doing something to trigger them.
           </p></div></div><div class="section" title="13.9.3.&nbsp;Shutdown Errors"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.rs.shutdown"></a>13.9.3.&nbsp;Shutdown Errors</h3></div></div></div></div></div><div class="section" title="13.10.&nbsp;Master"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="trouble.master"></a>13.10.&nbsp;Master</h2></div></div></div><p>For more information on the Master, see <a class="xref" href="#master" title="9.5.&nbsp;Master">Section&nbsp;9.5, &#8220;Master&#8221;</a>.
       </p><div class="section" title="13.10.1.&nbsp;Startup Errors"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.master.startup"></a>13.10.1.&nbsp;Startup Errors</h3></div></div></div><div class="section" title="13.10.1.1.&nbsp;Master says that you need to run the hbase migrations script"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.master.startup.migration"></a>13.10.1.1.&nbsp;Master says that you need to run the hbase migrations script</h4></div></div></div><p>Upon running that, the hbase migrations script says no files in root directory.</p><p>HBase expects the root directory to either not exist, or to have already been initialized by hbase running a previous time. If you create a new directory for HBase using Hadoop DFS, this error will occur.
             Make sure the HBase root directory does not currently exist or has been initialized by a previous run of HBase. Sure fire solution is to just use Hadoop dfs to delete the HBase root and let HBase create and initialize the directory itself.
             </p></div><div class="section" title="13.10.1.2.&nbsp;Packet len6080218 is out of range!"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.master.startup.zk.buffer"></a>13.10.1.2.&nbsp;Packet len6080218 is out of range!</h4></div></div></div><p>If you have many regions on your cluster and you see an error
                  like that reported above in this sections title in your logs, see
                  <a class="link" href="https://issues.apache.org/jira/browse/HBASE-4246" target="_top">HBASE-4246 Cluster with too many regions cannot withstand some master failover scenarios</a>.</p></div></div><div class="section" title="13.10.2.&nbsp;Shutdown Errors"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.master.shutdown"></a>13.10.2.&nbsp;Shutdown Errors</h3></div></div></div></div></div><div class="section" title="13.11.&nbsp;ZooKeeper"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="trouble.zookeeper"></a>13.11.&nbsp;ZooKeeper</h2></div></div></div><div class="section" title="13.11.1.&nbsp;Startup Errors"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.zookeeper.startup"></a>13.11.1.&nbsp;Startup Errors</h3></div></div></div><div class="section" title="13.11.1.1.&nbsp;Could not find my address: xyz in list of ZooKeeper quorum servers"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.zookeeper.startup.address"></a>13.11.1.1.&nbsp;Could not find my address: xyz in list of ZooKeeper quorum servers</h4></div></div></div><p>A ZooKeeper server wasn't able to start, throws that error. xyz is the name of your server.</p><p>This is a name lookup problem. HBase tries to start a ZooKeeper server on some machine but that machine isn't able to find itself in the <code class="varname">hbase.zookeeper.quorum</code> configuration.
             </p><p>Use the hostname presented in the error message instead of the value you used. If you have a DNS server, you can set <code class="varname">hbase.zookeeper.dns.interface</code> and <code class="varname">hbase.zookeeper.dns.nameserver</code> in <code class="filename">hbase-site.xml</code> to make sure it resolves to the correct FQDN.
             </p></div></div><div class="section" title="13.11.2.&nbsp;ZooKeeper, The Cluster Canary"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.zookeeper.general"></a>13.11.2.&nbsp;ZooKeeper, The Cluster Canary</h3></div></div></div><p>ZooKeeper is the cluster's "canary in the mineshaft". It'll be the first to notice issues if any so making sure its happy is the short-cut to a humming cluster.
          </p><p>
          See the <a class="link" href="http://wiki.apache.org/hadoop/ZooKeeper/Troubleshooting" target="_top">ZooKeeper Operating Environment Troubleshooting</a> page. It has suggestions and tools for checking disk and networking performance; i.e. the operating environment your ZooKeeper and HBase are running in.
          </p><p>Additionally, the utility <a class="xref" href="#trouble.tools.builtin.zkcli" title="13.4.1.3.&nbsp;zkcli">Section&nbsp;13.4.1.3, &#8220;zkcli&#8221;</a> may help investigate ZooKeeper issues.
         </p></div></div><div class="section" title="13.12.&nbsp;Amazon EC2"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="trouble.ec2"></a>13.12.&nbsp;Amazon EC2</h2></div></div></div><div class="section" title="13.12.1.&nbsp;ZooKeeper does not seem to work on Amazon EC2"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.ec2.zookeeper"></a>13.12.1.&nbsp;ZooKeeper does not seem to work on Amazon EC2</h3></div></div></div><p>HBase does not start when deployed as Amazon EC2 instances.  Exceptions like the below appear in the Master and/or RegionServer logs: </p><pre class="programlisting">
  2009-10-19 11:52:27,030 INFO org.apache.zookeeper.ClientCnxn: Attempting
  connection to server ec2-174-129-15-236.compute-1.amazonaws.com/10.244.9.171:2181
  2009-10-19 11:52:27,032 WARN org.apache.zookeeper.ClientCnxn: Exception
  closing session 0x0 to sun.nio.ch.SelectionKeyImpl@656dc861
  java.net.ConnectException: Connection refused
             </pre><p>
             Security group policy is blocking the ZooKeeper port on a public address.
             Use the internal EC2 host names when configuring the ZooKeeper quorum peer list.
             </p></div><div class="section" title="13.12.2.&nbsp;Instability on Amazon EC2"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.ec2.instability"></a>13.12.2.&nbsp;Instability on Amazon EC2</h3></div></div></div><p>Questions on HBase and Amazon EC2 come up frequently on the HBase dist-list. Search for old threads using <a class="link" href="http://search-hadoop.com/" target="_top">Search Hadoop</a>
             </p></div><div class="section" title="13.12.3.&nbsp;Remote Java Connection into EC2 Cluster Not Working"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.ec2.connection"></a>13.12.3.&nbsp;Remote Java Connection into EC2 Cluster Not Working</h3></div></div></div><p>
             See Andrew's answer here, up on the user list: <a class="link" href="http://search-hadoop.com/m/sPdqNFAwyg2" target="_top">Remote Java client connection into EC2 instance</a>.
             </p></div></div><div class="section" title="13.13.&nbsp;HBase and Hadoop version issues"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="trouble.versions"></a>13.13.&nbsp;HBase and Hadoop version issues</h2></div></div></div><div class="section" title="13.13.1.&nbsp;NoClassDefFoundError when trying to run 0.90.x on hadoop-0.20.205.x (or hadoop-1.0.x)"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.versions.205"></a>13.13.1.&nbsp;<code class="code">NoClassDefFoundError</code> when trying to run 0.90.x on hadoop-0.20.205.x (or hadoop-1.0.x)</h3></div></div></div><p>Apache HBase 0.90.x does not ship with hadoop-0.20.205.x, etc.  To make it run, you need to replace the hadoop
             jars that Apache HBase shipped with in its <code class="filename">lib</code> directory with those of the Hadoop you want to
             run HBase on.  If even after replacing Hadoop jars you get the below exception:
</p><pre class="programlisting">
sv4r6s38: Exception in thread "main" java.lang.NoClassDefFoundError: org/apache/commons/configuration/Configuration
sv4r6s38:       at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.&lt;init&gt;(DefaultMetricsSystem.java:37)
sv4r6s38:       at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.&lt;clinit&gt;(DefaultMetricsSystem.java:34)
sv4r6s38:       at org.apache.hadoop.security.UgiInstrumentation.create(UgiInstrumentation.java:51)
sv4r6s38:       at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:209)
sv4r6s38:       at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:177)
sv4r6s38:       at org.apache.hadoop.security.UserGroupInformation.isSecurityEnabled(UserGroupInformation.java:229)
sv4r6s38:       at org.apache.hadoop.security.KerberosName.&lt;clinit&gt;(KerberosName.java:83)
sv4r6s38:       at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:202)
sv4r6s38:       at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:177)
</pre><p>
you need to copy under <code class="filename">hbase/lib</code>, the <code class="filename">commons-configuration-X.jar</code> you find
in your Hadoop's <code class="filename">lib</code> directory.  That should fix the above complaint.
</p></div><div class="section" title="13.13.2.&nbsp;...cannot communicate with client version..."><div class="titlepage"><div><div><h3 class="title"><a name="trouble.wrong.version"></a>13.13.2.&nbsp;...cannot communicate with client version...</h3></div></div></div><p>If you see something like the following in your logs
<code class="computeroutput">...
2012-09-24 10:20:52,168 FATAL org.apache.hadoop.hbase.master.HMaster: Unhandled exception. Starting shutdown.
org.apache.hadoop.ipc.RemoteException: Server IPC version 7 cannot communicate with client version 4
...</code>
...are you trying to talk to an Hadoop 2.0.x from an HBase that has an Hadoop 1.0.x client?
Use the HBase built against Hadoop 2.0 or rebuild your HBase passing the <span class="command"><strong>-Dhadoop.profile=2.0</strong></span>
attribute to Maven (See <a class="xref" href="#maven.build.hadoop" title="16.8.3.&nbsp;Building against various hadoop versions.">Section&nbsp;16.8.3, &#8220;Building against various hadoop versions.&#8221;</a> for more).
</p></div></div><div class="section" title="13.14.&nbsp;Running unit or integration tests"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="trouble.tests"></a>13.14.&nbsp;Running unit or integration tests</h2></div></div></div><div class="section" title="13.14.1.&nbsp;Runtime exceptions from MiniDFSCluster when running tests"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.HDFS-2556"></a>13.14.1.&nbsp;Runtime exceptions from MiniDFSCluster when running tests</h3></div></div></div><p>If you see something like the following
</p><pre class="programlisting">...
java.lang.NullPointerException: null
at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes
at org.apache.hadoop.hdfs.MiniDFSCluster.&lt;init&gt;
at org.apache.hadoop.hbase.MiniHBaseCluster.&lt;init&gt;
at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniDFSCluster
at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster
...</pre><p>
or
</p><pre class="programlisting">...
java.io.IOException: Shutting down
at org.apache.hadoop.hbase.MiniHBaseCluster.init
at org.apache.hadoop.hbase.MiniHBaseCluster.&lt;init&gt;
at org.apache.hadoop.hbase.MiniHBaseCluster.&lt;init&gt;
at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniHBaseCluster
at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster
...</pre><p>
... then try issuing the command <span class="command"><strong>umask 022</strong></span> before launching tests. This is a workaround for
<a class="link" href="https://issues.apache.org/jira/browse/HDFS-2556" target="_top">HDFS-2556</a>
</p></div></div><div class="section" title="13.15.&nbsp;Case Studies"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="trouble.casestudy"></a>13.15.&nbsp;Case Studies</h2></div></div></div><p>For Performance and Troubleshooting Case Studies, see <a class="xref" href="#casestudies" title="Chapter&nbsp;14.&nbsp;Apache HBase Case Studies">Chapter&nbsp;14, <i>Apache HBase Case Studies</i></a>.
      </p></div><div class="section" title="13.16.&nbsp;Cryptographic Features"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="trouble.crypto"></a>13.16.&nbsp;Cryptographic Features</h2></div></div></div><div class="section" title="13.16.1.&nbsp;sun.security.pkcs11.wrapper.PKCS11Exception: CKR_ARGUMENTS_BAD"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.crypto.HBASE-10132"></a>13.16.1.&nbsp;sun.security.pkcs11.wrapper.PKCS11Exception: CKR_ARGUMENTS_BAD</h3></div></div></div><p>This problem manifests as exceptions ultimately caused by:</p><pre class="programlisting">
Caused by: sun.security.pkcs11.wrapper.PKCS11Exception: CKR_ARGUMENTS_BAD
	at sun.security.pkcs11.wrapper.PKCS11.C_DecryptUpdate(Native Method)
	at sun.security.pkcs11.P11Cipher.implDoFinal(P11Cipher.java:795)
</pre><p>
This problem appears to affect some versions of OpenJDK 7 shipped by some Linux vendors. NSS is configured as the default provider. If the host has an x86_64 architecture, depending on if the vendor packages contain the defect, the NSS provider will not function correctly.
</p><p>
To work around this problem, find the JRE home directory and edit the file <code class="filename">lib/security/java.security</code>. Edit the file to comment out the line:
</p><pre class="programlisting">
security.provider.1=sun.security.pkcs11.SunPKCS11 ${java.home}/lib/security/nss.cfg
</pre><p>
Then renumber the remaining providers accordingly.
</p></div></div><div class="footnotes"><br><hr width="100" align="left"><div class="footnote"><p><sup>[<a id="ftn.d366e9696" href="#d366e9696" class="para">32</a>] </sup>See Getting Answers</p></div></div></div><div class="chapter" title="Chapter&nbsp;14.&nbsp;Apache HBase Case Studies"><div class="titlepage"><div><div><h2 class="title"><a name="casestudies"></a>Chapter&nbsp;14.&nbsp;Apache HBase Case Studies</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#casestudies.overview">14.1. Overview</a></span></dt><dt><span class="section"><a href="#casestudies.schema">14.2. Schema Design</a></span></dt><dt><span class="section"><a href="#casestudies.perftroub">14.3. Performance/Troubleshooting</a></span></dt><dd><dl><dt><span class="section"><a href="#casestudies.slownode">14.3.1. Case Study #1 (Performance Issue On A Single Node)</a></span></dt><dt><span class="section"><a href="#casestudies.perf.1">14.3.2. Case Study #2 (Performance Research 2012)</a></span></dt><dt><span class="section"><a href="#casestudies.perf.2">14.3.3. Case Study #3 (Performance Research 2010))</a></span></dt><dt><span class="section"><a href="#casestudies.xceivers">14.3.4. Case Study #4 (xcievers Config)</a></span></dt></dl></dd></dl></div><div class="section" title="14.1.&nbsp;Overview"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="casestudies.overview"></a>14.1.&nbsp;Overview</h2></div></div></div><p>This chapter will describe a variety of performance and troubleshooting case studies that can 
      provide a useful blueprint on diagnosing Apache HBase cluster issues.</p><p>For more information on Performance and Troubleshooting, see <a class="xref" href="#performance" title="Chapter&nbsp;12.&nbsp;Apache HBase Performance Tuning">Chapter&nbsp;12, <i>Apache HBase Performance Tuning</i></a> and <a class="xref" href="#trouble" title="Chapter&nbsp;13.&nbsp;Troubleshooting and Debugging Apache HBase">Chapter&nbsp;13, <i>Troubleshooting and Debugging Apache HBase</i></a>.
      </p></div><div class="section" title="14.2.&nbsp;Schema Design"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="casestudies.schema"></a>14.2.&nbsp;Schema Design</h2></div></div></div><p>See the schema design case studies here: <a class="xref" href="#schema.casestudies" title="6.11.&nbsp;Schema Design Case Studies">Section&nbsp;6.11, &#8220;Schema Design Case Studies&#8221;</a>
    	</p></div><div class="section" title="14.3.&nbsp;Performance/Troubleshooting"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="casestudies.perftroub"></a>14.3.&nbsp;Performance/Troubleshooting</h2></div></div></div><div class="section" title="14.3.1.&nbsp;Case Study #1 (Performance Issue On A Single Node)"><div class="titlepage"><div><div><h3 class="title"><a name="casestudies.slownode"></a>14.3.1.&nbsp;Case Study #1 (Performance Issue On A Single Node)</h3></div></div></div><div class="section" title="14.3.1.1.&nbsp;Scenario"><div class="titlepage"><div><div><h4 class="title"><a name="d366e10587"></a>14.3.1.1.&nbsp;Scenario</h4></div></div></div><p>Following a scheduled reboot, one data node began exhibiting unusual behavior.  Routine MapReduce 
         jobs run against HBase tables which regularly completed in five or six minutes began taking 30 or 40 minutes 
         to finish. These jobs were consistently found to be waiting on map and reduce tasks assigned to the troubled data node 
         (e.g., the slow map tasks all had the same Input Split).           
         The situation came to a head during a distributed copy, when the copy was severely prolonged by the lagging node.
		</p></div><div class="section" title="14.3.1.2.&nbsp;Hardware"><div class="titlepage"><div><div><h4 class="title"><a name="d366e10592"></a>14.3.1.2.&nbsp;Hardware</h4></div></div></div><p>Datanodes:
        </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Two 12-core processors</li><li class="listitem">Six Enerprise SATA disks</li><li class="listitem">24GB of RAM</li><li class="listitem">Two bonded gigabit NICs</li></ul></div><p>
        </p><p>Network:
        </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">10 Gigabit top-of-rack switches</li><li class="listitem">20 Gigabit bonded interconnects between racks.</li></ul></div><p>
        </p></div><div class="section" title="14.3.1.3.&nbsp;Hypotheses"><div class="titlepage"><div><div><h4 class="title"><a name="d366e10615"></a>14.3.1.3.&nbsp;Hypotheses</h4></div></div></div><div class="section" title="14.3.1.3.1.&nbsp;HBase &#34;Hot Spot&#34; Region"><div class="titlepage"><div><div><h5 class="title"><a name="d366e10618"></a>14.3.1.3.1.&nbsp;HBase "Hot Spot" Region</h5></div></div></div><p>We hypothesized that we were experiencing a familiar point of pain: a "hot spot" region in an HBase table, 
		  where uneven key-space distribution can funnel a huge number of requests to a single HBase region, bombarding the RegionServer 
		  process and cause slow response time. Examination of the HBase Master status page showed that the number of HBase requests to the 
		  troubled node was almost zero.  Further, examination of the HBase logs showed that there were no region splits, compactions, or other region transitions 
		  in progress.  This effectively ruled out a "hot spot" as the root cause of the observed slowness.
          </p></div><div class="section" title="14.3.1.3.2.&nbsp;HBase Region With Non-Local Data"><div class="titlepage"><div><div><h5 class="title"><a name="d366e10623"></a>14.3.1.3.2.&nbsp;HBase Region With Non-Local Data</h5></div></div></div><p>Our next hypothesis was that one of the MapReduce tasks was requesting data from HBase that was not local to the datanode, thus 
		  forcing HDFS to request data blocks from other servers over the network.  Examination of the datanode logs showed that there were very 
		  few blocks being requested over the network, indicating that the HBase region was correctly assigned, and that the majority of the necessary 
		  data was located on the node. This ruled out the possibility of non-local data causing a slowdown.
          </p></div><div class="section" title="14.3.1.3.3.&nbsp;Excessive I/O Wait Due To Swapping Or An Over-Worked Or Failing Hard Disk"><div class="titlepage"><div><div><h5 class="title"><a name="d366e10628"></a>14.3.1.3.3.&nbsp;Excessive I/O Wait Due To Swapping Or An Over-Worked Or Failing Hard Disk</h5></div></div></div><p>After concluding that the Hadoop and HBase were not likely to be the culprits, we moved on to troubleshooting the datanode's hardware. 
          Java, by design, will periodically scan its entire memory space to do garbage collection.  If system memory is heavily overcommitted, the Linux 
          kernel may enter a vicious cycle, using up all of its resources swapping Java heap back and forth from disk to RAM as Java tries to run garbage 
          collection.  Further, a failing hard disk will often retry reads and/or writes many times before giving up and returning an error. This can manifest 
          as high iowait, as running processes wait for reads and writes to complete.  Finally, a disk nearing the upper edge of its performance envelope will 
          begin to cause iowait as it informs the kernel that it cannot accept any more data, and the kernel queues incoming data into the dirty write pool in memory.  
          However, using <code class="code">vmstat(1)</code> and <code class="code">free(1)</code>, we could see that no swap was being used, and the amount of disk IO was only a few kilobytes per second.
          </p></div><div class="section" title="14.3.1.3.4.&nbsp;Slowness Due To High Processor Usage"><div class="titlepage"><div><div><h5 class="title"><a name="d366e10639"></a>14.3.1.3.4.&nbsp;Slowness Due To High Processor Usage</h5></div></div></div><p>Next, we checked to see whether the system was performing slowly simply due to very high computational load.  <code class="code">top(1)</code> showed that the system load 
          was higher than normal, but <code class="code">vmstat(1)</code> and <code class="code">mpstat(1)</code> showed that the amount of processor being used for actual computation was low.
          </p></div><div class="section" title="14.3.1.3.5.&nbsp;Network Saturation (The Winner)"><div class="titlepage"><div><div><h5 class="title"><a name="d366e10653"></a>14.3.1.3.5.&nbsp;Network Saturation (The Winner)</h5></div></div></div><p>Since neither the disks nor the processors were being utilized heavily, we moved on to the performance of the network interfaces.  The datanode had two 
          gigabit ethernet adapters, bonded to form an active-standby interface.  <code class="code">ifconfig(8)</code> showed some unusual anomalies, namely interface errors, overruns, framing errors. 
          While not unheard of, these kinds of errors are exceedingly rare on modern hardware which is operating as it should:
</p><pre class="programlisting">		
$ /sbin/ifconfig bond0
bond0  Link encap:Ethernet  HWaddr 00:00:00:00:00:00  
inet addr:10.x.x.x  Bcast:10.x.x.255  Mask:255.255.255.0
UP BROADCAST RUNNING MASTER MULTICAST  MTU:1500  Metric:1
RX packets:2990700159 errors:12 dropped:0 overruns:1 frame:6          &lt;--- Look Here! Errors!
TX packets:3443518196 errors:0 dropped:0 overruns:0 carrier:0
collisions:0 txqueuelen:0 
RX bytes:2416328868676 (2.4 TB)  TX bytes:3464991094001 (3.4 TB)
</pre><p>
          </p><p>These errors immediately lead us to suspect that one or more of the ethernet interfaces might have negotiated the wrong line speed.  This was confirmed both by running an ICMP ping 
          from an external host and observing round-trip-time in excess of 700ms, and by running <code class="code">ethtool(8)</code> on the members of the bond interface and discovering that the active interface 
          was operating at 100Mbs/, full duplex.
</p><pre class="programlisting">		
$ sudo ethtool eth0
Settings for eth0:
Supported ports: [ TP ]
Supported link modes:   10baseT/Half 10baseT/Full 
                       100baseT/Half 100baseT/Full 
                       1000baseT/Full 
Supports auto-negotiation: Yes
Advertised link modes:  10baseT/Half 10baseT/Full 
                       100baseT/Half 100baseT/Full 
                       1000baseT/Full 
Advertised pause frame use: No
Advertised auto-negotiation: Yes
Link partner advertised link modes:  Not reported
Link partner advertised pause frame use: No
Link partner advertised auto-negotiation: No
Speed: 100Mb/s                                     &lt;--- Look Here!  Should say 1000Mb/s!
Duplex: Full
Port: Twisted Pair
PHYAD: 1
Transceiver: internal
Auto-negotiation: on
MDI-X: Unknown
Supports Wake-on: umbg
Wake-on: g
Current message level: 0x00000003 (3)
Link detected: yes
</pre><p>		
		  </p><p>In normal operation, the ICMP ping round trip time should be around 20ms, and the interface speed and duplex should read, "1000MB/s", and, "Full", respectively.  
		  </p></div></div><div class="section" title="14.3.1.4.&nbsp;Resolution"><div class="titlepage"><div><div><h4 class="title"><a name="d366e10674"></a>14.3.1.4.&nbsp;Resolution</h4></div></div></div><p>After determining that the active ethernet adapter was at the incorrect speed, we used the <code class="code">ifenslave(8)</code> command to make the standby interface 
   	  the active interface, which yielded an immediate improvement in MapReduce performance, and a 10 times improvement in network throughput:
	  </p><p>On the next trip to the datacenter, we determined that the line speed issue was ultimately caused by a bad network cable, which was replaced.
	  </p></div></div><div class="section" title="14.3.2.&nbsp;Case Study #2 (Performance Research 2012)"><div class="titlepage"><div><div><h3 class="title"><a name="casestudies.perf.1"></a>14.3.2.&nbsp;Case Study #2 (Performance Research 2012)</h3></div></div></div><p>Investigation results of a self-described "we're not sure what's wrong, but it seems slow" problem. 
      <a class="link" href="http://gbif.blogspot.com/2012/03/hbase-performance-evaluation-continued.html" target="_top">http://gbif.blogspot.com/2012/03/hbase-performance-evaluation-continued.html</a>
      </p></div><div class="section" title="14.3.3.&nbsp;Case Study #3 (Performance Research 2010))"><div class="titlepage"><div><div><h3 class="title"><a name="casestudies.perf.2"></a>14.3.3.&nbsp;Case Study #3 (Performance Research 2010))</h3></div></div></div><p>
      Investigation results of general cluster performance from 2010.  Although this research is on an older version of the codebase, this writeup
      is still very useful in terms of approach.
      <a class="link" href="http://hstack.org/hbase-performance-testing/" target="_top">http://hstack.org/hbase-performance-testing/</a>
      </p></div><div class="section" title="14.3.4.&nbsp;Case Study #4 (xcievers Config)"><div class="titlepage"><div><div><h3 class="title"><a name="casestudies.xceivers"></a>14.3.4.&nbsp;Case Study #4 (xcievers Config)</h3></div></div></div><p>Case study of configuring <code class="code">xceivers</code>, and diagnosing errors from mis-configurations.
      <a class="link" href="http://www.larsgeorge.com/2012/03/hadoop-hbase-and-xceivers.html" target="_top">http://www.larsgeorge.com/2012/03/hadoop-hbase-and-xceivers.html</a>
      </p><p>See also <a class="xref" href="#dfs.datanode.max.xcievers" title="2.1.3.5.&nbsp;dfs.datanode.max.xcievers">Section&nbsp;2.1.3.5, &#8220;<code class="varname">dfs.datanode.max.xcievers</code>&#8221;</a>.
      </p></div></div></div><div class="chapter" title="Chapter&nbsp;15.&nbsp;Apache HBase Operational Management"><div class="titlepage"><div><div><h2 class="title"><a name="ops_mgt"></a>Chapter&nbsp;15.&nbsp;Apache HBase Operational Management</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#tools">15.1. HBase Tools and Utilities</a></span></dt><dd><dl><dt><span class="section"><a href="#canary">15.1.1. Canary</a></span></dt><dt><span class="section"><a href="#health.check">15.1.2. Health Checker</a></span></dt><dt><span class="section"><a href="#driver">15.1.3. Driver</a></span></dt><dt><span class="section"><a href="#hbck">15.1.4. HBase <span class="application">hbck</span></a></span></dt><dt><span class="section"><a href="#hfile_tool2">15.1.5. HFile Tool</a></span></dt><dt><span class="section"><a href="#wal_tools">15.1.6. WAL Tools</a></span></dt><dt><span class="section"><a href="#compression.tool">15.1.7. Compression Tool</a></span></dt><dt><span class="section"><a href="#copytable">15.1.8. CopyTable</a></span></dt><dt><span class="section"><a href="#export">15.1.9. Export</a></span></dt><dt><span class="section"><a href="#import">15.1.10. Import</a></span></dt><dt><span class="section"><a href="#importtsv">15.1.11. ImportTsv</a></span></dt><dt><span class="section"><a href="#completebulkload">15.1.12. CompleteBulkLoad</a></span></dt><dt><span class="section"><a href="#walplayer">15.1.13. WALPlayer</a></span></dt><dt><span class="section"><a href="#rowcounter">15.1.14. RowCounter and CellCounter</a></span></dt><dt><span class="section"><a href="#mlockall">15.1.15. mlockall</a></span></dt><dt><span class="section"><a href="#compaction.tool">15.1.16. Offline Compaction Tool</a></span></dt></dl></dd><dt><span class="section"><a href="#ops.regionmgt">15.2. Region Management</a></span></dt><dd><dl><dt><span class="section"><a href="#ops.regionmgt.majorcompact">15.2.1. Major Compaction</a></span></dt><dt><span class="section"><a href="#ops.regionmgt.merge">15.2.2. Merge</a></span></dt></dl></dd><dt><span class="section"><a href="#node.management">15.3. Node Management</a></span></dt><dd><dl><dt><span class="section"><a href="#decommission">15.3.1. Node Decommission</a></span></dt><dt><span class="section"><a href="#rolling">15.3.2. Rolling Restart</a></span></dt><dt><span class="section"><a href="#adding.new.node">15.3.3. Adding a New Node</a></span></dt></dl></dd><dt><span class="section"><a href="#hbase_metrics">15.4. HBase Metrics</a></span></dt><dd><dl><dt><span class="section"><a href="#metric_setup">15.4.1. Metric Setup</a></span></dt><dt><span class="section"><a href="#rs_metrics_ganglia">15.4.2. Warning To Ganglia Users</a></span></dt><dt><span class="section"><a href="#rs_metrics">15.4.3. Most Important RegionServer Metrics</a></span></dt><dt><span class="section"><a href="#rs_metrics_other">15.4.4. Other RegionServer Metrics</a></span></dt></dl></dd><dt><span class="section"><a href="#ops.monitoring">15.5. HBase Monitoring</a></span></dt><dd><dl><dt><span class="section"><a href="#ops.monitoring.overview">15.5.1. Overview</a></span></dt><dt><span class="section"><a href="#ops.slow.query">15.5.2. Slow Query Log</a></span></dt></dl></dd><dt><span class="section"><a href="#cluster_replication">15.6. Cluster Replication</a></span></dt><dt><span class="section"><a href="#ops.backup">15.7. HBase Backup</a></span></dt><dd><dl><dt><span class="section"><a href="#ops.backup.fullshutdown">15.7.1. Full Shutdown Backup</a></span></dt><dt><span class="section"><a href="#ops.backup.live.replication">15.7.2. Live Cluster Backup - Replication</a></span></dt><dt><span class="section"><a href="#ops.backup.live.copytable">15.7.3. Live Cluster Backup - CopyTable</a></span></dt><dt><span class="section"><a href="#ops.backup.live.export">15.7.4. Live Cluster Backup - Export</a></span></dt></dl></dd><dt><span class="section"><a href="#ops.snapshots">15.8. HBase Snapshots</a></span></dt><dd><dl><dt><span class="section"><a href="#ops.snapshots.configuration">15.8.1. Configuration</a></span></dt><dt><span class="section"><a href="#ops.snapshots.takeasnapshot">15.8.2. Take a Snapshot</a></span></dt><dt><span class="section"><a href="#ops.snapshots.list">15.8.3. Listing Snapshots</a></span></dt><dt><span class="section"><a href="#ops.snapshots.delete">15.8.4. Deleting Snapshots</a></span></dt><dt><span class="section"><a href="#ops.snapshots.clone">15.8.5. Clone a table from snapshot</a></span></dt><dt><span class="section"><a href="#ops.snapshots.restore">15.8.6. Restore a snapshot</a></span></dt><dt><span class="section"><a href="#ops.snapshots.acls">15.8.7. Snapshots operations and ACLs</a></span></dt><dt><span class="section"><a href="#ops.snapshots.export">15.8.8. Export to another cluster</a></span></dt></dl></dd><dt><span class="section"><a href="#ops.capacity">15.9. Capacity Planning and Region Sizing</a></span></dt><dd><dl><dt><span class="section"><a href="#ops.capacity.nodes">15.9.1. Node count and hardware/VM configuration</a></span></dt><dt><span class="section"><a href="#ops.capacity.regions">15.9.2. Determining region count and size</a></span></dt><dt><span class="section"><a href="#ops.capacity.config">15.9.3. Initial configuration and tuning</a></span></dt></dl></dd><dt><span class="section"><a href="#table.rename">15.10. Table Rename</a></span></dt></dl></div>
  This chapter will cover operational tools and practices required of a running Apache HBase cluster.
  The subject of operations is related to the topics of <a class="xref" href="#trouble" title="Chapter&nbsp;13.&nbsp;Troubleshooting and Debugging Apache HBase">Chapter&nbsp;13, <i>Troubleshooting and Debugging Apache HBase</i></a>, <a class="xref" href="#performance" title="Chapter&nbsp;12.&nbsp;Apache HBase Performance Tuning">Chapter&nbsp;12, <i>Apache HBase Performance Tuning</i></a>,
  and <a class="xref" href="#configuration" title="Chapter&nbsp;2.&nbsp;Apache HBase Configuration">Chapter&nbsp;2, <i>Apache HBase Configuration</i></a> but is a distinct topic in itself.

  <div class="section" title="15.1.&nbsp;HBase Tools and Utilities"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="tools"></a>15.1.&nbsp;HBase Tools and Utilities</h2></div></div></div><p>Here we list HBase tools for administration, analysis, fixup, and debugging.</p><div class="section" title="15.1.1.&nbsp;Canary"><div class="titlepage"><div><div><h3 class="title"><a name="canary"></a>15.1.1.&nbsp;Canary</h3></div></div></div><p>There is a Canary class can help users to canary-test the HBase cluster status, with every column-family for every regions or regionservers granularity. To see the usage,
</p><pre class="programlisting">$ ${HBASE_HOME}/bin/hbase org.apache.hadoop.hbase.tool.Canary -help</pre><p>
Will output
</p><pre class="programlisting">Usage: bin/hbase org.apache.hadoop.hbase.tool.Canary [opts] [table1 [table2]...] | [regionserver1 [regionserver2]..]
 where [opts] are:
   -help          Show this help and exit.
   -regionserver  replace the table argument to regionserver,
      which means to enable regionserver mode
   -daemon        Continuous check at defined intervals.
   -interval &lt;N&gt;  Interval between checks (sec)
   -e             Use region/regionserver as regular expression
      which means the region/regionserver is regular expression pattern
   -f &lt;B&gt;         stop whole program if first error occurs, default is true
   -t &lt;N&gt;         timeout for a check, default is 600000 (milisecs)</pre><p>
This tool will return non zero error codes to user for collaborating with other monitoring tools, such as Nagios.
The error code definitions are...
</p><pre class="programlisting">private static final int USAGE_EXIT_CODE = 1;
private static final int INIT_ERROR_EXIT_CODE = 2;
private static final int TIMEOUT_ERROR_EXIT_CODE = 3;
private static final int ERROR_EXIT_CODE = 4;</pre><p>
Here are some examples based on the following given case. There are two HTable called test-01 and test-02, they have two column family cf1 and cf2 respectively, and deployed on the 3 regionservers. see following table.
	     </p><div class="table"><a name="d366e10747"></a><p class="title"><b>Table&nbsp;15.1.&nbsp;</b></p><div class="table-contents"><table border="1"><colgroup><col align="center" class="regionserver"><col align="center" class="test-01"><col align="center" class="test-02"></colgroup><thead><tr><th align="center">RegionServer</th><th align="center">test-01</th><th align="center">test-02</th></tr></thead><tbody><tr><td align="center">rs1</td><td align="center">r1</td><td align="center">r2</td></tr><tr><td align="center">rs2</td><td align="center">r2</td><td align="center">&nbsp;</td></tr><tr><td align="center">rs3</td><td align="center">r2</td><td align="center">r1</td></tr></tbody></table></div></div><p><br class="table-break">
Following are some examples based on the previous given case.
</p><div class="section" title="15.1.1.1.&nbsp;Canary test for every column family (store) of every region of every table"><div class="titlepage"><div><div><h4 class="title"><a name="d366e10782"></a>15.1.1.1.&nbsp;Canary test for every column family (store) of every region of every table</h4></div></div></div><p>
</p><pre class="programlisting">$ ${HBASE_HOME}/bin/hbase org.apache.hadoop.hbase.tool.Canary</pre><p>
The output log is...
</p><pre class="programlisting">13/12/09 03:26:32 INFO tool.Canary: read from region test-01,,1386230156732.0e3c7d77ffb6361ea1b996ac1042ca9a. column family cf1 in 2ms
13/12/09 03:26:32 INFO tool.Canary: read from region test-01,,1386230156732.0e3c7d77ffb6361ea1b996ac1042ca9a. column family cf2 in 2ms
13/12/09 03:26:32 INFO tool.Canary: read from region test-01,0004883,1386230156732.87b55e03dfeade00f441125159f8ca87. column family cf1 in 4ms
13/12/09 03:26:32 INFO tool.Canary: read from region test-01,0004883,1386230156732.87b55e03dfeade00f441125159f8ca87. column family cf2 in 1ms
...
13/12/09 03:26:32 INFO tool.Canary: read from region test-02,,1386559511167.aa2951a86289281beee480f107bb36ee. column family cf1 in 5ms
13/12/09 03:26:32 INFO tool.Canary: read from region test-02,,1386559511167.aa2951a86289281beee480f107bb36ee. column family cf2 in 3ms
13/12/09 03:26:32 INFO tool.Canary: read from region test-02,0004883,1386559511167.cbda32d5e2e276520712d84eaaa29d84. column family cf1 in 31ms
13/12/09 03:26:32 INFO tool.Canary: read from region test-02,0004883,1386559511167.cbda32d5e2e276520712d84eaaa29d84. column family cf2 in 8ms
</pre><p>
So you can see, table test-01 has two regions and two column families, so the Canary tool will pick 4 small piece of data from 4 (2 region * 2 store) different stores. This is a default behavior of the this tool does.
</p></div><div class="section" title="15.1.1.2.&nbsp;Canary test for every column family (store) of every region of specific table(s)"><div class="titlepage"><div><div><h4 class="title"><a name="d366e10793"></a>15.1.1.2.&nbsp;Canary test for every column family (store) of every region of specific table(s)</h4></div></div></div><p>
You can also test one or more specific tables.
</p><pre class="programlisting">$ ${HBASE_HOME}/bin/hbase orghapache.hadoop.hbase.tool.Canary test-01 test-02</pre><p>
</p></div><div class="section" title="15.1.1.3.&nbsp;Canary test with regionserver granularity"><div class="titlepage"><div><div><h4 class="title"><a name="d366e10801"></a>15.1.1.3.&nbsp;Canary test with regionserver granularity</h4></div></div></div><p>
This will pick one small piece of data from each regionserver, and can also put your resionserver name as input options for canary-test specific regionservers.
</p><pre class="programlisting">$ ${HBASE_HOME}/bin/hbase org.apache.hadoop.hbase.tool.Canary -regionserver</pre><p>
The output log is...
</p><pre class="programlisting">13/12/09 06:05:17 INFO tool.Canary: Read from table:test-01 on region server:rs2 in 72ms
13/12/09 06:05:17 INFO tool.Canary: Read from table:test-02 on region server:rs3 in 34ms
13/12/09 06:05:17 INFO tool.Canary: Read from table:test-01 on region server:rs1 in 56ms</pre><p>
</p></div><div class="section" title="15.1.1.4.&nbsp;Canary test with regular expression pattern"><div class="titlepage"><div><div><h4 class="title"><a name="d366e10812"></a>15.1.1.4.&nbsp;Canary test with regular expression pattern</h4></div></div></div><p>
This will test both table test-01 and test-02.
</p><pre class="programlisting">$ ${HBASE_HOME}/bin/hbase orghapache.hadoop.hbase.tool.Canary -e test-0[1-2]</pre><p>
</p></div><div class="section" title="15.1.1.5.&nbsp;Run canary test as daemon mode"><div class="titlepage"><div><div><h4 class="title"><a name="d366e10820"></a>15.1.1.5.&nbsp;Run canary test as daemon mode</h4></div></div></div><p>
Run repeatedly with interval defined in option -interval whose default value is 6 seconds. This daemon will stop itself and return non-zero error code if any error occurs, due to the default value of option -f is true.
</p><pre class="programlisting">$ ${HBASE_HOME}/bin/hbase orghapache.hadoop.hbase.tool.Canary -daemon</pre><p>
Run repeatedly with internal 5 seconds and will not stop itself even error occurs in the test.
</p><pre class="programlisting">$ ${HBASE_HOME}/bin/hbase orghapache.hadoop.hbase.tool.Canary -daemon -interval 50000 -f false</pre><p>
</p></div><div class="section" title="15.1.1.6.&nbsp;Force timeout if canary test stuck"><div class="titlepage"><div><div><h4 class="title"><a name="d366e10831"></a>15.1.1.6.&nbsp;Force timeout if canary test stuck</h4></div></div></div><p>In some cases, we suffered the request stucked on the regionserver and not response back to the client. The regionserver in problem, would also not indicated to be dead by Master, which would bring the clients hung. So we provide the timeout option to kill the canary test forcefully and return non-zero error code as well.
This run sets the timeout value to 60 seconds, the default value is 600 seconds.
</p><pre class="programlisting">$ ${HBASE_HOME}/bin/hbase orghapache.hadoop.hbase.tool.Canary -t 600000</pre><p>
</p></div></div><div class="section" title="15.1.2.&nbsp;Health Checker"><div class="titlepage"><div><div><h3 class="title"><a name="health.check"></a>15.1.2.&nbsp;Health Checker</h3></div></div></div><p>You can configure HBase to run a script on a period and if it fails N times (configurable), have the server exit.
            See HBASE-7351 Periodic health check script for configurations and detail.
        </p></div><div class="section" title="15.1.3.&nbsp;Driver"><div class="titlepage"><div><div><h3 class="title"><a name="driver"></a>15.1.3.&nbsp;Driver</h3></div></div></div><p>There is a <code class="code">Driver</code> class that is executed by the HBase jar can be used to invoke frequently accessed utilities.  For example,
</p><pre class="programlisting">HADOOP_CLASSPATH=`${HBASE_HOME}/bin/hbase classpath` ${HADOOP_HOME}/bin/hadoop jar ${HBASE_HOME}/hbase-VERSION.jar
</pre><p>
... will return...
</p><pre class="programlisting">
An example program must be given as the first argument.
Valid program names are:
  completebulkload: Complete a bulk data load.
  copytable: Export a table from local cluster to peer cluster
  export: Write table data to HDFS.
  import: Import data written by Export.
  importtsv: Import data in TSV format.
  rowcounter: Count rows in HBase table
  verifyrep: Compare the data from tables in two different clusters. WARNING: It doesn't work for incrementColumnValues'd cells since the timestamp is chan
</pre><p>
... for allowable program names.
      </p></div><div class="section" title="15.1.4.&nbsp;HBase hbck"><div class="titlepage"><div><div><h3 class="title"><a name="hbck"></a>15.1.4.&nbsp;HBase <span class="application">hbck</span></h3></div><div><h4 class="subtitle">An <span class="emphasis"><em>fsck</em></span> for your HBase install</h4></div></div></div><p>To run <span class="application">hbck</span> against your HBase cluster run
        </p><pre class="programlisting">$ ./bin/hbase hbck</pre><p>
        At the end of the commands output it prints <span class="emphasis"><em>OK</em></span>
        or <span class="emphasis"><em>INCONSISTENCY</em></span>. If your cluster reports
        inconsistencies, pass <span class="command"><strong>-details</strong></span> to see more detail emitted.
        If inconsistencies, run <span class="command"><strong>hbck</strong></span> a few times because the
        inconsistency may be transient (e.g. cluster is starting up or a region is
        splitting).
        Passing <span class="command"><strong>-fix</strong></span> may correct the inconsistency (This latter
        is an experimental feature).
        </p><p>For more information, see <a class="xref" href="#hbck.in.depth" title="Appendix&nbsp;B.&nbsp;hbck In Depth">Appendix&nbsp;B, <i>hbck In Depth</i></a>.
        </p></div><div class="section" title="15.1.5.&nbsp;HFile Tool"><div class="titlepage"><div><div><h3 class="title"><a name="hfile_tool2"></a>15.1.5.&nbsp;HFile Tool</h3></div></div></div><p>See <a class="xref" href="#hfile_tool" title="9.7.6.2.2.&nbsp;HFile Tool">Section&nbsp;9.7.6.2.2, &#8220;HFile Tool&#8221;</a>.</p></div><div class="section" title="15.1.6.&nbsp;WAL Tools"><div class="titlepage"><div><div><h3 class="title"><a name="wal_tools"></a>15.1.6.&nbsp;WAL Tools</h3></div></div></div><div class="section" title="15.1.6.1.&nbsp;FSHLog tool"><div class="titlepage"><div><div><h4 class="title"><a name="hlog_tool"></a>15.1.6.1.&nbsp;<code class="classname">FSHLog</code> tool</h4></div></div></div><p>The main method on <code class="classname">FSHLog</code> offers manual
        split and dump facilities. Pass it WALs or the product of a split, the
        content of the <code class="filename">recovered.edits</code>. directory.</p><p>You can get a textual dump of a WAL file content by doing the
        following:</p><pre class="programlisting"> <code class="code">$ ./bin/hbase org.apache.hadoop.hbase.regionserver.wal.FSHLog --dump hdfs://example.org:8020/hbase/.logs/example.org,60020,1283516293161/10.10.21.10%3A60020.1283973724012</code> </pre><p>The
        return code will be non-zero if issues with the file so you can test
        wholesomeness of file by redirecting <code class="varname">STDOUT</code> to
        <code class="code">/dev/null</code> and testing the program return.</p><p>Similarly you can force a split of a log file directory by
        doing:</p><pre class="programlisting"> $ ./<code class="code">bin/hbase org.apache.hadoop.hbase.regionserver.wal.FSHLog --split hdfs://example.org:8020/hbase/.logs/example.org,60020,1283516293161/</code></pre><div class="section" title="15.1.6.1.1.&nbsp;HLogPrettyPrinter"><div class="titlepage"><div><div><h5 class="title"><a name="hlog_tool.prettyprint"></a>15.1.6.1.1.&nbsp;<code class="classname">HLogPrettyPrinter</code></h5></div></div></div><p><code class="classname">HLogPrettyPrinter</code> is a tool with configurable options to print the contents of an HLog.
          </p></div></div></div><div class="section" title="15.1.7.&nbsp;Compression Tool"><div class="titlepage"><div><div><h3 class="title"><a name="compression.tool"></a>15.1.7.&nbsp;Compression Tool</h3></div></div></div><p>See <a class="xref" href="#compression.test" title="C.1.&nbsp;CompressionTest Tool">Section&nbsp;C.1, &#8220;CompressionTest Tool&#8221;</a>.</p></div><div class="section" title="15.1.8.&nbsp;CopyTable"><div class="titlepage"><div><div><h3 class="title"><a name="copytable"></a>15.1.8.&nbsp;CopyTable</h3></div></div></div><p>
          CopyTable is a utility that can copy part or of all of a table, either to the same cluster or another cluster. The target table must
          first exist. The usage is as follows:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.CopyTable [--starttime=X] [--endtime=Y] [--new.name=NEW] [--peer.adr=ADR] tablename
</pre><p>
        </p><p>
        Options:
        </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><code class="varname">starttime</code>  Beginning of the time range.  Without endtime means starttime to forever.</li><li class="listitem"><code class="varname">endtime</code>  End of the time range.  Without endtime means starttime to forever.</li><li class="listitem"><code class="varname">versions</code>  Number of cell versions to copy.</li><li class="listitem"><code class="varname">new.name</code>  New table's name.</li><li class="listitem"><code class="varname">peer.adr</code>  Address of the peer cluster given in the format hbase.zookeeper.quorum:hbase.zookeeper.client.port:zookeeper.znode.parent</li><li class="listitem"><code class="varname">families</code>  Comma-separated list of ColumnFamilies to copy.</li><li class="listitem"><code class="varname">all.cells</code>  Also copy delete markers and uncollected deleted cells (advanced option).</li></ul></div><p>
         Args:
        </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">tablename  Name of table to copy.</li></ul></div><p>
        </p><p>Example of copying 'TestTable' to a cluster that uses replication for a 1 hour window:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.CopyTable
--starttime=1265875194289 --endtime=1265878794289
--peer.adr=server1,server2,server3:2181:/hbase TestTable</pre><p>
        </p><div class="note" title="Scanner Caching" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Scanner Caching</h3><p>Caching for the input Scan is configured via <code class="code">hbase.client.scanner.caching</code> in the job configuration.
        </p></div><div class="note" title="Versions" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Versions</h3><p>By default, CopyTable utility only copies the latest version of row cells unless <code class="code">--versions=n</code> is explicitly specified in the command.
        </p></div><p>
        See Jonathan Hsieh's <a class="link" href="http://www.cloudera.com/blog/2012/06/online-hbase-backups-with-copytable-2/" target="_top">Online HBase Backups with CopyTable</a> blog post for more on <span class="command"><strong>CopyTable</strong></span>.
        </p></div><div class="section" title="15.1.9.&nbsp;Export"><div class="titlepage"><div><div><h3 class="title"><a name="export"></a>15.1.9.&nbsp;Export</h3></div></div></div><p>Export is a utility that will dump the contents of table to HDFS in a sequence file.  Invoke via:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.Export &lt;tablename&gt; &lt;outputdir&gt; [&lt;versions&gt; [&lt;starttime&gt; [&lt;endtime&gt;]]]
</pre><p>
       </p><p>Note:  caching for the input Scan is configured via <code class="code">hbase.client.scanner.caching</code> in the job configuration.
        </p></div><div class="section" title="15.1.10.&nbsp;Import"><div class="titlepage"><div><div><h3 class="title"><a name="import"></a>15.1.10.&nbsp;Import</h3></div></div></div><p>Import is a utility that will load data that has been exported back into HBase.  Invoke via:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.Import &lt;tablename&gt; &lt;inputdir&gt;
</pre><p>
       </p><p>To import 0.94 exported files in a 0.96 cluster or onwards, you need to set system property "hbase.import.version" when running the import command as below:
</p><pre class="programlisting">$ bin/hbase -Dhbase.import.version=0.94 org.apache.hadoop.hbase.mapreduce.Import &lt;tablename&gt; &lt;inputdir&gt;
</pre><p>
       </p></div><div class="section" title="15.1.11.&nbsp;ImportTsv"><div class="titlepage"><div><div><h3 class="title"><a name="importtsv"></a>15.1.11.&nbsp;ImportTsv</h3></div></div></div><p>ImportTsv is a utility that will load data in TSV format into HBase.  It has two distinct usages:  loading data from TSV format in HDFS
       into HBase via Puts, and preparing StoreFiles to be loaded via the <code class="code">completebulkload</code>.
       </p><p>To load data via Puts (i.e., non-bulk loading):
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.columns=a,b,c &lt;tablename&gt; &lt;hdfs-inputdir&gt;
</pre><p>
       </p><p>To generate StoreFiles for bulk-loading:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.columns=a,b,c -Dimporttsv.bulk.output=hdfs://storefile-outputdir &lt;tablename&gt; &lt;hdfs-data-inputdir&gt;
</pre><p>
       </p><p>These generated StoreFiles can be loaded into HBase via <a class="xref" href="#completebulkload" title="15.1.12.&nbsp;CompleteBulkLoad">Section&nbsp;15.1.12, &#8220;CompleteBulkLoad&#8221;</a>.
       </p><div class="section" title="15.1.11.1.&nbsp;ImportTsv Options"><div class="titlepage"><div><div><h4 class="title"><a name="importtsv.options"></a>15.1.11.1.&nbsp;ImportTsv Options</h4></div></div></div>
       Running ImportTsv with no arguments prints brief usage information:
<pre class="programlisting">
Usage: importtsv -Dimporttsv.columns=a,b,c &lt;tablename&gt; &lt;inputdir&gt;

Imports the given input directory of TSV data into the specified table.

The column names of the TSV data must be specified using the -Dimporttsv.columns
option. This option takes the form of comma-separated column names, where each
column name is either a simple column family, or a columnfamily:qualifier. The special
column name HBASE_ROW_KEY is used to designate that this column should be used
as the row key for each imported record. You must specify exactly one column
to be the row key, and you must specify a column name for every column that exists in the
input data.

By default importtsv will load data directly into HBase. To instead generate
HFiles of data to prepare for a bulk data load, pass the option:
  -Dimporttsv.bulk.output=/path/for/output
  Note: the target table will be created with default column family descriptors if it does not already exist.

Other options that may be specified with -D include:
  -Dimporttsv.skip.bad.lines=false - fail if encountering an invalid line
  '-Dimporttsv.separator=|' - eg separate on pipes instead of tabs
  -Dimporttsv.timestamp=currentTimeAsLong - use the specified timestamp for the import
  -Dimporttsv.mapper.class=my.Mapper - A user-defined Mapper to use instead of org.apache.hadoop.hbase.mapreduce.TsvImporterMapper
</pre></div><div class="section" title="15.1.11.2.&nbsp;ImportTsv Example"><div class="titlepage"><div><div><h4 class="title"><a name="importtsv.example"></a>15.1.11.2.&nbsp;ImportTsv Example</h4></div></div></div><p>For example, assume that we are loading data into a table called 'datatsv' with a ColumnFamily called 'd' with two columns "c1" and "c2".
         </p><p>Assume that an input file exists as follows:
</p><pre class="programlisting">
row1	c1	c2
row2	c1	c2
row3	c1	c2
row4	c1	c2
row5	c1	c2
row6	c1	c2
row7	c1	c2
row8	c1	c2
row9	c1	c2
row10	c1	c2
</pre><p>
         </p><p>For ImportTsv to use this imput file, the command line needs to look like this:
 </p><pre class="programlisting">
 HADOOP_CLASSPATH=`${HBASE_HOME}/bin/hbase classpath` ${HADOOP_HOME}/bin/hadoop jar ${HBASE_HOME}/hbase-VERSION.jar importtsv -Dimporttsv.columns=HBASE_ROW_KEY,d:c1,d:c2 -Dimporttsv.bulk.output=hdfs://storefileoutput datatsv hdfs://inputfile
 </pre><p>
         ... and in this example the first column is the rowkey, which is why the HBASE_ROW_KEY is used.  The second and third columns in the file will be imported as "d:c1" and "d:c2", respectively.
         </p></div><div class="section" title="15.1.11.3.&nbsp;ImportTsv Warning"><div class="titlepage"><div><div><h4 class="title"><a name="importtsv.warning"></a>15.1.11.3.&nbsp;ImportTsv Warning</h4></div></div></div><p>If you have preparing a lot of data for bulk loading, make sure the target HBase table is pre-split appropriately.
         </p></div><div class="section" title="15.1.11.4.&nbsp;See Also"><div class="titlepage"><div><div><h4 class="title"><a name="importtsv.also"></a>15.1.11.4.&nbsp;See Also</h4></div></div></div>
       For more information about bulk-loading HFiles into HBase, see <a class="xref" href="#arch.bulk.load" title="9.8.&nbsp;Bulk Loading">Section&nbsp;9.8, &#8220;Bulk Loading&#8221;</a></div></div><div class="section" title="15.1.12.&nbsp;CompleteBulkLoad"><div class="titlepage"><div><div><h3 class="title"><a name="completebulkload"></a>15.1.12.&nbsp;CompleteBulkLoad</h3></div></div></div><p>The <code class="code">completebulkload</code> utility will move generated StoreFiles into an HBase table.  This utility is often used
	   in conjunction with output from <a class="xref" href="#importtsv" title="15.1.11.&nbsp;ImportTsv">Section&nbsp;15.1.11, &#8220;ImportTsv&#8221;</a>.
	   </p><p>There are two ways to invoke this utility, with explicit classname and via the driver:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles &lt;hdfs://storefileoutput&gt; &lt;tablename&gt;
</pre><p>
.. and via the Driver..
</p><pre class="programlisting">HADOOP_CLASSPATH=`${HBASE_HOME}/bin/hbase classpath` ${HADOOP_HOME}/bin/hadoop jar ${HBASE_HOME}/hbase-VERSION.jar completebulkload &lt;hdfs://storefileoutput&gt; &lt;tablename&gt;
</pre><p>
	  </p><div class="section" title="15.1.12.1.&nbsp;CompleteBulkLoad Warning"><div class="titlepage"><div><div><h4 class="title"><a name="completebulkload.warning"></a>15.1.12.1.&nbsp;CompleteBulkLoad Warning</h4></div></div></div><p>Data generated via MapReduce is often created with file permissions that are not compatible with the running HBase process. Assuming you're running HDFS with permissions enabled, those permissions will need to be updated before you run CompleteBulkLoad.
          </p></div><p>For more information about bulk-loading HFiles into HBase, see <a class="xref" href="#arch.bulk.load" title="9.8.&nbsp;Bulk Loading">Section&nbsp;9.8, &#8220;Bulk Loading&#8221;</a>.
       </p></div><div class="section" title="15.1.13.&nbsp;WALPlayer"><div class="titlepage"><div><div><h3 class="title"><a name="walplayer"></a>15.1.13.&nbsp;WALPlayer</h3></div></div></div><p>WALPlayer is a utility to replay WAL files into HBase.
       </p><p>The WAL can be replayed for a set of tables or all tables, and a
           timerange can be provided (in milliseconds). The WAL is filtered to
           this set of tables. The output can optionally be mapped to another set of tables.
       </p><p>WALPlayer can also generate HFiles for later bulk importing, in that case
           only a single table and no mapping can be specified.
       </p><p>Invoke via:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.WALPlayer [options] &lt;wal inputdir&gt; &lt;tables&gt; [&lt;tableMappings&gt;]&gt;
</pre><p>
       </p><p>For example:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.WALPlayer /backuplogdir oldTable1,oldTable2 newTable1,newTable2
</pre><p>
       </p><p>
           WALPlayer, by default, runs as a mapreduce job.  To NOT run WALPlayer as a mapreduce job on your cluster,
           force it to run all in the local process by adding the flags <code class="code">-Dmapred.job.tracker=local</code> on the command line.
       </p></div><div class="section" title="15.1.14.&nbsp;RowCounter and CellCounter"><div class="titlepage"><div><div><h3 class="title"><a name="rowcounter"></a>15.1.14.&nbsp;RowCounter and CellCounter</h3></div></div></div><p><a class="ulink" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/RowCounter.html" target="_top">RowCounter</a> is a
       mapreduce job to count all the rows of a table.  This is a good utility to use as a sanity check to ensure that HBase can read
       all the blocks of a table if there are any concerns of metadata inconsistency. It will run the mapreduce all in a single
       process but it will run faster if you have a MapReduce cluster in place for it to exploit.
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.RowCounter &lt;tablename&gt; [&lt;column1&gt; &lt;column2&gt;...]
</pre><p>
       </p><p>Note: caching for the input Scan is configured via <code class="code">hbase.client.scanner.caching</code> in the job configuration.
       </p><p>HBase ships another diagnostic mapreduce job called
         <a class="ulink" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/CellCounter.html" target="_top">CellCounter</a>. Like
         RowCounter, it gathers more fine-grained statistics about your table. The statistics gathered by RowCounter are more fine-grained
         and include:
         </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Total number of rows in the table.</li><li class="listitem">Total number of CFs across all rows.</li><li class="listitem">Total qualifiers across all rows.</li><li class="listitem">Total occurrence of each CF.</li><li class="listitem">Total occurrence of each qualifier.</li><li class="listitem">Total number of versions of each qualifier.</li></ul></div><p>
       </p><p>The program allows you to limit the scope of the run. Provide a row regex or prefix to limit the rows to analyze. Use
         <code class="code">hbase.mapreduce.scan.column.family</code> to specify scanning a single column family.
         </p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.CellCounter &lt;tablename&gt; &lt;outputDir&gt; [regex or prefix]</pre><p>
       </p><p>Note: just like RowCounter, caching for the input Scan is configured via <code class="code">hbase.client.scanner.caching</code> in the
       job configuration. </p></div><div class="section" title="15.1.15.&nbsp;mlockall"><div class="titlepage"><div><div><h3 class="title"><a name="mlockall"></a>15.1.15.&nbsp;mlockall</h3></div></div></div><p>It is possible to optionally pin your servers in physical memory making them less likely
            to be swapped out in oversubscribed environments by having the servers call
            <a class="link" href="http://linux.die.net/man/2/mlockall" target="_top">mlockall</a> on startup.
            See <a class="link" href="https://issues.apache.org/jira/browse/HBASE-4391" target="_top">HBASE-4391 Add ability to start RS as root and call mlockall</a>
            for how to build the optional library and have it run on startup.
        </p></div><div class="section" title="15.1.16.&nbsp;Offline Compaction Tool"><div class="titlepage"><div><div><h3 class="title"><a name="compaction.tool"></a>15.1.16.&nbsp;Offline Compaction Tool</h3></div></div></div><p>See the usage for the <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/regionserver/CompactionTool.html" target="_top">Compaction Tool</a>.
            Run it like this <span class="command"><strong>./bin/hbase org.apache.hadoop.hbase.regionserver.CompactionTool</strong></span>
        </p></div></div><div class="section" title="15.2.&nbsp;Region Management"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="ops.regionmgt"></a>15.2.&nbsp;Region Management</h2></div></div></div><div class="section" title="15.2.1.&nbsp;Major Compaction"><div class="titlepage"><div><div><h3 class="title"><a name="ops.regionmgt.majorcompact"></a>15.2.1.&nbsp;Major Compaction</h3></div></div></div><p>Major compactions can be requested via the HBase shell or <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HBaseAdmin.html#majorCompact%28java.lang.String%29" target="_top">HBaseAdmin.majorCompact</a>.
      </p><p>Note:  major compactions do NOT do region merges.  See <a class="xref" href="#compaction" title="9.7.6.5.&nbsp;Compaction">Section&nbsp;9.7.6.5, &#8220;Compaction&#8221;</a> for more information about compactions.

      </p></div><div class="section" title="15.2.2.&nbsp;Merge"><div class="titlepage"><div><div><h3 class="title"><a name="ops.regionmgt.merge"></a>15.2.2.&nbsp;Merge</h3></div></div></div><p>Merge is a utility that can merge adjoining regions in the same table (see org.apache.hadoop.hbase.util.Merge).</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.util.Merge &lt;tablename&gt; &lt;region1&gt; &lt;region2&gt;
</pre><p>If you feel you have too many regions and want to consolidate them, Merge is the utility you need.  Merge must
      run be done when the cluster is down.
      See the <a class="link" href="http://ofps.oreilly.com/titles/9781449396107/performance.html" target="_top">O'Reilly HBase Book</a> for
      an example of usage.
      </p><p>You will need to pass 3 parameters to this application. The first one is the table name. The second one is the fully
      qualified name of the first region to merge, like "table_name,\x0A,1342956111995.7cef47f192318ba7ccc75b1bbf27a82b.". The third one
      is the fully qualified name for the second region to merge.
      </p><p>Additionally, there is a Ruby script attached to <a class="link" href="https://issues.apache.org/jira/browse/HBASE-1621" target="_top">HBASE-1621</a>
      for region merging.
      </p></div></div><div class="section" title="15.3.&nbsp;Node Management"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="node.management"></a>15.3.&nbsp;Node Management</h2></div></div></div><div class="section" title="15.3.1.&nbsp;Node Decommission"><div class="titlepage"><div><div><h3 class="title"><a name="decommission"></a>15.3.1.&nbsp;Node Decommission</h3></div></div></div><p>You can stop an individual RegionServer by running the following
            script in the HBase directory on the particular  node:
            </p><pre class="programlisting">$ ./bin/hbase-daemon.sh stop regionserver</pre><p>
            The RegionServer will first close all regions and then shut itself down.
            On shutdown, the RegionServer's ephemeral node in ZooKeeper will expire.
            The master will notice the RegionServer gone and will treat it as
            a 'crashed' server; it will reassign the nodes the RegionServer was carrying.
            </p><div class="note" title="Disable the Load Balancer before Decommissioning a node" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Disable the Load Balancer before Decommissioning a node</h3><p>If the load balancer runs while a node is shutting down, then
                 there could be contention between the Load Balancer and the
                 Master's recovery of the just decommissioned RegionServer.
                 Avoid any problems by disabling the balancer first.
                 See <a class="xref" href="#lb" title="Load Balancer">Load Balancer</a> below.
             </p></div><p>
        </p><p>
        A downside to the above stop of a RegionServer is that regions could be offline for
        a good period of time.  Regions are closed in order.  If many regions on the server, the
        first region to close may not be back online until all regions close and after the master
        notices the RegionServer's znode gone.  In Apache HBase 0.90.2, we added facility for having
        a node gradually shed its load and then shutdown itself down. Apache HBase 0.90.2 added the
            <code class="filename">graceful_stop.sh</code> script.  Here is its usage:
            </p><pre class="programlisting">$ ./bin/graceful_stop.sh
Usage: graceful_stop.sh [--config &amp;conf-dir&gt;] [--restart] [--reload] [--thrift] [--rest] &amp;hostname&gt;
 thrift      If we should stop/start thrift before/after the hbase stop/start
 rest        If we should stop/start rest before/after the hbase stop/start
 restart     If we should restart after graceful stop
 reload      Move offloaded regions back on to the stopped server
 debug       Move offloaded regions back on to the stopped server
 hostname    Hostname of server we are to stop</pre><p>
        </p><p>
            To decommission a loaded RegionServer, run the following:
            </p><pre class="programlisting">$ ./bin/graceful_stop.sh HOSTNAME</pre><p>
            where <code class="varname">HOSTNAME</code> is the host carrying the RegionServer
            you would decommission.
            </p><div class="note" title="On HOSTNAME" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">On <code class="varname">HOSTNAME</code></h3><p>The <code class="varname">HOSTNAME</code> passed to <code class="filename">graceful_stop.sh</code>
            must match the hostname that hbase is using to identify RegionServers.
            Check the list of RegionServers in the master UI for how HBase is
            referring to servers. Its usually hostname but can also be FQDN.
            Whatever HBase is using, this is what you should pass the
            <code class="filename">graceful_stop.sh</code> decommission
            script.  If you pass IPs, the script is not yet smart enough to make
            a hostname (or FQDN) of it and so it will fail when it checks if server is
            currently running; the graceful unloading of regions will not run.
            </p></div><p> The <code class="filename">graceful_stop.sh</code> script will move the regions off the
            decommissioned RegionServer one at a time to minimize region churn.
            It will verify the region deployed in the new location before it
            will moves the next region and so on until the decommissioned server
            is carrying zero regions.  At this point, the <code class="filename">graceful_stop.sh</code>
            tells the RegionServer <span class="command"><strong>stop</strong></span>.  The master will at this point notice the
            RegionServer gone but all regions will have already been redeployed
            and because the RegionServer went down cleanly, there will be no
            WAL logs to split.
            </p><div class="note" title="Load Balancer" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a name="lb"></a>Load Balancer</h3><p>
                It is assumed that the Region Load Balancer is disabled while the
                <span class="command"><strong>graceful_stop</strong></span> script runs (otherwise the balancer
                and the decommission script will end up fighting over region deployments).
                Use the shell to disable the balancer:
                </p><pre class="programlisting">hbase(main):001:0&gt; balance_switch false
true
0 row(s) in 0.3590 seconds</pre><p>
This turns the balancer OFF.  To reenable, do:
                </p><pre class="programlisting">hbase(main):001:0&gt; balance_switch true
false
0 row(s) in 0.3590 seconds</pre><p>
            </p><p>The <span class="command"><strong>graceful_stop</strong></span> will check the balancer
                and if enabled, will turn it off before it goes to work.  If it
                exits prematurely because of error, it will not have reset the
                balancer.  Hence, it is better to manage the balancer apart from
                <span class="command"><strong>graceful_stop</strong></span> reenabling it after you are done
                w/ graceful_stop.
            </p></div><p>
        </p><div class="section" title="15.3.1.1.&nbsp;Decommissioning several Regions Servers concurrently"><div class="titlepage"><div><div><h4 class="title"><a name="draining.servers"></a>15.3.1.1.&nbsp;Decommissioning several Regions Servers concurrently</h4></div></div></div><p>If you have a large cluster, you may want to
            decommission more than one machine at a time by gracefully
            stopping mutiple RegionServers concurrently.
            To gracefully drain multiple regionservers at the
	    same time, RegionServers can be put into a "draining"
	    state.  This is done by marking a RegionServer as a
	    draining node by creating an entry in ZooKeeper under the
        <code class="filename">hbase_root/draining</code> znode.  This znode has format
        </p><pre class="programlisting">name,port,startcode</pre><p> just like the regionserver entries
        under <code class="filename">hbase_root/rs</code> znode.
	    </p><p>Without this facility, decommissioning mulitple nodes
	    may be non-optimal because regions that are being drained
	    from one region server may be moved to other regionservers that
	    are also draining.  Marking RegionServers to be in the
        draining state prevents this from happening<sup>[<a name="d366e11363" href="#ftn.d366e11363" class="footnote">33</a>]</sup>.
	    </p></div><div class="section" title="15.3.1.2.&nbsp;Bad or Failing Disk"><div class="titlepage"><div><div><h4 class="title"><a name="bad.disk"></a>15.3.1.2.&nbsp;Bad or Failing Disk</h4></div></div></div><p>It is good having <a class="xref" href="#dfs.datanode.failed.volumes.tolerated" title="2.5.2.2.1.&nbsp;dfs.datanode.failed.volumes.tolerated">Section&nbsp;2.5.2.2.1, &#8220;dfs.datanode.failed.volumes.tolerated&#8221;</a> set if you have a decent number of disks
            per machine for the case where a disk plain dies.  But usually disks do the "John Wayne" -- i.e. take a while
            to go down spewing errors in <code class="filename">dmesg</code> -- or for some reason, run much slower than their
            companions.  In this case you want to decommission the disk.  You have two options.  You can
            <a class="link" href="http://wiki.apache.org/hadoop/FAQ#I_want_to_make_a_large_cluster_smaller_by_taking_out_a_bunch_of_nodes_simultaneously._How_can_this_be_done.3F" target="_top">decommission the datanode</a>
            or, less disruptive in that only the bad disks data will be rereplicated, can stop the datanode,
            unmount the bad volume (You can't umount a volume while the datanode is using it), and then restart the
            datanode (presuming you have set dfs.datanode.failed.volumes.tolerated &gt; 0).  The regionserver will
            throw some errors in its logs as it recalibrates where to get its data from -- it will likely
            roll its WAL log too -- but in general but for some latency spikes, it should keep on chugging.
            </p><div class="note" title="Short Circuit Reads" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Short Circuit Reads</h3><p>If you are doing short-circuit reads, you will have to move the regions off the regionserver
                    before you stop the datanode; when short-circuiting reading, though chmod'd so regionserver cannot
                    have access, because it already has the files open, it will be able to keep reading the file blocks
                    from the bad disk even though the datanode is down.  Move the regions back after you restart the
                datanode.</p></div><p>
            </p></div></div><div class="section" title="15.3.2.&nbsp;Rolling Restart"><div class="titlepage"><div><div><h3 class="title"><a name="rolling"></a>15.3.2.&nbsp;Rolling Restart</h3></div></div></div><p>
            You can also ask this script to restart a RegionServer after the shutdown
            AND move its old regions back into place.  The latter you might do to
            retain data locality.  A primitive rolling restart might be effected by
            running something like the following:
            </p><pre class="programlisting">$ for i in `cat conf/regionservers|sort`; do ./bin/graceful_stop.sh --restart --reload --debug $i; done &amp;&gt; /tmp/log.txt &amp;
            </pre><p>
            Tail the output of <code class="filename">/tmp/log.txt</code> to follow the scripts
            progress. The above does RegionServers only.  The script will also disable the
            load balancer before moving the regions.  You'd need to do the master
            update separately.  Do it before you run the above script.
            Here is a pseudo-script for how you might craft a rolling restart script:
            </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Untar your release, make sure of its configuration and
                        then rsync it across the cluster. If this is 0.90.2, patch it
                        with HBASE-3744 and HBASE-3756.
                    </p></li><li class="listitem"><p>Run hbck to ensure the cluster consistent
                        </p><pre class="programlisting">$ ./bin/hbase hbck</pre><p>
                    Effect repairs if inconsistent.
                    </p></li><li class="listitem"><p>Restart the Master: </p><pre class="programlisting">$ ./bin/hbase-daemon.sh stop master; ./bin/hbase-daemon.sh start master</pre><p>
                    </p></li><li class="listitem"><p>Run the <code class="filename">graceful_stop.sh</code> script per RegionServer.  For example:
            </p><pre class="programlisting">$ for i in `cat conf/regionservers|sort`; do ./bin/graceful_stop.sh --restart --reload --debug $i; done &amp;&gt; /tmp/log.txt &amp;
            </pre><p>
                     If you are running thrift or rest servers on the RegionServer, pass --thrift or --rest options (See usage
                     for <code class="filename">graceful_stop.sh</code> script).
                 </p></li><li class="listitem"><p>Restart the Master again.  This will clear out dead servers list and reenable the balancer.
                    </p></li><li class="listitem"><p>Run hbck to ensure the cluster is consistent.
                    </p></li></ol></div><p>
        </p><p>It is important to drain HBase regions slowly when
	restarting regionservers. Otherwise, multiple regions go
	offline simultaneously as they are re-assigned to other
	nodes. Depending on your usage patterns, this might not be
	desirable.
	</p></div><div class="section" title="15.3.3.&nbsp;Adding a New Node"><div class="titlepage"><div><div><h3 class="title"><a name="adding.new.node"></a>15.3.3.&nbsp;Adding a New Node</h3></div></div></div><p>Adding a new regionserver in HBase is essentially free, you simply start it like this:
              </p><pre class="programlisting">$ ./bin/hbase-daemon.sh start regionserver</pre><p>
              and it will register itself with the master. Ideally you also started a DataNode on the same
              machine so that the RS can eventually start to have local files. If you rely on ssh to start your
              daemons, don't forget to add the new hostname in <code class="filename">conf/regionservers</code> on the master.
        </p><p>At this point the region server isn't serving data because no regions have moved to it yet. If the balancer is
              enabled, it will start moving regions to the new RS. On a small/medium cluster this can have a very adverse effect
              on latency as a lot of regions will be offline at the same time. It is thus recommended to disable the balancer
              the same way it's done when decommissioning a node and move the regions manually (or even better, using a script
              that moves them one by one).
        </p><p>The moved regions will all have 0% locality and won't have any blocks in cache so the region server will have
              to use the network to serve requests. Apart from resulting in higher latency, it may also be able to use all of
              your network card's capacity. For practical purposes, consider that a standard 1GigE NIC won't be able to read
              much more than <span class="emphasis"><em>100MB/s</em></span>. In this case, or if you are in a OLAP environment and require having
              locality, then it is recommended to major compact the moved regions.
        </p></div></div><div class="section" title="15.4.&nbsp;HBase Metrics"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="hbase_metrics"></a>15.4.&nbsp;HBase Metrics</h2></div></div></div><div class="section" title="15.4.1.&nbsp;Metric Setup"><div class="titlepage"><div><div><h3 class="title"><a name="metric_setup"></a>15.4.1.&nbsp;Metric Setup</h3></div></div></div><p>See <a class="link" href="http://hbase.apache.org/metrics.html" target="_top">Metrics</a> for
  an introduction and how to enable Metrics emission.  Still valid for HBase 0.94.x.
  </p><p>For HBase 0.95.x and up, see <a class="link" href="http://hadoop.apache.org/docs/current/api/org/apache/hadoop/metrics2/package-summary.html" target="_top">http://hadoop.apache.org/docs/current/api/org/apache/hadoop/metrics2/package-summary.html</a>
  </p></div><div class="section" title="15.4.2.&nbsp;Warning To Ganglia Users"><div class="titlepage"><div><div><h3 class="title"><a name="rs_metrics_ganglia"></a>15.4.2.&nbsp;Warning To Ganglia Users</h3></div></div></div><p>Warning to Ganglia Users:  by default, HBase will emit a LOT of metrics per RegionServer which may swamp your installation.
     Options include either increasing Ganglia server capacity, or configuring HBase to emit fewer metrics.
     </p></div><div class="section" title="15.4.3.&nbsp;Most Important RegionServer Metrics"><div class="titlepage"><div><div><h3 class="title"><a name="rs_metrics"></a>15.4.3.&nbsp;Most Important RegionServer Metrics</h3></div></div></div><div class="section" title="15.4.3.1.&nbsp;blockCacheExpressCachingRatio (formerly blockCacheHitCachingRatio)"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.blockCacheHitCachingRatio"></a>15.4.3.1.&nbsp;<code class="varname">blockCacheExpressCachingRatio (formerly blockCacheHitCachingRatio)</code></h4></div></div></div><p>Block cache hit caching ratio (0 to 100).  The cache-hit ratio for reads configured to look in the cache (i.e., cacheBlocks=true). </p></div><div class="section" title="15.4.3.2.&nbsp;callQueueLength"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.callQueueLength"></a>15.4.3.2.&nbsp;<code class="varname">callQueueLength</code></h4></div></div></div><p>Point in time length of the RegionServer call queue.  If requests arrive faster than the RegionServer handlers can process
          them they will back up in the callQueue.</p></div><div class="section" title="15.4.3.3.&nbsp;compactionQueueLength (formerly compactionQueueSize)"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.compactionQueueSize"></a>15.4.3.3.&nbsp;<code class="varname">compactionQueueLength (formerly compactionQueueSize)</code></h4></div></div></div><p>Point in time length of the compaction queue.  This is the number of Stores in the RegionServer that have been targeted for compaction.</p></div><div class="section" title="15.4.3.4.&nbsp;flushQueueSize"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.flushQueueSize"></a>15.4.3.4.&nbsp;<code class="varname">flushQueueSize</code></h4></div></div></div><p>Point in time number of enqueued regions in the MemStore awaiting flush.</p></div><div class="section" title="15.4.3.5.&nbsp;hdfsBlocksLocalityIndex"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.hdfsBlocksLocalityIndex"></a>15.4.3.5.&nbsp;<code class="varname">hdfsBlocksLocalityIndex</code></h4></div></div></div><p>Point in time percentage of HDFS blocks that are local to this RegionServer.  The higher the better.  </p></div><div class="section" title="15.4.3.6.&nbsp;memstoreSizeMB"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.memstoreSizeMB"></a>15.4.3.6.&nbsp;<code class="varname">memstoreSizeMB</code></h4></div></div></div><p>Point in time sum of all the memstore sizes in this RegionServer (MB).  Watch for this nearing or exceeding
          the configured high-watermark for MemStore memory in the RegionServer. </p></div><div class="section" title="15.4.3.7.&nbsp;numberOfOnlineRegions"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.regions"></a>15.4.3.7.&nbsp;<code class="varname">numberOfOnlineRegions</code></h4></div></div></div><p>Point in time number of regions served by the RegionServer.  This is an important metric to track for RegionServer-Region density.
          </p></div><div class="section" title="15.4.3.8.&nbsp;readRequestsCount"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.readRequestsCount"></a>15.4.3.8.&nbsp;<code class="varname">readRequestsCount</code></h4></div></div></div><p>Number of read requests for this RegionServer since startup.  Note:  this is a 32-bit integer and can roll. </p></div><div class="section" title="15.4.3.9.&nbsp;slowHLogAppendCount"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.slowHLogAppendCount"></a>15.4.3.9.&nbsp;<code class="varname">slowHLogAppendCount</code></h4></div></div></div><p>Number of slow HLog append writes for this RegionServer since startup, where "slow" is &gt; 1 second.  This is
          a good "canary" metric for HDFS. </p></div><div class="section" title="15.4.3.10.&nbsp;usedHeapMB"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.usedHeapMB"></a>15.4.3.10.&nbsp;<code class="varname">usedHeapMB</code></h4></div></div></div><p>Point in time amount of memory used by the RegionServer (MB).</p></div><div class="section" title="15.4.3.11.&nbsp;writeRequestsCount"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.writeRequestsCount"></a>15.4.3.11.&nbsp;<code class="varname">writeRequestsCount</code></h4></div></div></div><p>Number of write requests for this RegionServer since startup.  Note:  this is a 32-bit integer and can roll. </p></div></div><div class="section" title="15.4.4.&nbsp;Other RegionServer Metrics"><div class="titlepage"><div><div><h3 class="title"><a name="rs_metrics_other"></a>15.4.4.&nbsp;Other RegionServer Metrics</h3></div></div></div><div class="section" title="15.4.4.1.&nbsp;blockCacheCount"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.blockCacheCount"></a>15.4.4.1.&nbsp;<code class="varname">blockCacheCount</code></h4></div></div></div><p>Point in time block cache item count in memory.  This is the number of blocks of StoreFiles (HFiles) in the cache.</p></div><div class="section" title="15.4.4.2.&nbsp;blockCacheEvictedCount"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.blockCacheEvictedCount"></a>15.4.4.2.&nbsp;<code class="varname">blockCacheEvictedCount</code></h4></div></div></div><p>Number of blocks that had to be evicted from the block cache due to heap size constraints by RegionServer since startup.</p></div><div class="section" title="15.4.4.3.&nbsp;blockCacheFreeMB"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.blockCacheFree"></a>15.4.4.3.&nbsp;<code class="varname">blockCacheFreeMB</code></h4></div></div></div><p>Point in time block cache memory available (MB).</p></div><div class="section" title="15.4.4.4.&nbsp;blockCacheHitCount"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.blockCacheHitCount"></a>15.4.4.4.&nbsp;<code class="varname">blockCacheHitCount</code></h4></div></div></div><p>Number of blocks of StoreFiles (HFiles) read from the cache by RegionServer since startup.</p></div><div class="section" title="15.4.4.5.&nbsp;blockCacheHitRatio"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.blockCacheHitRatio"></a>15.4.4.5.&nbsp;<code class="varname">blockCacheHitRatio</code></h4></div></div></div><p>Block cache hit ratio (0 to 100) from RegionServer startup.  Includes all read requests, although those with cacheBlocks=false
           will always read from disk and be counted as a "cache miss", which means that full-scan MapReduce jobs can affect
           this metric significantly.</p></div><div class="section" title="15.4.4.6.&nbsp;blockCacheMissCount"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.blockCacheMissCount"></a>15.4.4.6.&nbsp;<code class="varname">blockCacheMissCount</code></h4></div></div></div><p>Number of blocks of StoreFiles (HFiles) requested but not read from the cache from RegionServer startup.</p></div><div class="section" title="15.4.4.7.&nbsp;blockCacheSizeMB"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.blockCacheSize"></a>15.4.4.7.&nbsp;<code class="varname">blockCacheSizeMB</code></h4></div></div></div><p>Point in time block cache size in memory (MB).  i.e., memory in use by the BlockCache</p></div><div class="section" title="15.4.4.8.&nbsp;fsPreadLatency*"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.fsPreadLatency"></a>15.4.4.8.&nbsp;<code class="varname">fsPreadLatency*</code></h4></div></div></div><p>There are several filesystem positional read latency (ms) metrics, all measured from RegionServer startup.</p></div><div class="section" title="15.4.4.9.&nbsp;fsReadLatency*"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.fsReadLatency"></a>15.4.4.9.&nbsp;<code class="varname">fsReadLatency*</code></h4></div></div></div><p>There are several filesystem read latency (ms) metrics, all measured from RegionServer startup.  The issue with
          interpretation is that ALL reads go into this metric (e.g., single-record Gets, full table Scans), including
          reads required for compactions.  This metric is only interesting "over time" when comparing
          major releases of HBase or your own code.</p></div><div class="section" title="15.4.4.10.&nbsp;fsWriteLatency*"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.fsWriteLatency"></a>15.4.4.10.&nbsp;<code class="varname">fsWriteLatency*</code></h4></div></div></div><p>There are several filesystem write latency (ms) metrics, all measured from RegionServer startup.  The issue with
          interpretation is that ALL writes go into this metric (e.g., single-record Puts, full table re-writes due to compaction).
          This metric is only interesting "over time" when comparing
          major releases of HBase or your own code.</p></div><div class="section" title="15.4.4.11.&nbsp;NumberOfStores"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.stores"></a>15.4.4.11.&nbsp;<code class="varname">NumberOfStores</code></h4></div></div></div><p>Point in time number of Stores open on the RegionServer.  A Store corresponds to a ColumnFamily.  For example,
          if a table (which contains the column family) has 3 regions on a RegionServer, there will be 3 stores open for that
          column family. </p></div><div class="section" title="15.4.4.12.&nbsp;NumberOfStorefiles"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.storeFiles"></a>15.4.4.12.&nbsp;<code class="varname">NumberOfStorefiles</code></h4></div></div></div><p>Point in time number of StoreFiles open on the RegionServer.  A store may have more than one StoreFile (HFile).</p></div><div class="section" title="15.4.4.13.&nbsp;requestsPerSecond"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.requests"></a>15.4.4.13.&nbsp;<code class="varname">requestsPerSecond</code></h4></div></div></div><p>Point in time number of read and write requests.  Requests correspond to RegionServer RPC calls,
           thus a single Get will result in 1 request, but a Scan with caching set to 1000 will result in 1 request for each 'next' call
            (i.e., not each row).  A bulk-load request will constitute 1 request per HFile.
            This metric is less interesting than readRequestsCount and writeRequestsCount in terms of measuring activity
            due to this metric being periodic. </p></div><div class="section" title="15.4.4.14.&nbsp;storeFileIndexSizeMB"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.storeFileIndexSizeMB"></a>15.4.4.14.&nbsp;<code class="varname">storeFileIndexSizeMB</code></h4></div></div></div><p>Point in time sum of all the StoreFile index sizes in this RegionServer (MB)</p></div></div></div><div class="section" title="15.5.&nbsp;HBase Monitoring"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="ops.monitoring"></a>15.5.&nbsp;HBase Monitoring</h2></div></div></div><div class="section" title="15.5.1.&nbsp;Overview"><div class="titlepage"><div><div><h3 class="title"><a name="ops.monitoring.overview"></a>15.5.1.&nbsp;Overview</h3></div></div></div><p>The following metrics are arguably the most important to monitor for each RegionServer for
      "macro monitoring", preferably with a system like <a class="link" href="http://opentsdb.net/" target="_top">OpenTSDB</a>.
      If your cluster is having performance issues it's likely that you'll see something unusual with
      this group.
      </p><p>HBase:
      </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">See <a class="xref" href="#rs_metrics" title="15.4.3.&nbsp;Most Important RegionServer Metrics">Section&nbsp;15.4.3, &#8220;Most Important RegionServer Metrics&#8221;</a></li></ul></div><p>
      </p><p>OS:
      </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">IO Wait</li><li class="listitem">User CPU</li></ul></div><p>
      </p><p>Java:
      </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">GC</li></ul></div><p>
      </p><p>
      </p><p>
      For more information on HBase metrics, see <a class="xref" href="#hbase_metrics" title="15.4.&nbsp;HBase Metrics">Section&nbsp;15.4, &#8220;HBase Metrics&#8221;</a>.
      </p></div><div class="section" title="15.5.2.&nbsp;Slow Query Log"><div class="titlepage"><div><div><h3 class="title"><a name="ops.slow.query"></a>15.5.2.&nbsp;Slow Query Log</h3></div></div></div><p>The HBase slow query log consists of parseable JSON structures describing the properties of those client operations (Gets, Puts, Deletes, etc.) that either took too long to run, or produced too much output. The thresholds for "too long to run" and "too much output" are configurable, as described below. The output is produced inline in the main region server logs so that it is easy to discover further details from context with other logged events. It is also prepended with identifying tags <code class="constant">(responseTooSlow)</code>, <code class="constant">(responseTooLarge)</code>, <code class="constant">(operationTooSlow)</code>, and <code class="constant">(operationTooLarge)</code> in order to enable easy filtering with grep, in case the user desires to see only slow queries.
</p><div class="section" title="15.5.2.1.&nbsp;Configuration"><div class="titlepage"><div><div><h4 class="title"><a name="d366e11687"></a>15.5.2.1.&nbsp;Configuration</h4></div></div></div><p>There are two configuration knobs that can be used to adjust the thresholds for when queries are logged.
</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><code class="varname">hbase.ipc.warn.response.time</code> Maximum number of milliseconds that a query can be run without being logged. Defaults to 10000, or 10 seconds. Can be set to -1 to disable logging by time.
</li><li class="listitem"><code class="varname">hbase.ipc.warn.response.size</code> Maximum byte size of response that a query can return without being logged. Defaults to 100 megabytes. Can be set to -1 to disable logging by size.
</li></ul></div></div><div class="section" title="15.5.2.2.&nbsp;Metrics"><div class="titlepage"><div><div><h4 class="title"><a name="d366e11701"></a>15.5.2.2.&nbsp;Metrics</h4></div></div></div><p>The slow query log exposes to metrics to JMX.
</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><code class="varname">hadoop.regionserver_rpc_slowResponse</code> a global metric reflecting the durations of all responses that triggered logging.</li><li class="listitem"><code class="varname">hadoop.regionserver_rpc_methodName.aboveOneSec</code> A metric reflecting the durations of all responses that lasted for more than one second.</li></ul></div><p>
</p></div><div class="section" title="15.5.2.3.&nbsp;Output"><div class="titlepage"><div><div><h4 class="title"><a name="d366e11716"></a>15.5.2.3.&nbsp;Output</h4></div></div></div><p>The output is tagged with operation e.g. <code class="constant">(operationTooSlow)</code> if the call was a client operation, such as a Put, Get, or Delete, which we expose detailed fingerprint information for. If not, it is tagged <code class="constant">(responseTooSlow)</code> and still produces parseable JSON output, but with less verbose information solely regarding its duration and size in the RPC itself. <code class="constant">TooLarge</code> is substituted for <code class="constant">TooSlow</code> if the response size triggered the logging, with <code class="constant">TooLarge</code> appearing even in the case that both size and duration triggered logging.
</p></div><div class="section" title="15.5.2.4.&nbsp;Example"><div class="titlepage"><div><div><h4 class="title"><a name="d366e11736"></a>15.5.2.4.&nbsp;Example</h4></div></div></div><p>
</p><pre class="programlisting">2011-09-08 10:01:25,824 WARN org.apache.hadoop.ipc.HBaseServer: (operationTooSlow): {"tables":{"riley2":{"puts":[{"totalColumns":11,"families":{"actions":[{"timestamp":1315501284459,"qualifier":"0","vlen":9667580},{"timestamp":1315501284459,"qualifier":"1","vlen":10122412},{"timestamp":1315501284459,"qualifier":"2","vlen":11104617},{"timestamp":1315501284459,"qualifier":"3","vlen":13430635}]},"row":"cfcd208495d565ef66e7dff9f98764da:0"}],"families":["actions"]}},"processingtimems":956,"client":"10.47.34.63:33623","starttimems":1315501284456,"queuetimems":0,"totalPuts":1,"class":"HRegionServer","responsesize":0,"method":"multiPut"}</pre><p>
</p><p>Note that everything inside the "tables" structure is output produced by MultiPut's fingerprint, while the rest of the information is RPC-specific, such as processing time and client IP/port. Other client operations follow the same pattern and the same general structure, with necessary differences due to the nature of the individual operations. In the case that the call is not a client operation, that detailed fingerprint information will be completely absent.
</p><p>This particular example, for example, would indicate that the likely cause of slowness is simply a very large (on the order of 100MB) multiput, as we can tell by the "vlen," or value length, fields of each put in the multiPut.
</p></div></div></div><div class="section" title="15.6.&nbsp;Cluster Replication"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="cluster_replication"></a>15.6.&nbsp;Cluster Replication</h2></div></div></div><p>See <a class="link" href="http://hbase.apache.org/replication.html" target="_top">Cluster Replication</a>.
    </p></div><div class="section" title="15.7.&nbsp;HBase Backup"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="ops.backup"></a>15.7.&nbsp;HBase Backup</h2></div></div></div><p>There are two broad strategies for performing HBase backups: backing up with a full cluster shutdown, and backing up on a live cluster.
    Each approach has pros and cons.
    </p><p>For additional information, see <a class="link" href="http://blog.sematext.com/2011/03/11/hbase-backup-options/" target="_top">HBase Backup Options</a> over on the Sematext Blog.
    </p><div class="section" title="15.7.1.&nbsp;Full Shutdown Backup"><div class="titlepage"><div><div><h3 class="title"><a name="ops.backup.fullshutdown"></a>15.7.1.&nbsp;Full Shutdown Backup</h3></div></div></div><p>Some environments can tolerate a periodic full shutdown of their HBase cluster, for example if it is being used a back-end analytic capacity
      and not serving front-end web-pages.  The benefits are that the NameNode/Master are RegionServers are down, so there is no chance of missing
      any in-flight changes to either StoreFiles or metadata.  The obvious con is that the cluster is down.  The steps include:
      </p><div class="section" title="15.7.1.1.&nbsp;Stop HBase"><div class="titlepage"><div><div><h4 class="title"><a name="ops.backup.fullshutdown.stop"></a>15.7.1.1.&nbsp;Stop HBase</h4></div></div></div><p>
        </p></div><div class="section" title="15.7.1.2.&nbsp;Distcp"><div class="titlepage"><div><div><h4 class="title"><a name="ops.backup.fullshutdown.distcp"></a>15.7.1.2.&nbsp;Distcp</h4></div></div></div><p>Distcp could be used to either copy the contents of the HBase directory in HDFS to either the same cluster in another directory, or
        to a different cluster.
        </p><p>Note:  Distcp works in this situation because the cluster is down and there are no in-flight edits to files.
        Distcp-ing of files in the HBase directory is not generally recommended on a live cluster.
        </p></div><div class="section" title="15.7.1.3.&nbsp;Restore (if needed)"><div class="titlepage"><div><div><h4 class="title"><a name="ops.backup.fullshutdown.restore"></a>15.7.1.3.&nbsp;Restore (if needed)</h4></div></div></div><p>The backup of the hbase directory from HDFS is copied onto the 'real' hbase directory via distcp.  The act of copying these files
        creates new HDFS metadata, which is why a restore of the NameNode edits from the time of the HBase backup isn't required for this kind of
        restore, because it's a restore (via distcp) of a specific HDFS directory (i.e., the HBase part) not the entire HDFS file-system.
        </p></div></div><div class="section" title="15.7.2.&nbsp;Live Cluster Backup - Replication"><div class="titlepage"><div><div><h3 class="title"><a name="ops.backup.live.replication"></a>15.7.2.&nbsp;Live Cluster Backup - Replication</h3></div></div></div><p>This approach assumes that there is a second cluster.
      See the HBase page on <a class="link" href="http://hbase.apache.org/replication.html" target="_top">replication</a> for more information.
      </p></div><div class="section" title="15.7.3.&nbsp;Live Cluster Backup - CopyTable"><div class="titlepage"><div><div><h3 class="title"><a name="ops.backup.live.copytable"></a>15.7.3.&nbsp;Live Cluster Backup - CopyTable</h3></div></div></div><p>The <a class="xref" href="#copytable" title="15.1.8.&nbsp;CopyTable">Section&nbsp;15.1.8, &#8220;CopyTable&#8221;</a> utility could either be used to copy data from one table to another on the
      same cluster, or to copy data to another table on another cluster.
      </p><p>Since the cluster is up, there is a risk that edits could be missed in the copy process.
      </p></div><div class="section" title="15.7.4.&nbsp;Live Cluster Backup - Export"><div class="titlepage"><div><div><h3 class="title"><a name="ops.backup.live.export"></a>15.7.4.&nbsp;Live Cluster Backup - Export</h3></div></div></div><p>The <a class="xref" href="#export" title="15.1.9.&nbsp;Export">Section&nbsp;15.1.9, &#8220;Export&#8221;</a> approach dumps the content of a table to HDFS on the same cluster.  To restore the data, the
      <a class="xref" href="#import" title="15.1.10.&nbsp;Import">Section&nbsp;15.1.10, &#8220;Import&#8221;</a> utility would be used.
      </p><p>Since the cluster is up, there is a risk that edits could be missed in the export process.
      </p></div></div><div class="section" title="15.8.&nbsp;HBase Snapshots"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="ops.snapshots"></a>15.8.&nbsp;HBase Snapshots</h2></div></div></div><p>HBase Snapshots allow you to take a snapshot of a table without too much impact on Region Servers.
      Snapshot, Clone and restore operations don't involve data copying.
      Also, Exporting the snapshot to another cluster doesn't have impact on the Region Servers.
    </p><p>Prior to version 0.94.6, the only way to backup or to clone a table is to use CopyTable/ExportTable,
      or to copy all the hfiles in HDFS after disabling the table.
      The disadvantages of these methods are that you can degrade region server performance
      (Copy/Export Table) or you need to disable the table, that means no reads or writes;
      and this is usually unacceptable.
    </p><div class="section" title="15.8.1.&nbsp;Configuration"><div class="titlepage"><div><div><h3 class="title"><a name="ops.snapshots.configuration"></a>15.8.1.&nbsp;Configuration</h3></div></div></div><p>To turn on the snapshot support just set the
        <code class="varname">hbase.snapshot.enabled</code> property to true.
        (Snapshots are enabled by default in 0.95+ and off by default in 0.94.6+)
        </p><pre class="programlisting">
  &lt;property&gt;
    &lt;name&gt;hbase.snapshot.enabled&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;
        </pre><p>
      </p></div><div class="section" title="15.8.2.&nbsp;Take a Snapshot"><div class="titlepage"><div><div><h3 class="title"><a name="ops.snapshots.takeasnapshot"></a>15.8.2.&nbsp;Take a Snapshot</h3></div></div></div><p>You can take a snapshot of a table regardless of whether it is enabled or disabled.
        The snapshot operation doesn't involve any data copying.
        </p><pre class="programlisting">
    $ ./bin/hbase shell
    hbase&gt; snapshot 'myTable', 'myTableSnapshot-122112'
        </pre><p>
      </p></div><div class="section" title="15.8.3.&nbsp;Listing Snapshots"><div class="titlepage"><div><div><h3 class="title"><a name="ops.snapshots.list"></a>15.8.3.&nbsp;Listing Snapshots</h3></div></div></div><p>List all snapshots taken (by printing the names and relative information).
        </p><pre class="programlisting">
    $ ./bin/hbase shell
    hbase&gt; list_snapshots
        </pre><p>
      </p></div><div class="section" title="15.8.4.&nbsp;Deleting Snapshots"><div class="titlepage"><div><div><h3 class="title"><a name="ops.snapshots.delete"></a>15.8.4.&nbsp;Deleting Snapshots</h3></div></div></div><p>You can remove a snapshot, and the files retained for that snapshot will be removed
        if no longer needed.
        </p><pre class="programlisting">
    $ ./bin/hbase shell
    hbase&gt; delete_snapshot 'myTableSnapshot-122112'
        </pre><p>
      </p></div><div class="section" title="15.8.5.&nbsp;Clone a table from snapshot"><div class="titlepage"><div><div><h3 class="title"><a name="ops.snapshots.clone"></a>15.8.5.&nbsp;Clone a table from snapshot</h3></div></div></div><p>From a snapshot you can create a new table (clone operation) with the same data
      that you had when the snapshot was taken.
      The clone operation, doesn't involve data copies, and a change to the cloned table
      doesn't impact the snapshot or the original table.
        </p><pre class="programlisting">
    $ ./bin/hbase shell
    hbase&gt; clone_snapshot 'myTableSnapshot-122112', 'myNewTestTable'
        </pre><p>
      </p></div><div class="section" title="15.8.6.&nbsp;Restore a snapshot"><div class="titlepage"><div><div><h3 class="title"><a name="ops.snapshots.restore"></a>15.8.6.&nbsp;Restore a snapshot</h3></div></div></div><p>The restore operation requires the table to be disabled, and the table will be
      restored to the state at the time when the snapshot was taken,
      changing both data and schema if required.
        </p><pre class="programlisting">
    $ ./bin/hbase shell
    hbase&gt; disable 'myTable'
    hbase&gt; restore_snapshot 'myTableSnapshot-122112'
        </pre><p>
      </p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>Since Replication works at log level and snapshots at file-system level,
      after a restore, the replicas will be in a different state from the master.
      If you want to use restore, you need to stop replication and redo the bootstrap.
        </p></div><p>In case of partial data-loss due to misbehaving client, instead of a full restore
      that requires the table to be disabled, you can clone the table from the snapshot
      and use a Map-Reduce job to copy the data that you need, from the clone to the main one.
      </p></div><div class="section" title="15.8.7.&nbsp;Snapshots operations and ACLs"><div class="titlepage"><div><div><h3 class="title"><a name="ops.snapshots.acls"></a>15.8.7.&nbsp;Snapshots operations and ACLs</h3></div></div></div>
    If you are using security with the AccessController Coprocessor (See <a class="xref" href="#hbase.accesscontrol.configuration" title="8.4.&nbsp;Access Control">Section&nbsp;8.4, &#8220;Access Control&#8221;</a>),
    only a global administrator can take, clone, or restore a snapshot, and these actions do not capture the ACL rights.
    This means that restoring a table preserves the ACL rights of the existing table,
    while cloning a table creates a new table that has no ACL rights until the administrator adds them.
    </div><div class="section" title="15.8.8.&nbsp;Export to another cluster"><div class="titlepage"><div><div><h3 class="title"><a name="ops.snapshots.export"></a>15.8.8.&nbsp;Export to another cluster</h3></div></div></div><p>The ExportSnapshot tool copies all the data related to a snapshot (hfiles, logs, snapshot metadata) to another cluster.
        The tool executes a Map-Reduce job, similar to distcp, to copy files between the two clusters,
        and since it works at file-system level the hbase cluster does not have to be online.
        </p><p>To copy a snapshot called MySnapshot to an HBase cluster srv2 (hdfs:///srv2:8082/hbase) using 16 mappers:
</p><pre class="programlisting">$ bin/hbase class org.apache.hadoop.hbase.snapshot.ExportSnapshot -snapshot MySnapshot -copy-to hdfs://srv2:8082/hbase -mappers 16</pre><p>
        </p><p>
      </p></div></div><div class="section" title="15.9.&nbsp;Capacity Planning and Region Sizing"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="ops.capacity"></a>15.9.&nbsp;Capacity Planning and Region Sizing</h2></div></div></div><p>There are several considerations when planning the capacity for an HBase cluster and performing the initial configuration. Start with a solid understanding of how HBase handles data internally.</p><div class="section" title="15.9.1.&nbsp;Node count and hardware/VM configuration"><div class="titlepage"><div><div><h3 class="title"><a name="ops.capacity.nodes"></a>15.9.1.&nbsp;Node count and hardware/VM configuration</h3></div></div></div><div class="section" title="15.9.1.1.&nbsp;Physical data size"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.nodes.datasize"></a>15.9.1.1.&nbsp;Physical data size</h4></div></div></div><p>Physical data size on disk is distinct from logical size of your data and is affected by the following:
</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Increased by HBase overhead
<div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">See <a class="xref" href="#keyvalue" title="9.7.6.4.&nbsp;KeyValue">Section&nbsp;9.7.6.4, &#8220;KeyValue&#8221;</a> and <a class="xref" href="#keysize" title="6.3.2.&nbsp;Try to minimize row and column sizes">Section&nbsp;6.3.2, &#8220;Try to minimize row and column sizes&#8221;</a>. At least 24 bytes per key-value (cell), can be more. Small keys/values means more relative overhead.</li><li class="listitem">KeyValue instances are aggregated into blocks, which are indexed. Indexes also have to be stored. Blocksize is configurable on a per-ColumnFamily basis. See <a class="xref" href="#regions.arch" title="9.7.&nbsp;Regions">Section&nbsp;9.7, &#8220;Regions&#8221;</a>.</li></ul></div></li><li class="listitem">Decreased by <a class="xref" href="#compression" title="Appendix&nbsp;C.&nbsp;Compression In HBase">compression</a> and data block encoding, depending on data. See also <a class="ulink" href="http://search-hadoop.com/m/lL12B1PFVhp1" target="_top">this thread</a>. You might want to test what compression and encoding (if any) make sense for your data.</li><li class="listitem">Increased by size of region server <a class="xref" href="#wal" title="9.6.5.&nbsp;Write Ahead Log (WAL)">WAL</a> (usually fixed and negligible - less than half of RS memory size, per RS).</li><li class="listitem">Increased by HDFS replication - usually x3.</li></ul></div><p>Aside from the disk space necessary to store the data, one RS may not be able to serve arbitrarily large amounts of data due to some practical limits on region count and size (see <a class="xref" href="#ops.capacity.regions" title="15.9.2.&nbsp;Determining region count and size">below</a>).</p></div><div class="section" title="15.9.1.2.&nbsp;Read/Write throughput"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.nodes.throughput"></a>15.9.1.2.&nbsp;Read/Write throughput</h4></div></div></div><p>Number of nodes can also be driven by required thoughput for reads and/or writes. The  throughput one can get per node depends a lot on data (esp. key/value sizes) and request patterns, as well as node and system configuration. Planning should be done for peak load if it is likely that the load would be the main driver of the increase of the node count. PerformanceEvaluation and <a class="xref" href="#ycsb">YCSB</a> tools can be used to test single node or a test cluster.</p><p>For write, usually 5-15Mb/s per RS can be expected, since every region server has only one active WAL. There's no good estimate for reads, as it depends vastly on data, requests, and cache hit rate. <a class="xref" href="#perf.casestudy" title="12.14.&nbsp;Case Studies">Section&nbsp;12.14, &#8220;Case Studies&#8221;</a> might be helpful.</p></div><div class="section" title="15.9.1.3.&nbsp;JVM GC limitations"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.nodes.gc"></a>15.9.1.3.&nbsp;JVM GC limitations</h4></div></div></div><p>RS cannot currently utilize very large heap due to cost of GC. There's also no good way of running multiple RS-es per server (other than running several VMs per machine). Thus, ~20-24Gb or less memory dedicated to one RS is recommended. GC tuning is required for large heap sizes. See <a class="xref" href="#gcpause" title="12.3.1.1.&nbsp;Long GC pauses">Section&nbsp;12.3.1.1, &#8220;Long GC pauses&#8221;</a>, <a class="xref" href="#trouble.log.gc" title="13.2.3.&nbsp;JVM Garbage Collection Logs">Section&nbsp;13.2.3, &#8220;JVM Garbage Collection Logs&#8221;</a> and elsewhere (TODO: where?)</p></div></div><div class="section" title="15.9.2.&nbsp;Determining region count and size"><div class="titlepage"><div><div><h3 class="title"><a name="ops.capacity.regions"></a>15.9.2.&nbsp;Determining region count and size</h3></div></div></div><p>Generally less regions makes for a smoother running cluster (you can always manually split the big regions later (if necessary) to spread the data, or request load, over the cluster); 20-200 regions per RS is a reasonable range. The number of regions cannot be configured directly (unless you go for fully <a class="xref" href="#disable.splitting" title="2.5.2.7.&nbsp;Managed Splitting">manual splitting</a>); adjust the region size to achieve the target region size given table size.</p><p>When configuring regions for multiple tables, note that most region settings can be set on a per-table basis via <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HTableDescriptor.html" target="_top">HTableDescriptor</a>, as well as shell commands. These settings will override the ones in <code class="varname">hbase-site.xml</code>. That is useful if your tables have different workloads/use cases.</p><p>Also note that in the discussion of region sizes here, <span class="bold"><strong>HDFS replication factor is not (and should not be) taken into account, whereas other factors <a class="xref" href="#ops.capacity.nodes.datasize" title="15.9.1.1.&nbsp;Physical data size">above</a> should be.</strong></span> So, if your data is compressed and replicated 3 ways by HDFS, "9 Gb region" means 9 Gb of compressed data. HDFS replication factor only affects your disk usage and is invisible to most HBase code.</p><div class="section" title="15.9.2.1.&nbsp;Number of regions per RS - upper bound"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.regions.count"></a>15.9.2.1.&nbsp;Number of regions per RS - upper bound</h4></div></div></div><p>In production scenarios, where you have a lot of data, you are normally concerned with the maximum number of regions you can have per server. <a class="xref" href="#too_many_regions" title="9.7.1.1.&nbsp;Why cannot I have too many regions?">Section&nbsp;9.7.1.1, &#8220;Why cannot I have too many regions?&#8221;</a> has technical discussion on the subject; in short, maximum number of regions is mostly determined by memstore memory usage. Each region has its own memstores; these grow up to a configurable size; usually in 128-256Mb range, see <a class="xref" href="#hbase.hregion.memstore.flush.size" title="hbase.hregion.memstore.flush.size"><code class="varname">hbase.hregion.memstore.flush.size</code></a>. There's one memstore per column family (so there's only one per region if there's one CF in the table). RS dedicates some fraction of total memory (see <a class="xref" href="#">???</a>) to region memstores. If this memory is exceeded (too much memstore usage), undesirable consequences such as unresponsive server, or later compaction storms, can result. Thus, a good starting point for the number of regions per RS (assuming one table) is </p><pre class="programlisting">(RS memory)*(total memstore fraction)/((memstore size)*(# column families))</pre><p>
E.g. if RS has 16Gb RAM, with default settings, it is 16384*0.4/128 ~ 51 regions per RS is a starting point. The formula can be extended to multiple tables; if they all have the same configuration, just use total number of families.</p><p>This number can be adjusted; the formula above assumes all your regions are filled at approximately the same rate. If only a fraction of your regions are going to be actively written to, you can divide the result by that fraction to get a larger region count. Then, even if all regions are written to, all region memstores are not filled evenly, and eventually jitter appears even if they are (due to limited number of concurrent flushes). Thus, one can have as many as 2-3 times more regions than the starting point; however, increased numbers carry increased risk.</p><p>For write-heavy workload, memstore fraction can be increased in configuration at the expense of block cache; this will also allow one to have more regions.</p></div><div class="section" title="15.9.2.2.&nbsp;Number of regions per RS - lower bound"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.regions.mincount"></a>15.9.2.2.&nbsp;Number of regions per RS - lower bound</h4></div></div></div><p>HBase scales by having regions across many servers. Thus if you have 2 regions for 16GB data, on a 20 node machine your data will be concentrated on just a few machines - nearly the entire        cluster will be idle. This really can't be stressed enough, since a common problem is loading 200MB data into HBase and then wondering why your awesome 10 node cluster isn't doing anything.</p><p>On the other hand, if you have a very large amount of data, you may also want to go for a larger number of regions to avoid having regions that are too large.</p></div><div class="section" title="15.9.2.3.&nbsp;Maximum region size"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.regions.size"></a>15.9.2.3.&nbsp;Maximum region size</h4></div></div></div><p>For large tables in production scenarios, maximum region size is mostly limited by compactions - very large compactions, esp. major, can degrade cluster performance. Currently, the recommended maximum region size is 10-20Gb, and 5-10Gb is optimal. For older 0.90.x codebase, the upper-bound of regionsize is about 4Gb, with a default of 256Mb.</p><p>The size at which the region is split into two is generally configured via <a class="xref" href="#hbase.hregion.max.filesize" title="hbase.hregion.max.filesize"><code class="varname">hbase.hregion.max.filesize</code></a>; for details, see <a class="xref" href="#arch.region.splits" title="9.7.4.&nbsp;Region Splits">Section&nbsp;9.7.4, &#8220;Region Splits&#8221;</a>.</p><p>If you cannot estimate the size of your tables well, when starting off, it's probably best to stick to the default region size, perhaps going smaller for hot tables (or manually split hot regions to spread the load over the cluster), or go with larger region sizes if your cell sizes tend to be largish (100k and up).</p><p>In HBase 0.98, experimental stripe compactions feature was added that would allow for larger regions, especially for log data. See <a class="xref" href="#ops.stripe" title="9.7.6.5.6.&nbsp;Experimental: stripe compactions">Section&nbsp;9.7.6.5.6, &#8220;Experimental: stripe compactions&#8221;</a>.</p></div><div class="section" title="15.9.2.4.&nbsp;Total data size per region server"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.regions.total"></a>15.9.2.4.&nbsp;Total data size per region server</h4></div></div></div><p>According to above numbers for region size and number of regions per region server, in an optimistic estimate 10 GB x 100 regions per RS will give up to 1TB served per region server, which is in line with some of the reported multi-PB use cases. However, it is important to think about the data vs cache size ratio at the RS level. With 1TB of data per server and 10 GB block cache, only 1% of the data will be cached, which may barely cover all block indices.</p></div></div><div class="section" title="15.9.3.&nbsp;Initial configuration and tuning"><div class="titlepage"><div><div><h3 class="title"><a name="ops.capacity.config"></a>15.9.3.&nbsp;Initial configuration and tuning</h3></div></div></div><p>First, see <a class="xref" href="#important_configurations" title="2.5.&nbsp;The Important Configurations">Section&nbsp;2.5, &#8220;The Important Configurations&#8221;</a>. Note that some configurations, more than others, depend on specific scenarios. Pay special attention to 
</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><a class="xref" href="#hbase.regionserver.handler.count" title="hbase.regionserver.handler.count"><code class="varname">hbase.regionserver.handler.count</code></a> - request handler thread count, vital for high-throughput workloads.</li><li class="listitem"><a class="xref" href="#config.wals" title="2.5.2.6.&nbsp;Configuring the size and number of WAL files">Section&nbsp;2.5.2.6, &#8220;Configuring the size and number of WAL files&#8221;</a> - the blocking number of WAL files depends on your memstore configuration and should be set accordingly to prevent potential blocking when doing high volume of writes.</li></ul></div><p>Then, there are some considerations when setting up your cluster and tables.</p><div class="section" title="15.9.3.1.&nbsp;Compactions"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.config.compactions"></a>15.9.3.1.&nbsp;Compactions</h4></div></div></div><p>Depending on read/write volume and latency requirements, optimal compaction settings may be different. See <a class="xref" href="#compaction" title="9.7.6.5.&nbsp;Compaction">Section&nbsp;9.7.6.5, &#8220;Compaction&#8221;</a> for some details.</p><p>When provisioning for large data sizes, however, it's good to keep in mind that compactions can affect write throughput. Thus, for write-intensive workloads, you may opt for less frequent compactions and more store files per regions. Minimum number of files for compactions (<code class="varname">hbase.hstore.compaction.min</code>) can be set to higher value; <a class="xref" href="#hbase.hstore.blockingStoreFiles" title="hbase.hstore.blockingStoreFiles"><code class="varname">hbase.hstore.blockingStoreFiles</code></a> should also be increased, as more files might accumulate in such case. You may also consider manually managing compactions: <a class="xref" href="#managed.compactions" title="2.5.2.8.&nbsp;Managed Compactions">Section&nbsp;2.5.2.8, &#8220;Managed Compactions&#8221;</a></p></div><div class="section" title="15.9.3.2.&nbsp;Pre-splitting the table"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.config.presplit"></a>15.9.3.2.&nbsp;Pre-splitting the table</h4></div></div></div><p>Based on the target number of the regions per RS (see <a class="xref" href="#ops.capacity.regions.count" title="15.9.2.1.&nbsp;Number of regions per RS - upper bound">above</a>) and number of RSes, one can pre-split the table at creation time. This would both avoid some costly splitting as the table starts to fill up, and ensure that the table starts out already distributed across many servers.</p><p>If the table is expected to grow large enough to justify that, at least one region per RS should be created. It is not recommended to split immediately into the full target number of regions (e.g. 50 * number of RSes), but a low intermediate value can be chosen. For multiple tables, it is recommended to be conservative with presplitting (e.g. pre-split 1 region per RS at most), especially if you don't know how much each table will grow. If you split too much, you may end up with too many regions, with some tables having too many small regions.</p><p>For pre-splitting howto, see <a class="xref" href="#precreate.regions" title="12.8.2.&nbsp; Table Creation: Pre-Creating Regions">Section&nbsp;12.8.2, &#8220;
    Table Creation: Pre-Creating Regions
    &#8221;</a>.</p></div></div></div><div class="section" title="15.10.&nbsp;Table Rename"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="table.rename"></a>15.10.&nbsp;Table Rename</h2></div></div></div><p>In versions 0.90.x of hbase and earlier, we had a simple script that would rename the hdfs
          table directory and then do an edit of the .META. table replacing all mentions of the old
          table name with the new.  The script was called <span class="command"><strong>./bin/rename_table.rb</strong></span>.
          The script was deprecated and removed mostly because it was unmaintained and the operation
          performed by the script was brutal.
      </p><p>
          As of hbase 0.94.x, you can use the snapshot facility renaming a table.  Here is how you would
do it using the hbase shell:
</p><pre class="programlisting">hbase shell&gt; disable 'tableName'
hbase shell&gt; snapshot 'tableName', 'tableSnapshot'
hbase shell&gt; clone_snapshot 'tableSnapshot', 'newTableName'
hbase shell&gt; delete_snapshot 'tableSnapshot'
hbase shell&gt; drop 'tableName'</pre><p>
or in code it would be as follows:
</p><pre class="programlisting">void rename(HBaseAdmin admin, String oldTableName, String newTableName) {
    String snapshotName = randomName();
    admin.disableTable(oldTableName);
    admin.snapshot(snapshotName, oldTableName);
    admin.cloneSnapshot(snapshotName, newTableName);
    admin.deleteSnapshot(snapshotName);
    admin.deleteTable(oldTableName);
}</pre><p>
      </p></div><div class="footnotes"><br><hr width="100" align="left"><div class="footnote"><p><sup>[<a id="ftn.d366e11363" href="#d366e11363" class="para">33</a>] </sup>See
	    this <a class="link" href="http://inchoate-clatter.blogspot.com/2012/03/hbase-ops-automation.html" target="_top">blog
            post</a> for more details.</p></div></div></div><div class="chapter" title="Chapter&nbsp;16.&nbsp;Building and Developing Apache HBase"><div class="titlepage"><div><div><h2 class="title"><a name="developer"></a>Chapter&nbsp;16.&nbsp;Building and Developing Apache HBase</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#repos">16.1. Apache HBase Repositories</a></span></dt><dd><dl><dt><span class="section"><a href="#svn">16.1.1. SVN</a></span></dt><dt><span class="section"><a href="#git">16.1.2. Git</a></span></dt></dl></dd><dt><span class="section"><a href="#ides">16.2. IDEs</a></span></dt><dd><dl><dt><span class="section"><a href="#eclipse">16.2.1. Eclipse</a></span></dt></dl></dd><dt><span class="section"><a href="#build">16.3. Building Apache HBase</a></span></dt><dd><dl><dt><span class="section"><a href="#build.basic">16.3.1. Basic Compile</a></span></dt><dt><span class="section"><a href="#build.protobuf">16.3.2. Build Protobuf</a></span></dt><dt><span class="section"><a href="#build.gotchas">16.3.3. Build Gotchas</a></span></dt><dt><span class="section"><a href="#build.snappy">16.3.4. Building in snappy compression support</a></span></dt></dl></dd><dt><span class="section"><a href="#releasing">16.4. Releasing Apache HBase</a></span></dt><dd><dl><dt><span class="section"><a href="#d366e12336">16.4.1. Building against HBase 0.96-0.98</a></span></dt><dt><span class="section"><a href="#maven.release">16.4.2. Making a Release Candidate</a></span></dt><dt><span class="section"><a href="#maven.snapshot">16.4.3. Publishing a SNAPSHOT to maven</a></span></dt></dl></dd><dt><span class="section"><a href="#documentation">16.5. Generating the HBase Reference Guide</a></span></dt><dt><span class="section"><a href="#hbase.org">16.6. Updating hbase.apache.org</a></span></dt><dd><dl><dt><span class="section"><a href="#hbase.org.site.contributing">16.6.1. Contributing to hbase.apache.org</a></span></dt><dt><span class="section"><a href="#hbase.org.site.publishing">16.6.2. Publishing hbase.apache.org</a></span></dt></dl></dd><dt><span class="section"><a href="#hbase.tests">16.7. Tests</a></span></dt><dd><dl><dt><span class="section"><a href="#hbase.moduletests">16.7.1. Apache HBase Modules</a></span></dt><dt><span class="section"><a href="#hbase.unittests">16.7.2. Unit Tests</a></span></dt><dt><span class="section"><a href="#hbase.unittests.cmds">16.7.3. Running tests</a></span></dt><dt><span class="section"><a href="#hbase.tests.writing">16.7.4. Writing Tests</a></span></dt><dt><span class="section"><a href="#integration.tests">16.7.5. Integration Tests</a></span></dt></dl></dd><dt><span class="section"><a href="#maven.build.commands">16.8. Maven Build Commands</a></span></dt><dd><dl><dt><span class="section"><a href="#maven.build.commands.compile">16.8.1. Compile</a></span></dt><dt><span class="section"><a href="#maven.build.commands.unitall">16.8.2. Running all or individual Unit Tests</a></span></dt><dt><span class="section"><a href="#maven.build.hadoop">16.8.3. Building against various hadoop versions.</a></span></dt></dl></dd><dt><span class="section"><a href="#getting.involved">16.9. Getting Involved</a></span></dt><dd><dl><dt><span class="section"><a href="#mailing.list">16.9.1. Mailing Lists</a></span></dt><dt><span class="section"><a href="#jira">16.9.2. Jira</a></span></dt></dl></dd><dt><span class="section"><a href="#developing">16.10. Developing</a></span></dt><dd><dl><dt><span class="section"><a href="#codelines">16.10.1. Codelines</a></span></dt><dt><span class="section"><a href="#unit.tests">16.10.2. Unit Tests</a></span></dt><dt><span class="section"><a href="#code.standards">16.10.3. Code Standards</a></span></dt><dt><span class="section"><a href="#design.invariants">16.10.4. Invariants</a></span></dt><dt><span class="section"><a href="#run.insitu">16.10.5. Running In-Situ</a></span></dt><dt><span class="section"><a href="#add.metrics">16.10.6. Adding Metrics</a></span></dt></dl></dd><dt><span class="section"><a href="#submitting.patches">16.11. Submitting Patches</a></span></dt><dd><dl><dt><span class="section"><a href="#submitting.patches.create">16.11.1. Create Patch</a></span></dt><dt><span class="section"><a href="#submitting.patches.naming">16.11.2. Patch File Naming</a></span></dt><dt><span class="section"><a href="#submitting.patches.tests">16.11.3. Unit Tests</a></span></dt><dt><span class="section"><a href="#submitting.patches.jira">16.11.4. Attach Patch to Jira</a></span></dt><dt><span class="section"><a href="#common.patch.feedback">16.11.5. Common Patch Feedback</a></span></dt><dt><span class="section"><a href="#d366e13575">16.11.6. Submitting a patch again</a></span></dt><dt><span class="section"><a href="#d366e13596">16.11.7. Submitting incremental patches</a></span></dt><dt><span class="section"><a href="#reviewboard">16.11.8. ReviewBoard</a></span></dt><dt><span class="section"><a href="#committing.patches">16.11.9. Committing Patches</a></span></dt></dl></dd></dl></div><p>This chapter will be of interest only to those building and developing Apache HBase (i.e., as opposed to
    just downloading the latest distribution).
    </p><div class="section" title="16.1.&nbsp;Apache HBase Repositories"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="repos"></a>16.1.&nbsp;Apache HBase Repositories</h2></div></div></div><p>There are two different repositories for Apache HBase: Subversion (SVN) and Git. The former
          is the system of record for committers, but the latter is easier to work with to build and contribute.
          SVN updates get automatically propagated to the Git repo.</p><div class="section" title="16.1.1.&nbsp;SVN"><div class="titlepage"><div><div><h3 class="title"><a name="svn"></a>16.1.1.&nbsp;SVN</h3></div></div></div><pre class="programlisting">
svn co http://svn.apache.org/repos/asf/hbase/trunk hbase-core-trunk
        </pre></div><div class="section" title="16.1.2.&nbsp;Git"><div class="titlepage"><div><div><h3 class="title"><a name="git"></a>16.1.2.&nbsp;Git</h3></div></div></div><pre class="programlisting">
git clone git://git.apache.org/hbase.git
        </pre><p>
              There is also a github repository that mirrors Apache git repository.  If you'd like to develop within github environment (collaborating, pull requests) you can get the source code by:
              </p><pre class="programlisting">
git clone git://github.com/apache/hbase.git
              </pre><p>
          </p></div></div><div class="section" title="16.2.&nbsp;IDEs"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="ides"></a>16.2.&nbsp;IDEs</h2></div></div></div><div class="section" title="16.2.1.&nbsp;Eclipse"><div class="titlepage"><div><div><h3 class="title"><a name="eclipse"></a>16.2.1.&nbsp;Eclipse</h3></div></div></div><div class="section" title="16.2.1.1.&nbsp;Code Formatting"><div class="titlepage"><div><div><h4 class="title"><a name="eclipse.code.formatting"></a>16.2.1.1.&nbsp;Code Formatting</h4></div></div></div><p>Under the <code class="filename">dev-support</code> folder, you will find <code class="filename">hbase_eclipse_formatter.xml</code>.
            We encourage you to have this formatter in place in eclipse when editing HBase code.  To load it into eclipse:
</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Go to Eclipse-&gt;Preferences...</p></li><li class="listitem"><p>In Preferences, Go to Java-&gt;Code Style-&gt;Formatter</p></li><li class="listitem"><p>Import... <code class="filename">hbase_eclipse_formatter.xml</code></p></li><li class="listitem"><p>Click Apply</p></li><li class="listitem"><p>Still in Preferences, Go to Java-&gt;Editor-&gt;Save Actions</p></li><li class="listitem"><p>Check the following:
</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p>Perform the selected actions on save</p></li><li class="listitem"><p>Format source code</p></li><li class="listitem"><p>Format edited lines</p></li></ol></div><p>
</p></li><li class="listitem"><p>Click Apply</p></li></ol></div><p>
</p><p>In addition to the automatic formatting, make sure you follow the style guidelines explained in <a class="xref" href="#common.patch.feedback" title="16.11.5.&nbsp;Common Patch Feedback">Section&nbsp;16.11.5, &#8220;Common Patch Feedback&#8221;</a></p><p>Also, no @author tags - that's a rule.  Quality Javadoc comments are appreciated.  And include the Apache license.</p></div><div class="section" title="16.2.1.2.&nbsp;Subversive Plugin"><div class="titlepage"><div><div><h4 class="title"><a name="eclipse.svn"></a>16.2.1.2.&nbsp;Subversive Plugin</h4></div></div></div><p>Download and install the Subversive plugin.</p><p>Set up an SVN Repository target from <a class="xref" href="#svn" title="16.1.1.&nbsp;SVN">Section&nbsp;16.1.1, &#8220;SVN&#8221;</a>, then check out the code.</p></div><div class="section" title="16.2.1.3.&nbsp;Git Plugin"><div class="titlepage"><div><div><h4 class="title"><a name="eclipse.git.plugin"></a>16.2.1.3.&nbsp;Git Plugin</h4></div></div></div><p>If you cloned the project via git, download and install the Git plugin (EGit). Attach to your local git repo (via the Git Repositories window) and you'll be able to see file revision history, generate patches, etc.</p></div><div class="section" title="16.2.1.4.&nbsp;HBase Project Setup in Eclipse"><div class="titlepage"><div><div><h4 class="title"><a name="eclipse.maven.setup"></a>16.2.1.4.&nbsp;HBase Project Setup in Eclipse</h4></div></div></div><p>The easiest way is to use the m2eclipse plugin for Eclipse. Eclipse Indigo or newer has m2eclipse built-in, or it can be found here:http://www.eclipse.org/m2e/. M2Eclipse provides Maven integration for Eclipse - it even lets you use the direct Maven commands from within Eclipse to compile and test your project.</p><p>To import the project, you merely need to go to File-&gt;Import...Maven-&gt;Existing Maven Projects and then point Eclipse at the HBase root directory; m2eclipse will automatically find all the hbase modules for you.</p><p>If you install m2eclipse and import HBase in your workspace, you will have to fix your eclipse Build Path.
            Remove <code class="filename">target</code> folder, add <code class="filename">target/generated-jamon</code>
            and <code class="filename">target/generated-sources/java</code> folders. You may also remove from your Build Path
            the exclusions on the <code class="filename">src/main/resources</code> and <code class="filename">src/test/resources</code>
            to avoid error message in the console 'Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.6:run (default) on project hbase:
            'An Ant BuildException has occured: Replace: source file .../target/classes/hbase-default.xml doesn't exist'. This will also
            reduce the eclipse build cycles and make your life easier when developing.</p></div><div class="section" title="16.2.1.5.&nbsp;Import into eclipse with the command line"><div class="titlepage"><div><div><h4 class="title"><a name="eclipse.commandline"></a>16.2.1.5.&nbsp;Import into eclipse with the command line</h4></div></div></div><p>For those not inclined to use m2eclipse, you can generate the Eclipse files from the command line. First, run (you should only have to do this once):
            </p><pre class="programlisting">mvn clean install -DskipTests</pre><p>
            and then close Eclipse and execute...
            </p><pre class="programlisting">mvn eclipse:eclipse</pre><p>
            ... from your local HBase project directory in your workspace to generate some new <code class="filename">.project</code>
            and <code class="filename">.classpath</code>files.  Then reopen Eclipse, or refresh your eclipse project (F5), and import
            the .project file in the HBase directory to a workspace.
            </p></div><div class="section" title="16.2.1.6.&nbsp;Maven Classpath Variable"><div class="titlepage"><div><div><h4 class="title"><a name="eclipse.maven.class"></a>16.2.1.6.&nbsp;Maven Classpath Variable</h4></div></div></div><p>The <code class="varname">M2_REPO</code> classpath variable needs to be set up for the project.  This needs to be set to
            your local Maven repository, which is usually <code class="filename">~/.m2/repository</code></p>
            If this classpath variable is not configured, you will see compile errors in Eclipse like this...
            <pre class="programlisting">
Description	Resource	Path	Location	Type
The project cannot be built until build path errors are resolved	hbase		Unknown	Java Problem
Unbound classpath variable: 'M2_REPO/asm/asm/3.1/asm-3.1.jar' in project 'hbase'	hbase		Build path	Build Path Problem
Unbound classpath variable: 'M2_REPO/com/google/guava/guava/r09/guava-r09.jar' in project 'hbase'	hbase		Build path	Build Path Problem
Unbound classpath variable: 'M2_REPO/com/google/protobuf/protobuf-java/2.3.0/protobuf-java-2.3.0.jar' in project 'hbase'	hbase		Build path	Build Path Problem Unbound classpath variable:
            </pre></div><div class="section" title="16.2.1.7.&nbsp;Eclipse Known Issues"><div class="titlepage"><div><div><h4 class="title"><a name="eclipse.issues"></a>16.2.1.7.&nbsp;Eclipse Known Issues</h4></div></div></div><p>Eclipse will currently complain about <code class="filename">Bytes.java</code>.  It is not possible to turn these errors off.</p><pre class="programlisting">
Description	Resource	Path	Location	Type
Access restriction: The method arrayBaseOffset(Class) from the type Unsafe is not accessible due to restriction on required library /System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Classes/classes.jar	Bytes.java	/hbase/src/main/java/org/apache/hadoop/hbase/util	line 1061	Java Problem
Access restriction: The method arrayIndexScale(Class) from the type Unsafe is not accessible due to restriction on required library /System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Classes/classes.jar	Bytes.java	/hbase/src/main/java/org/apache/hadoop/hbase/util	line 1064	Java Problem
Access restriction: The method getLong(Object, long) from the type Unsafe is not accessible due to restriction on required library /System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Classes/classes.jar	Bytes.java	/hbase/src/main/java/org/apache/hadoop/hbase/util	line 1111	Java Problem
             </pre></div><div class="section" title="16.2.1.8.&nbsp;Eclipse - More Information"><div class="titlepage"><div><div><h4 class="title"><a name="eclipse.more"></a>16.2.1.8.&nbsp;Eclipse - More Information</h4></div></div></div><p>For additional information on setting up Eclipse for HBase development on Windows, see
             <a class="link" href="http://michaelmorello.blogspot.com/2011/09/hbase-subversion-eclipse-windows.html" target="_top">Michael Morello's blog</a> on the topic.
             </p></div></div></div><div class="section" title="16.3.&nbsp;Building Apache HBase"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="build"></a>16.3.&nbsp;Building Apache HBase</h2></div></div></div><div class="section" title="16.3.1.&nbsp;Basic Compile"><div class="titlepage"><div><div><h3 class="title"><a name="build.basic"></a>16.3.1.&nbsp;Basic Compile</h3></div></div></div><p>Thanks to maven, building HBase is pretty easy. You can read about the various maven commands in <a class="xref" href="#maven.build.commands" title="16.8.&nbsp;Maven Build Commands">Section&nbsp;16.8, &#8220;Maven Build Commands&#8221;</a>,
           but the simplest command to compile HBase from its java source code is:
       </p><pre class="programlisting">
mvn package -DskipTests
       </pre><p>
       Or, to clean up before compiling:
       </p><pre class="programlisting">
mvn clean package -DskipTests
       </pre><p>
       With Eclipse set up as explained above in <a class="xref" href="#eclipse" title="16.2.1.&nbsp;Eclipse">Section&nbsp;16.2.1, &#8220;Eclipse&#8221;</a>, you can also simply use the build command in Eclipse.
       To create the full installable HBase package takes a little bit more work, so read on.
       </p></div><div class="section" title="16.3.2.&nbsp;Build Protobuf"><div class="titlepage"><div><div><h3 class="title"><a name="build.protobuf"></a>16.3.2.&nbsp;Build Protobuf</h3></div></div></div><p>You may need to change the protobuf definitions that reside in the hbase-protocol module or other modules.</p><p>
              The protobuf files are located in <a class="link" href="https://svn.apache.org/repos/asf/hbase/trunk/hbase-protocol/src/main/protobuf" target="_top">hbase-protocol/src/main/protobuf</a>.
              For the change to be effective, you will need to regenerate the classes. You can use maven profile compile-protobuf to do this.
            </p><pre class="programlisting">
mvn compile -Dcompile-protobuf
or
mvn compile -Pcompile-protobuf
            </pre><p>

You may also want to define protoc.path for the protoc binary
             </p><pre class="programlisting">
mvn compile -Dcompile-protobuf -Dprotoc.path=/opt/local/bin/protoc
             </pre><p>
 Read the <a class="link" href="https://svn.apache.org/repos/asf/hbase/trunk/hbase-protocol/README.txt" target="_top">hbase-protocol/README.txt</a> for more details.
           </p></div><div class="section" title="16.3.3.&nbsp;Build Gotchas"><div class="titlepage"><div><div><h3 class="title"><a name="build.gotchas"></a>16.3.3.&nbsp;Build Gotchas</h3></div></div></div><p>If you see <code class="code">Unable to find resource 'VM_global_library.vm'</code>, ignore it.
			Its not an error.  It is <a class="link" href="http://jira.codehaus.org/browse/MSITE-286" target="_top">officially ugly</a> though.
           </p></div><div class="section" title="16.3.4.&nbsp;Building in snappy compression support"><div class="titlepage"><div><div><h3 class="title"><a name="build.snappy"></a>16.3.4.&nbsp;Building in snappy compression support</h3></div></div></div><p>Pass <code class="code">-Dsnappy</code> to trigger the snappy maven profile for building
            snappy native libs into hbase.  See also <a class="xref" href="#snappy.compression" title="C.5.&nbsp; SNAPPY">Section&nbsp;C.5, &#8220;
    SNAPPY
    &#8221;</a></p></div></div><div class="section" title="16.4.&nbsp;Releasing Apache HBase"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="releasing"></a>16.4.&nbsp;Releasing Apache HBase</h2></div></div></div><p>HBase 0.96.x will run on hadoop 1.x or hadoop 2.x.  HBase 0.98 will run on
             both also (but HBase 0.98 deprecates use of hadoop 1).   HBase 1.x will NOT
             run on hadoop 1. In what follows, we make a distinction between HBase 1.x
             builds and the awkward process involved building HBase 0.96/0.98 for either
             hadoop 1 or hadoop 2 targets. 
             </p><div class="section" title="16.4.1.&nbsp;Building against HBase 0.96-0.98"><div class="titlepage"><div><div><h3 class="title"><a name="d366e12336"></a>16.4.1.&nbsp;Building against HBase 0.96-0.98</h3></div></div></div><p>Building 0.98 and 0.96, you must choose which hadoop to build against;
             we cannot make a single HBase binary that can run against both hadoop1 and
             hadoop2. Since we include the Hadoop we were built
             against -- so we can do standalone mode -- the set of modules included
             in the tarball changes dependent on whether the hadoop1 or hadoop2 target
             is chosen.  You can tell which HBase you have -- whether it is for hadoop1
             or hadoop2 by looking at the version; the HBase for hadoop1 bundle will
             include 'hadoop1' in its version.  Ditto for hadoop2.
         </p><p>Maven, our build system, natively will not let you have a single product
             built against different dependencies.  It is understandable.  But neither could
             we convince maven to change the set of included modules and write out
             the correct poms w/ appropriate dependencies even though we have two
             build targets; one for hadoop1 and another for hadoop2.  So, there is a prestep
             required.  This prestep takes as input the current pom.xmls and it generates hadoop1 or
             hadoop2 versions using a script in <code class="filename">dev-tools</code> called
             <code class="filename">generate-hadoopX-poms.sh</code>.  You then reference these generated
             poms when you build. For now, just be aware of the difference between HBase 1.x
             builds and those of HBase 0.96-0.98. Below we will come back to this difference
             when we list out build instructions.</p></div><p><a name="mvn.settings.file"></a>Publishing to maven requires you sign the artifacts you want to upload.  To have the
         build do this for you, you need to make sure you have a properly configured
         <code class="filename">settings.xml</code> in your local repository under <code class="filename">.m2</code>.
            Here is my <code class="filename">~/.m2/settings.xml</code>.
        </p><pre class="programlisting">&lt;settings xmlns="http://maven.apache.org/SETTINGS/1.0.0"
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xsi:schemaLocation="http://maven.apache.org/SETTINGS/1.0.0
                      http://maven.apache.org/xsd/settings-1.0.0.xsd"&gt;
  &lt;servers&gt;
    &lt;!- To publish a snapshot of some part of Maven --&gt;
    &lt;server&gt;
      &lt;id&gt;apache.snapshots.https&lt;/id&gt;
      &lt;username&gt;YOUR_APACHE_ID
      &lt;/username&gt;
      &lt;password&gt;YOUR_APACHE_PASSWORD
      &lt;/password&gt;
    &lt;/server&gt;
    &lt;!-- To publish a website using Maven --&gt;
    &lt;!-- To stage a release of some part of Maven --&gt;
    &lt;server&gt;
      &lt;id&gt;apache.releases.https&lt;/id&gt;
      &lt;username&gt;YOUR_APACHE_ID
      &lt;/username&gt;
      &lt;password&gt;YOUR_APACHE_PASSWORD
      &lt;/password&gt;
    &lt;/server&gt;
  &lt;/servers&gt;
  &lt;profiles&gt;
    &lt;profile&gt;
      &lt;id&gt;apache-release&lt;/id&gt;
      &lt;properties&gt;
    &lt;gpg.keyname&gt;YOUR_KEYNAME&lt;/gpg.keyname&gt;
    &lt;!--Keyname is something like this ... 00A5F21E... do gpg --list-keys to find it--&gt;
    &lt;gpg.passphrase&gt;YOUR_KEY_PASSWORD
    &lt;/gpg.passphrase&gt;
      &lt;/properties&gt;
    &lt;/profile&gt;
  &lt;/profiles&gt;
&lt;/settings&gt;
        </pre><p>
        </p><p>You must use maven 3.0.x (Check by running <span class="command"><strong>mvn -version</strong></span>).
        </p><div class="section" title="16.4.2.&nbsp;Making a Release Candidate"><div class="titlepage"><div><div><h3 class="title"><a name="maven.release"></a>16.4.2.&nbsp;Making a Release Candidate</h3></div></div></div><p>I'll explain by running through the process.  See later in this section for more detail on particular steps.
These instructions are for building HBase 1.0.x.  For building earlier versions, the process is different.  See this section
under the respective release documentation folders.
         </p><p>If you are making a point release (for example to quickly address a critical incompatability or security
             problem) off of a release branch instead of a development branch the tagging instructions are slightly different.
             I'll prefix those special steps with <span class="emphasis"><em>Point Release Only</em></span>.
	 </p><p>I would advise before you go about making a release candidate, do a practise run by deploying a SNAPSHOT.
             Also, make sure builds have been passing recently for the branch from where you are going to take your
             release.  You should also have tried recent branch tips out on a cluster under load running for instance
             our hbase-it integration test suite for a few hours to 'burn in' the near-candidate bits.
         </p><div class="note" title="Point Release Only" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Point Release Only</h3><p>At this point you should make svn copy of the previous release branch (ex: 0.96.1) with
             the new point release tag (e.g. 0.96.1.1 tag).  Any commits with changes or mentioned below for the point release
	     should be appled to the new tag. 
	 </p><pre class="programlisting">
$ svn copy http://svn.apache.org/repos/asf/hbase/tags/0.96.1 http://svn.apache.org/repos/asf/hbase/tags/0.96.1.1
$ svn checkout http://svn.apache.org/repos/asf/hbase/tags/0.96.1.1
	 </pre></div><p>The <a class="link" href="http://wiki.apache.org/hadoop/HowToRelease" target="_top">Hadoop How To Release</a> wiki
         page informs much of the below and may have more detail on particular sections so it is worth review.</p><p>Update CHANGES.txt with the changes since the last release.
            Make sure the URL to the JIRA points to the properly location listing fixes for this release.
            Adjust the version in all the poms appropriately.  If you are making a release candidate, you must
            remove the <span class="emphasis"><em>-SNAPSHOT</em></span> from all versions.  If you are running this receipe to
            publish a SNAPSHOT, you must keep the <span class="emphasis"><em>-SNAPSHOT</em></span> suffix on the hbase version.
             The <a class="link" href="http://mojo.codehaus.org/versions-maven-plugin/" target="_top">Versions Maven Plugin</a> can be of use here.  To
             set a version in all the many poms of the hbase multi-module project, do something like this:
             </p><pre class="programlisting">$ mvn clean org.codehaus.mojo:versions-maven-plugin:1.3.1:set -DnewVersion=0.96.0</pre><p>
             Checkin the <code class="filename">CHANGES.txt</code> and any version changes.
        </p><p>
            Update the documentation under <code class="filename">src/main/docbkx</code>.  This usually involves copying the
            latest from trunk making version-particular adjustments to suit this release candidate version.
        </p><p>Now, build the src tarball.  This tarball is hadoop version independent.  It is just the pure src code and documentation without a particular hadoop taint, etc.
            Add the <code class="varname">-Prelease</code> profile when building; it checks files for licenses and will fail the build if unlicensed files present.
            </p><pre class="programlisting">$ MAVEN_OPTS="-Xmx2g" mvn clean install -DskipTests assembly:single -Dassembly.file=hbase-assembly/src/main/assembly/src.xml -Prelease</pre><p>
            Undo the tarball and make sure it looks good.  A good test for the src tarball being 'complete' is to see if
            you can build new tarballs from this source bundle.
            If the source tarball is good, save it off to a <span class="emphasis"><em>version directory</em></span>, i.e a directory somewhere where you are collecting
            all of the tarballs you will publish as part of the release candidate.  For example if we were building a
            hbase-0.96.0 release candidate, we might call the directory <code class="filename">hbase-0.96.0RC0</code>.  Later
            we will publish this directory as our release candidate up on people.apache.org/~YOU.
        </p><p>Now lets build the binary tarball.
            Add the <code class="varname">-Prelease</code> profile when building; it checks files for licenses and will fail the build if unlicensed files present.
            Do it in two steps. First install into the local repository and then generate documentation and assemble the tarball
            (Otherwise build complains that hbase modules are not in maven repo when we try to do it all in the one go especially on fresh repo).
            It seems that you need the install goal in both steps.
            </p><pre class="programlisting">$ MAVEN_OPTS="-Xmx3g" mvn clean install -DskipTests -Prelease
$ MAVEN_OPTS="-Xmx3g" mvn install -DskipTests site assembly:single -Prelease</pre><p>
Undo the generated tarball and check it out.  Look at doc. and see if it runs, etc.
If good, copy the tarball to the above mentioned <span class="emphasis"><em>version directory</em></span>.
</p><div class="note" title="Point Release Only" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Point Release Only</h3><p>The following step that creates a new tag can be skipped since you've already created the point release tag</p></div><p>I'll tag the release at this point since its looking good.  If we find an issue later, we can delete the tag and start over.  Release needs to be tagged when we do next step.</p><p>Now deploy hbase to the apache maven repository.
This time we use the <code class="varname">apache-release</code> profile instead of just <code class="varname">release</code> profile when doing mvn deploy;
it will invoke the apache pom referenced by our poms.  It will also sign your artifacts published to mvn as long as your settings.xml in your local <code class="filename">.m2</code>
repository is configured correctly (your <code class="filename">settings.xml</code> adds your gpg password property to the apache profile).
</p><pre class="programlisting">$ MAVEN_OPTS="-Xmx3g" mvn deploy -DskipTests -Papache-release</pre><p>
The last command above copies all artifacts up to a temporary staging apache mvn repo in an 'open' state.
We'll need to do more work on these maven artifacts to make them generally available.
</p><p>The script <code class="filename">dev-support/make_rc.sh</code> automates alot of the above listed release steps.
             It does not do the modification of the CHANGES.txt for the release, the close of the
             staging repository up in apache maven (human intervention is needed here), the checking of
             the produced artifacts to ensure they are 'good' -- e.g.  undoing the produced tarballs, eyeballing them to make
             sure they look right then starting and checking all is running properly --  and then the signing and pushing of
             the tarballs to people.apache.org but it does the other stuff; it can come in handy.
            </p><p>Now lets get back to what is up in maven. Our artifacts should be up in maven repository in the staging area 
in the 'open' state.  While in this 'open' state you can check out what you've published to make sure all is good.
To do this, login at repository.apache.org
using your apache id.  Find your artifacts in the staging repository.  Browse the content.  Make sure all artifacts made it up
and that the poms look generally good.  If it checks out, 'close' the repo.  This will make the artifacts publically available.
You will receive an email with the URL to give out for the temporary staging repository for others to use trying out this new
release candidate.  Include it in the email that announces the release candidate. Folks will need to add this repo URL to their
local poms or to their local settings.xml file to pull the published release candidate artifacts.  If the published artifacts are incomplete
or borked, just delete the 'open' staged artifacts.
</p><div class="note" title="hbase-downstreamer" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">hbase-downstreamer</h3><p>
        See the <a class="link" href="https://github.com/saintstack/hbase-downstreamer" target="_top">hbase-downstreamer</a> test for a simple
        example of a project that is downstream of hbase an depends on it.
        Check it out and run its simple test to make sure maven artifacts are properly deployed to the maven repository.
        Be sure to edit the pom to point at the proper staging repo.  Make sure you are pulling from the repo when tests run and that you are not
        getting from your local repo (pass -U or delete your local repo content and check maven is pulling from remote out of the staging repo).
    </p></div><p>
            See <a class="link" href="http://www.apache.org/dev/publishing-maven-artifacts.html" target="_top">Publishing Maven Artifacts</a> for
            some pointers on this maven staging process.
            </p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>We no longer publish using the maven release plugin.  Instead we do mvn deploy.  It seems to give
                    us a backdoor to maven release publishing.  If no <span class="emphasis"><em>-SNAPSHOT</em></span> on the version
                    string, then we are 'deployed' to the apache maven repository staging directory from which we
                    can publish URLs for candidates and later, if they pass, publish as release (if a
                    <span class="emphasis"><em>-SNAPSHOT</em></span> on the version string, deploy will put the artifacts up into
                    apache snapshot repos).
                </p></div><p>
</p><p>If the hbase version ends in <code class="varname">-SNAPSHOT</code>, the artifacts go elsewhere.  They are put into the apache snapshots repository
    directly and are immediately available.  Making a SNAPSHOT release, this is what you want to happen.</p><p>
            At this stage we have two tarballs in our 'version directory' and a set of artifacts up in maven in staging area in the
            'closed' state publically available in a temporary staging repository whose URL you should have gotten in an email.
        The above mentioned script, <code class="filename">make_rc.sh</code> does all of the above for you minus the check of the artifacts built,
        the closing of the staging repository up in maven, and the tagging of the release.  If you run the script, do your checks at this
        stage verifying the src and bin tarballs and checking what is up in staging using hbase-downstreamer project. Tag before you start
        the build.  You can always delete it if the build goes haywire.
    </p><p>
        If all checks out, next put the <span class="emphasis"><em>version directory</em></span> up on people.apache.org.  You will need to sign and fingerprint them before you
        push them up. In the <span class="emphasis"><em>version directory</em></span> do this:
        </p><pre class="programlisting">$ for i in *.tar.gz; do echo $i; gpg --print-mds $i &gt; $i.mds ; done
$ for i in *.tar.gz; do echo $i; gpg --armor --output $i.asc --detach-sig $i  ; done
$ cd ..
# Presuming our 'version directory' is named 0.96.0RC0, now copy it up to people.apache.org.
$ rsync -av 0.96.0RC0 people.apache.org:public_html
        </pre><p>
        </p><p>Make sure the people.apache.org directory is showing and that the
            mvn repo urls are good.
            Announce the release candidate on the mailing list and call a vote.
        </p></div><div class="section" title="16.4.3.&nbsp;Publishing a SNAPSHOT to maven"><div class="titlepage"><div><div><h3 class="title"><a name="maven.snapshot"></a>16.4.3.&nbsp;Publishing a SNAPSHOT to maven</h3></div></div></div><p>Make sure your <code class="filename">settings.xml</code> is set up properly (see above for how).
              Make sure the hbase version includes <code class="varname">-SNAPSHOT</code> as a suffix.  Here is how I published SNAPSHOTS of
              a release that had an hbase version of 0.96.0 in its poms.
          </p><pre class="programlisting">$ MAVEN_OPTS="-Xmx3g" mvn clean install -DskipTests  javadoc:aggregate site assembly:single -Prelease
 $ MAVEN_OPTS="-Xmx3g" mvn -DskipTests  deploy -Papache-release</pre><p>
</p><p>The <code class="filename">make_rc.sh</code> script mentioned above in the
    (see <a class="xref" href="#maven.release" title="16.4.2.&nbsp;Making a Release Candidate">Section&nbsp;16.4.2, &#8220;Making a Release Candidate&#8221;</a>) can help you publish <code class="varname">SNAPSHOTS</code>.
    Make sure your hbase.version has a <code class="varname">-SNAPSHOT</code> suffix and then run
    the script.  It will put a snapshot up into the apache snapshot repository for you.
</p></div></div><div class="section" title="16.5.&nbsp;Generating the HBase Reference Guide"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="documentation"></a>16.5.&nbsp;Generating the HBase Reference Guide</h2></div></div></div><p>The manual is marked up using <a class="link" href="http://www.docbook.org/" target="_top">docbook</a>.
              We then use the <a class="link" href="http://code.google.com/p/docbkx-tools/" target="_top">docbkx maven plugin</a>
              to transform the markup to html.  This plugin is run when you specify the <span class="command"><strong>site</strong></span>
              goal as in when you run <span class="command"><strong>mvn site</strong></span> or you can call the plugin explicitly to
              just generate the manual by doing <span class="command"><strong>mvn docbkx:generate-html</strong></span>
              (TODO: It looks like you have to run <span class="command"><strong>mvn site</strong></span> first because docbkx wants to
              include a transformed <code class="filename">hbase-default.xml</code>.  Fix).
              When you run mvn site, we do the document generation twice, once to generate the multipage
              manual and then again for the single page manual (the single page version is easier to search).
          </p></div><div class="section" title="16.6.&nbsp;Updating hbase.apache.org"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="hbase.org"></a>16.6.&nbsp;Updating hbase.apache.org</h2></div></div></div><div class="section" title="16.6.1.&nbsp;Contributing to hbase.apache.org"><div class="titlepage"><div><div><h3 class="title"><a name="hbase.org.site.contributing"></a>16.6.1.&nbsp;Contributing to hbase.apache.org</h3></div></div></div><p>The Apache HBase apache web site (including this reference guide) is maintained as part of the main
          Apache HBase source tree, under <code class="filename">/src/main/docbkx</code> and <code class="filename">/src/main/site</code>
          <sup>[<a name="d366e12583" href="#ftn.d366e12583" class="footnote">34</a>]</sup>.
          The former -- docbkx -- is this reference guide as a bunch of xml marked up using <a class="link" href="http://docbook.org" target="_top">docbook</a>;
          the latter is the hbase site (the navbars, the header, the layout, etc.),
          and some of the documentation, legacy pages mostly that are in the process of being merged into the docbkx tree that is
      converted to html by a maven plugin by the site build.</p><p>To contribute to the reference guide, edit these files under site or docbkx and submit them as a patch
          (see <a class="xref" href="#submitting.patches" title="16.11.&nbsp;Submitting Patches">Section&nbsp;16.11, &#8220;Submitting Patches&#8221;</a>). Your Jira should contain a summary of the changes in each
          section (see <a class="link" href="https://issues.apache.org/jira/browse/HBASE-6081" target="_top">HBASE-6081</a> for an example).</p><p>To generate the site locally while you're working on it, run:
      </p><pre class="programlisting">mvn site</pre><p>
      Then you can load up the generated HTML files in your browser (file are under <code class="filename">/target/site</code>).</p></div><div class="section" title="16.6.2.&nbsp;Publishing hbase.apache.org"><div class="titlepage"><div><div><h3 class="title"><a name="hbase.org.site.publishing"></a>16.6.2.&nbsp;Publishing hbase.apache.org</h3></div></div></div><p>As of <a class="link" href="https://issues.apache.org/jira/browse/INFRA-5680" target="_top">INFRA-5680 Migrate apache hbase website</a>,
          to publish the website, build it, and then deploy it over a checkout of <code class="filename">https://svn.apache.org/repos/asf/hbase/hbase.apache.org/trunk</code>.
          Finally, check it in.  For example, if trunk is checked out out at <code class="filename">/Users/stack/checkouts/trunk</code>
          and the hbase website, hbase.apache.org, is checked out at <code class="filename">/Users/stack/checkouts/hbase.apache.org/trunk</code>, to update
          the site, do the following:
          </p><pre class="programlisting">
              # Build the site and deploy it to the checked out directory
              # Getting the javadoc into site is a little tricky.  You have to build it before you invoke 'site'.
              $ MAVEN_OPTS=" -Xmx3g" mvn clean install -DskipTests javadoc:aggregate site  site:stage -DstagingDirectory=/Users/stack/checkouts/hbase.apache.org/trunk
          </pre><p>
          Now check the deployed site by viewing in a brower, browse to file:////Users/stack/checkouts/hbase.apache.org/trunk/index.html and check all is good.
          If all checks out, commit it and your new build will show up immediately at http://hbase.apache.org
          </p><pre class="programlisting">
              $ cd /Users/stack/checkouts/hbase.apache.org/trunk
              $ svn status
              # Do an svn add of any new content...
              $ svn add ....
              $ svn commit -m 'Committing latest version of website...'
          </pre><p>
      </p></div></div><div class="section" title="16.7.&nbsp;Tests"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="hbase.tests"></a>16.7.&nbsp;Tests</h2></div></div></div><p> Developers, at a minimum, should familiarize themselves with the unit test detail; unit tests in
HBase have a character not usually seen in other projects.</p><div class="section" title="16.7.1.&nbsp;Apache HBase Modules"><div class="titlepage"><div><div><h3 class="title"><a name="hbase.moduletests"></a>16.7.1.&nbsp;Apache HBase Modules</h3></div></div></div><p>As of 0.96, Apache HBase is split into multiple modules which creates "interesting" rules for
how and where tests are written. If you are writting code for <code class="classname">hbase-server</code>, see
<a class="xref" href="#hbase.unittests" title="16.7.2.&nbsp;Unit Tests">Section&nbsp;16.7.2, &#8220;Unit Tests&#8221;</a> for how to write your tests; these tests can spin
up a minicluster and will need to be categorized. For any other module, for example
<code class="classname">hbase-common</code>, the tests must be strict unit tests and just test the class
under test - no use of the HBaseTestingUtility or minicluster is allowed (or even possible
given the dependency tree).</p><div class="section" title="16.7.1.1.&nbsp;Running Tests in other Modules"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.moduletest.run"></a>16.7.1.1.&nbsp;Running Tests in other Modules</h4></div></div></div>
  If the module you are developing in has no other dependencies on other HBase modules, then
  you can cd into that module and just run:
  <pre class="programlisting">mvn test</pre>
  which will just run the tests IN THAT MODULE. If there are other dependencies on other modules,
  then you will have run the command from the ROOT HBASE DIRECTORY. This will run the tests in the other
  modules, unless you specify to skip the tests in that module. For instance, to skip the tests in the hbase-server module,
  you would run:
  <pre class="programlisting">mvn clean test -PskipServerTests</pre>
  from the top level directory to run all the tests in modules other than hbase-server. Note that you
  can specify to skip tests in multiple modules as well as just for a single module. For example, to skip
  the tests in <code class="classname">hbase-server</code> and <code class="classname">hbase-common</code>, you would run:
  <pre class="programlisting">mvn clean test -PskipServerTests -PskipCommonTests</pre><p>Also, keep in mind that if you are running tests in the <code class="classname">hbase-server</code> module you will need to
  apply the maven profiles discussed in <a class="xref" href="#hbase.unittests.cmds" title="16.7.3.&nbsp;Running tests">Section&nbsp;16.7.3, &#8220;Running tests&#8221;</a> to get the tests to run properly.</p></div></div><div class="section" title="16.7.2.&nbsp;Unit Tests"><div class="titlepage"><div><div><h3 class="title"><a name="hbase.unittests"></a>16.7.2.&nbsp;Unit Tests</h3></div></div></div><p>Apache HBase unit tests are subdivided into four categories: small, medium, large, and
integration with corresponding JUnit <a class="link" href="http://www.junit.org/node/581" target="_top">categories</a>:
<code class="classname">SmallTests</code>, <code class="classname">MediumTests</code>,
<code class="classname">LargeTests</code>, <code class="classname">IntegrationTests</code>.
JUnit categories are denoted using java annotations and look like this in your unit test code.
</p><pre class="programlisting">...
@Category(SmallTests.class)
public class TestHRegionInfo {
  @Test
  public void testCreateHRegionInfoName() throws Exception {
    // ...
  }
}</pre><p>
The above example shows how to mark a unit test as belonging to the small category.
All unit tests in HBase have a categorization.
</p><p>
The first three categories, small, medium, and large are for tests run when
you type <code class="code">$ mvn test</code>; i.e. these three categorizations are for
HBase unit tests. The integration category is for not for unit tests but for integration
tests.  These are run when you invoke <code class="code">$ mvn verify</code>.  Integration tests
are described in <a class="xref" href="#integration.tests" title="16.7.5.&nbsp;Integration Tests">Section&nbsp;16.7.5, &#8220;Integration Tests&#8221;</a> and will not be discussed further
in this section on HBase unit tests.</p><p>
Apache HBase uses a patched maven surefire plugin and maven profiles to implement
its unit test characterizations.
</p><p>Read the below to figure which annotation of the set small, medium, and large to
put on your new HBase unit test.
</p><div class="section" title="16.7.2.1.&nbsp;Small Tests"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.unittests.small"></a>16.7.2.1.&nbsp;Small Tests<a class="indexterm" name="d366e12712"></a></h4></div></div></div><p>
<span class="emphasis"><em>Small</em></span> tests are executed in a shared JVM. We put in this category all the tests that can
be executed quickly in a shared JVM.  The maximum execution time for a small test is 15 seconds,
and small tests should not use a (mini)cluster.</p></div><div class="section" title="16.7.2.2.&nbsp;Medium Tests"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.unittests.medium"></a>16.7.2.2.&nbsp;Medium Tests<a class="indexterm" name="d366e12723"></a></h4></div></div></div><p><span class="emphasis"><em>Medium</em></span> tests represent tests that must be executed
before proposing a patch. They are designed to run in less than 30 minutes altogether,
and are quite stable in their results. They are designed to last less than 50 seconds
individually. They can use a cluster, and each of them is executed in a separate JVM.
</p></div><div class="section" title="16.7.2.3.&nbsp;Large Tests"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.unittests.large"></a>16.7.2.3.&nbsp;Large Tests<a class="indexterm" name="d366e12733"></a></h4></div></div></div><p><span class="emphasis"><em>Large</em></span> tests are everything else. They are typically large-scale
tests, regression tests for specific bugs, timeout tests, performance tests.
They are executed before a commit on the pre-integration machines. They can be run on
the developer machine as well.
</p></div><div class="section" title="16.7.2.4.&nbsp;Integration Tests"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.unittests.integration"></a>16.7.2.4.&nbsp;Integration Tests<a class="indexterm" name="d366e12743"></a></h4></div></div></div><p><span class="emphasis"><em>Integration</em></span> tests are system level tests. See
<a class="xref" href="#integration.tests" title="16.7.5.&nbsp;Integration Tests">Section&nbsp;16.7.5, &#8220;Integration Tests&#8221;</a> for more info.
</p></div></div><div class="section" title="16.7.3.&nbsp;Running tests"><div class="titlepage"><div><div><h3 class="title"><a name="hbase.unittests.cmds"></a>16.7.3.&nbsp;Running tests</h3></div></div></div><p>Below we describe how to run the Apache HBase junit categories.</p><div class="section" title="16.7.3.1.&nbsp;Default: small and medium category tests"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.unittests.cmds.test"></a>16.7.3.1.&nbsp;Default: small and medium category tests
</h4></div></div></div><p>Running </p><pre class="programlisting">mvn test</pre><p> will execute all small tests in a single JVM
(no fork) and then medium tests in a separate JVM for each test instance.
Medium tests are NOT executed if there is an error in a small test.
Large tests are NOT executed.  There is one report for small tests, and one report for
medium tests if they are executed.
</p></div><div class="section" title="16.7.3.2.&nbsp;Running all tests"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.unittests.cmds.test.runAllTests"></a>16.7.3.2.&nbsp;Running all tests</h4></div></div></div><p>Running </p><pre class="programlisting">mvn test -P runAllTests</pre><p>
will execute small tests in a single JVM then medium and large tests in a separate JVM for each test.
Medium and large tests are NOT executed if there is an error in a small test.
Large tests are NOT executed if there is an error in a small or medium test.
There is one report for small tests, and one report for medium and large tests if they are executed.
</p></div><div class="section" title="16.7.3.3.&nbsp;Running a single test or all tests in a package"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.unittests.cmds.test.localtests.mytest"></a>16.7.3.3.&nbsp;Running a single test or all tests in a package</h4></div></div></div><p>To run an individual test, e.g. <code class="classname">MyTest</code>, do
</p><pre class="programlisting">mvn test -Dtest=MyTest</pre><p>  You can also
pass multiple, individual tests as a comma-delimited list:
</p><pre class="programlisting">mvn test -Dtest=MyTest1,MyTest2,MyTest3</pre><p>
You can also pass a package, which will run all tests under the package:
</p><pre class="programlisting">mvn test -Dtest=org.apache.hadoop.hbase.client.*</pre><p>
</p><p>
When <code class="code">-Dtest</code> is specified, <code class="code">localTests</code> profile will be used. It will use the official release
of maven surefire, rather than our custom surefire plugin, and the old connector (The HBase build uses a patched
version of the maven surefire plugin). Each junit tests is executed in a separate JVM (A fork per test class).
There is no parallelization when tests are running in this mode. You will see a new message at the end of the
-report: "[INFO] Tests are skipped". It's harmless. While you need to make sure the sum of <code class="code">Tests run:</code> in
the <code class="code">Results :</code> section of test reports matching the number of tests you specified because no
error will be reported when a non-existent test case is specified.
</p></div><div class="section" title="16.7.3.4.&nbsp;Other test invocation permutations"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.unittests.cmds.test.profiles"></a>16.7.3.4.&nbsp;Other test invocation permutations</h4></div></div></div><p>Running </p><pre class="programlisting">mvn test -P runSmallTests</pre><p> will execute "small" tests only, using a single JVM.
</p><p>Running </p><pre class="programlisting">mvn test -P runMediumTests</pre><p> will execute "medium" tests only, launching a new JVM for each test-class.
</p><p>Running </p><pre class="programlisting">mvn test -P runLargeTests</pre><p> will execute "large" tests only, launching a new JVM for each test-class.
</p><p>For convenience, you can run </p><pre class="programlisting">mvn test -P runDevTests</pre><p> to execute both small and medium tests, using a single JVM.
</p></div><div class="section" title="16.7.3.5.&nbsp;Running tests faster"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.unittests.test.faster"></a>16.7.3.5.&nbsp;Running tests faster</h4></div></div></div><p>
By default, <code class="code">$ mvn test -P runAllTests</code> runs 5 tests in parallel.
It can be increased on a developer's machine. Allowing that you can have 2
tests in parallel per core, and you need about 2Gb of memory per test (at the
extreme), if you have an 8 core, 24Gb box, you can have 16 tests in parallel.
but the memory available limits it to 12 (24/2), To run all tests with 12 tests
in parallell, do this:
<span class="command"><strong>mvn test -P runAllTests -Dsurefire.secondPartThreadCount=12</strong></span>.
To increase the speed, you can as well use a ramdisk. You will need 2Gb of memory
to run all tests. You will also need to delete the files between two test run.
The typical way to configure a ramdisk on Linux is:
</p><pre class="programlisting">$ sudo mkdir /ram2G
sudo mount -t tmpfs -o size=2048M tmpfs /ram2G</pre><p>
You can then use it to run all HBase tests with the command:
<span class="command"><strong>mvn test -P runAllTests -Dsurefire.secondPartThreadCount=12 -Dtest.build.data.basedirectory=/ram2G</strong></span>
</p></div><div class="section" title="16.7.3.6.&nbsp;hbasetests.sh"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.unittests.cmds.test.hbasetests"></a>16.7.3.6.&nbsp;<span class="command"><strong>hbasetests.sh</strong></span></h4></div></div></div><p>It's also possible to use the script <span class="command"><strong>hbasetests.sh</strong></span>. This script runs the medium and
large tests in parallel with two maven instances, and provides a single report.  This script does not use
the hbase version of surefire so no parallelization is being done other than the two maven instances the
script sets up.
It must be executed from the directory which contains the <code class="filename">pom.xml</code>.</p><p>For example running
</p><pre class="programlisting">./dev-support/hbasetests.sh</pre><p> will execute small and medium tests.
Running </p><pre class="programlisting">./dev-support/hbasetests.sh runAllTests</pre><p> will execute all tests.
Running </p><pre class="programlisting">./dev-support/hbasetests.sh replayFailed</pre><p> will rerun the failed tests a
second time, in a separate jvm and without parallelisation.
</p></div><div class="section" title="16.7.3.7.&nbsp;Test Resource Checker"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.unittests.resource.checker"></a>16.7.3.7.&nbsp;Test Resource Checker<a class="indexterm" name="d366e12871"></a></h4></div></div></div><p>
A custom Maven SureFire plugin listener checks a  number of resources before
and after each HBase unit test runs and logs its findings at the end of the test
output files which can be found in <code class="filename">target/surefire-reports</code>
per Maven module (Tests write test reports named for the test class into this directory.
Check the <code class="filename">*-out.txt</code> files).  The resources counted are the number
of threads, the number of file descriptors, etc. If the number has increased, it adds
a <span class="emphasis"><em>LEAK?</em></span> comment in the logs. As you can have an HBase instance
running in the background, some threads can be deleted/created without any specific
action in the test. However, if the test does not work as expected, or if the test
should not impact these resources, it's worth checking these log lines
<code class="computeroutput">...hbase.ResourceChecker(157): before...</code> and
<code class="computeroutput">...hbase.ResourceChecker(157): after...</code>. For example:
<code class="computeroutput">
2012-09-26 09:22:15,315 INFO  [pool-1-thread-1] hbase.ResourceChecker(157): after: regionserver.TestColumnSeeking#testReseeking Thread=65 (was 65), OpenFileDescriptor=107 (was 107), MaxFileDescriptor=10240 (was 10240), ConnectionCount=1 (was 1)
</code>
</p></div></div><div class="section" title="16.7.4.&nbsp;Writing Tests"><div class="titlepage"><div><div><h3 class="title"><a name="hbase.tests.writing"></a>16.7.4.&nbsp;Writing Tests</h3></div></div></div><div class="section" title="16.7.4.1.&nbsp;General rules"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.tests.rules"></a>16.7.4.1.&nbsp;General rules</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
As much as possible, tests should be written as category small tests.
</li><li class="listitem">
All tests must be written to support parallel execution on the same machine, hence they should not use shared resources as fixed ports or fixed file names.
</li><li class="listitem">
Tests should not overlog. More than 100 lines/second makes the logs complex to read and use i/o that are hence not available for the other tests.
</li><li class="listitem">
Tests can be written with <code class="classname">HBaseTestingUtility</code>.
This class offers helper functions to create a temp directory and do the cleanup, or to start a cluster.
</li></ul></div></div><div class="section" title="16.7.4.2.&nbsp;Categories and execution time"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.tests.categories"></a>16.7.4.2.&nbsp;Categories and execution time</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
All tests must be categorized, if not they could be skipped.
</li><li class="listitem">
All tests should be written to be as fast as possible.
</li><li class="listitem">
Small category tests should last less than 15 seconds, and must not have any side effect.
</li><li class="listitem">
Medium category tests should last less than 50 seconds.
</li><li class="listitem">
Large category tests should last less than 3 minutes.  This should ensure a good parallelization for people using it, and ease the analysis when the test fails.
</li></ul></div></div><div class="section" title="16.7.4.3.&nbsp;Sleeps in tests"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.tests.sleeps"></a>16.7.4.3.&nbsp;Sleeps in tests</h4></div></div></div><p>Whenever possible, tests should not use <code class="methodname">Thread.sleep</code>, but rather waiting for the real event they need. This is faster and clearer for the reader.
Tests should not do a <code class="methodname">Thread.sleep</code> without testing an ending condition. This allows understanding what the test is waiting for. Moreover, the test will work whatever the machine performance is.
Sleep should be minimal to be as fast as possible. Waiting for a variable should be done in a 40ms sleep loop. Waiting for a socket operation should be done in a 200 ms sleep loop.
</p></div><div class="section" title="16.7.4.4.&nbsp;Tests using a cluster"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.tests.cluster"></a>16.7.4.4.&nbsp;Tests using a cluster
</h4></div></div></div><p>Tests using a HRegion do not have to start a cluster: A region can use the local file system.
Start/stopping a cluster cost around 10 seconds. They should not be started per test method but per test class.
Started cluster must be shutdown using <code class="methodname">HBaseTestingUtility#shutdownMiniCluster</code>, which cleans the directories.
As most as possible, tests should use the default settings for the cluster. When they don't, they should document it. This will allow to share the cluster later.
</p></div></div><div class="section" title="16.7.5.&nbsp;Integration Tests"><div class="titlepage"><div><div><h3 class="title"><a name="integration.tests"></a>16.7.5.&nbsp;Integration Tests</h3></div></div></div><p>HBase integration/system tests are tests that are beyond HBase unit tests.  They
are generally long-lasting, sizeable (the test can be asked to 1M rows or 1B rows),
targetable (they can take configuration that will point them at the ready-made cluster
they are to run against; integration tests do not include cluster start/stop code),
and verifying success, integration tests rely on public APIs only; they do not
attempt to examine server internals asserting success/fail. Integration tests
are what you would run when you need to more elaborate proofing of a release candidate
beyond what unit tests can do. They are not generally run on the Apache Continuous Integration
build server, however, some sites opt to run integration tests as a part of their
continuous testing on an actual cluster.
</p><p>
Integration tests currently live under the <code class="filename">src/test</code> directory
in the hbase-it submodule and will match the regex: <code class="filename">**/IntegrationTest*.java</code>.
All integration tests are also annotated with <code class="code">@Category(IntegrationTests.class)</code>.
</p><p>
Integration tests can be run in two modes: using a mini cluster, or against an actual distributed cluster.
Maven failsafe is used to run the tests using the mini cluster. IntegrationTestsDriver class is used for
executing the tests against a distributed cluster. Integration tests SHOULD NOT assume that they are running against a
mini cluster, and SHOULD NOT use private API's to access cluster state. To interact with the distributed or mini
cluster uniformly, <code class="code">IntegrationTestingUtility</code>, and <code class="code">HBaseCluster</code> classes,
and public client API's can be used.
</p><p>
On a distributed cluster, integration tests that use ChaosMonkey or otherwise manipulate services thru cluster manager (e.g. restart regionservers) use SSH to do it.
To run these, test process should be able to run commands on remote end, so ssh should be configured accordingly (for example, if HBase runs under hbase
user in your cluster, you can set up passwordless ssh for that user and run the test also under it). To facilitate that, <code class="code">hbase.it.clustermanager.ssh.user</code>,
<code class="code">hbase.it.clustermanager.ssh.opts</code> and <code class="code">hbase.it.clustermanager.ssh.cmd</code> configuration settings can be used. "User" is the remote user that cluster manager should use to perform ssh commands.
"Opts" contains additional options that are passed to SSH (for example, "-i /tmp/my-key").
Finally, if you have some custom environment setup, "cmd" is the override format for the entire tunnel (ssh) command. The default string is {<code class="code">/usr/bin/ssh %1$s %2$s%3$s%4$s "%5$s"</code>} and is a good starting point. This is a standard Java format string with 5 arguments that is used to execute the remote command. The argument 1 (%1$s) is SSH options set the via opts setting or via environment variable, 2 is SSH user name, 3 is "@" if username is set or "" otherwise, 4 is the target host name, and 5 is the logical command to execute (that may include single quotes, so don't use them). For example, if you run the tests under non-hbase user and want to ssh as that user and change to hbase on remote machine, you can use {<code class="code">/usr/bin/ssh %1$s %2$s%3$s%4$s "su hbase - -c \"%5$s\""</code>}. That way, to kill RS (for example) integration tests may run {<code class="code">/usr/bin/ssh some-hostname "su hbase - -c \"ps aux | ... | kill ...\""</code>}.
The command is logged in the test logs, so you can verify it is correct for your environment.
</p><div class="section" title="16.7.5.1.&nbsp;Running integration tests against mini cluster"><div class="titlepage"><div><div><h4 class="title"><a name="maven.build.commands.integration.tests.mini"></a>16.7.5.1.&nbsp;Running integration tests against mini cluster</h4></div></div></div><p>HBase 0.92 added a <code class="varname">verify</code> maven target.
Invoking it, for example by doing <code class="code">mvn verify</code>, will
run all the phases up to and including the verify phase via the
maven <a class="link" href="http://maven.apache.org/plugins/maven-failsafe-plugin/" target="_top">failsafe plugin</a>,
running all the above mentioned HBase unit tests as well as tests that are in the HBase integration test group.
After you have completed
          </p><pre class="programlisting">mvn install -DskipTests</pre><p>
You can run just the integration tests by invoking:
          </p><pre class="programlisting">
cd hbase-it
mvn verify</pre><p>

If you just want to run the integration tests in top-level, you need to run two commands. First:
          </p><pre class="programlisting">mvn failsafe:integration-test</pre><p>
This actually runs ALL the integration tests.
          </p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>This command will always output <code class="code">BUILD SUCCESS</code> even if there are test failures.
          </p></div><p>
          At this point, you could grep the output by hand looking for failed tests. However, maven will do this for us; just use:
          </p><pre class="programlisting">mvn failsafe:verify</pre><p>
          The above command basically looks at all the test results (so don't remove the 'target' directory) for test failures and reports the results.</p><div class="section" title="16.7.5.1.1.&nbsp;Running a subset of Integration tests"><div class="titlepage"><div><div><h5 class="title"><a name="maven.build.commanas.integration.tests2"></a>16.7.5.1.1.&nbsp;Running a subset of Integration tests</h5></div></div></div><p>This is very similar to how you specify running a subset of unit tests (see above), but use the property
	      <code class="code">it.test</code> instead of <code class="code">test</code>.
To just run <code class="classname">IntegrationTestClassXYZ.java</code>, use:
          </p><pre class="programlisting">mvn failsafe:integration-test -Dit.test=IntegrationTestClassXYZ</pre><p>
          The next thing you might want to do is run groups of integration tests, say all integration tests that are named IntegrationTestClassX*.java:
          </p><pre class="programlisting">mvn failsafe:integration-test -Dit.test=*ClassX*</pre><p>
          This runs everything that is an integration test that matches *ClassX*. This means anything matching: "**/IntegrationTest*ClassX*".
          You can also run multiple groups of integration tests using comma-delimited lists (similar to unit tests). Using a list of matches still supports full regex matching for each of the groups.This would look something like:
          </p><pre class="programlisting">mvn failsafe:integration-test -Dit.test=*ClassX*, *ClassY</pre><p>
          </p></div></div><div class="section" title="16.7.5.2.&nbsp;Running integration tests against distributed cluster"><div class="titlepage"><div><div><h4 class="title"><a name="maven.build.commands.integration.tests.distributed"></a>16.7.5.2.&nbsp;Running integration tests against distributed cluster</h4></div></div></div><p>
If you have an already-setup HBase cluster, you can launch the integration tests by invoking the class <code class="code">IntegrationTestsDriver</code>. You may have to
run test-compile first. The configuration will be picked by the bin/hbase script.
</p><pre class="programlisting">mvn test-compile</pre><p>
Then launch the tests with:
</p><pre class="programlisting">bin/hbase [--config config_dir] org.apache.hadoop.hbase.IntegrationTestsDriver</pre><p>
Pass <code class="code">-h</code> to get usage on this sweet tool.  Running the IntegrationTestsDriver without any argument will launch tests found under <code class="code">hbase-it/src/test</code>, having <code class="code">@Category(IntegrationTests.class)</code> annotation,
and a name starting with <code class="code">IntegrationTests</code>.  See the usage, by passing -h, to see how to filter test classes.
You can pass a regex which is checked against the full class name; so, part of class name can be used.
IntegrationTestsDriver uses Junit to run the tests. Currently there is no support for running integration tests against a distributed cluster using maven (see <a class="link" href="https://issues.apache.org/jira/browse/HBASE-6201" target="_top">HBASE-6201</a>).
</p><p>
The tests interact with the distributed cluster by using the methods in the <code class="code">DistributedHBaseCluster</code> (implementing <code class="code">HBaseCluster</code>) class, which in turn uses a pluggable <code class="code">ClusterManager</code>. Concrete implementations provide actual functionality for carrying out deployment-specific and environment-dependent tasks (SSH, etc). The default <code class="code">ClusterManager</code> is <code class="code">HBaseClusterManager</code>, which uses SSH to remotely execute start/stop/kill/signal commands, and assumes some posix commands (ps, etc). Also assumes the user running the test has enough "power" to start/stop servers on the remote machines. By default, it picks up <code class="code">HBASE_SSH_OPTS, HBASE_HOME, HBASE_CONF_DIR</code> from the env, and uses <code class="code">bin/hbase-daemon.sh</code> to carry out the actions. Currently tarball deployments, deployments which uses hbase-daemons.sh, and <a class="link" href="http://incubator.apache.org/ambari/" target="_top">Apache Ambari</a> deployments are supported. /etc/init.d/ scripts are not supported for now, but it can be easily added. For other deployment options, a ClusterManager can be implemented and plugged in.
</p></div><div class="section" title="16.7.5.3.&nbsp;Destructive integration / system tests"><div class="titlepage"><div><div><h4 class="title"><a name="maven.build.commands.integration.tests.destructive"></a>16.7.5.3.&nbsp;Destructive integration / system tests</h4></div></div></div><p>
	In 0.96, a tool named <code class="code">ChaosMonkey</code> has been introduced. It is modeled after the <a class="link" href="http://techblog.netflix.com/2012/07/chaos-monkey-released-into-wild.html" target="_top">same-named tool by Netflix</a>.
Some of the tests use ChaosMonkey to simulate faults in the running cluster in the way of killing random servers,
disconnecting servers, etc. ChaosMonkey can also be used as a stand-alone tool to run a (misbehaving) policy while you
are running other tests.
</p><p>
ChaosMonkey defines Action's and Policy's. Actions are sequences of events. We have at least the following actions:
</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Restart active master (sleep 5 sec)</li><li class="listitem">Restart random regionserver (sleep 5 sec)</li><li class="listitem">Restart random regionserver (sleep 60 sec)</li><li class="listitem">Restart META regionserver (sleep 5 sec)</li><li class="listitem">Restart ROOT regionserver (sleep 5 sec)</li><li class="listitem">Batch restart of 50% of regionservers (sleep 5 sec)</li><li class="listitem">Rolling restart of 100% of regionservers (sleep 5 sec)</li></ul></div><p>

Policies on the other hand are responsible for executing the actions based on a strategy.
The default policy is to execute a random action every minute based on predefined action
weights. ChaosMonkey executes predefined named policies until it is stopped. More than one
policy can be active at any time.
</p><p>
  To run ChaosMonkey as a standalone tool deploy your HBase cluster as usual. ChaosMonkey uses the configuration
from the bin/hbase script, thus no extra configuration needs to be done. You can invoke the ChaosMonkey by running:
</p><pre class="programlisting">bin/hbase org.apache.hadoop.hbase.util.ChaosMonkey</pre><p>

This will output smt like:
</p><pre class="programlisting">
12/11/19 23:21:57 INFO util.ChaosMonkey: Using ChaosMonkey Policy: class org.apache.hadoop.hbase.util.ChaosMonkey$PeriodicRandomActionPolicy, period:60000
12/11/19 23:21:57 INFO util.ChaosMonkey: Sleeping for 26953 to add jitter
12/11/19 23:22:24 INFO util.ChaosMonkey: Performing action: Restart active master
12/11/19 23:22:24 INFO util.ChaosMonkey: Killing master:master.example.com,60000,1353367210440
12/11/19 23:22:24 INFO hbase.HBaseCluster: Aborting Master: master.example.com,60000,1353367210440
12/11/19 23:22:24 INFO hbase.ClusterManager: Executing remote command: ps aux | grep master | grep -v grep | tr -s ' ' | cut -d ' ' -f2 | xargs kill -s SIGKILL , hostname:master.example.com
12/11/19 23:22:25 INFO hbase.ClusterManager: Executed remote command, exit code:0 , output:
12/11/19 23:22:25 INFO hbase.HBaseCluster: Waiting service:master to stop: master.example.com,60000,1353367210440
12/11/19 23:22:25 INFO hbase.ClusterManager: Executing remote command: ps aux | grep master | grep -v grep | tr -s ' ' | cut -d ' ' -f2 , hostname:master.example.com
12/11/19 23:22:25 INFO hbase.ClusterManager: Executed remote command, exit code:0 , output:
12/11/19 23:22:25 INFO util.ChaosMonkey: Killed master server:master.example.com,60000,1353367210440
12/11/19 23:22:25 INFO util.ChaosMonkey: Sleeping for:5000
12/11/19 23:22:30 INFO util.ChaosMonkey: Starting master:master.example.com
12/11/19 23:22:30 INFO hbase.HBaseCluster: Starting Master on: master.example.com
12/11/19 23:22:30 INFO hbase.ClusterManager: Executing remote command: /homes/enis/code/hbase-0.94/bin/../bin/hbase-daemon.sh --config /homes/enis/code/hbase-0.94/bin/../conf start master , hostname:master.example.com
12/11/19 23:22:31 INFO hbase.ClusterManager: Executed remote command, exit code:0 , output:starting master, logging to /homes/enis/code/hbase-0.94/bin/../logs/hbase-enis-master-master.example.com.out
....
12/11/19 23:22:33 INFO util.ChaosMonkey: Started master: master.example.com,60000,1353367210440
12/11/19 23:22:33 INFO util.ChaosMonkey: Sleeping for:51321
12/11/19 23:23:24 INFO util.ChaosMonkey: Performing action: Restart random region server
12/11/19 23:23:24 INFO util.ChaosMonkey: Killing region server:rs3.example.com,60020,1353367027826
12/11/19 23:23:24 INFO hbase.HBaseCluster: Aborting RS: rs3.example.com,60020,1353367027826
12/11/19 23:23:24 INFO hbase.ClusterManager: Executing remote command: ps aux | grep regionserver | grep -v grep | tr -s ' ' | cut -d ' ' -f2 | xargs kill -s SIGKILL , hostname:rs3.example.com
12/11/19 23:23:25 INFO hbase.ClusterManager: Executed remote command, exit code:0 , output:
12/11/19 23:23:25 INFO hbase.HBaseCluster: Waiting service:regionserver to stop: rs3.example.com,60020,1353367027826
12/11/19 23:23:25 INFO hbase.ClusterManager: Executing remote command: ps aux | grep regionserver | grep -v grep | tr -s ' ' | cut -d ' ' -f2 , hostname:rs3.example.com
12/11/19 23:23:25 INFO hbase.ClusterManager: Executed remote command, exit code:0 , output:
12/11/19 23:23:25 INFO util.ChaosMonkey: Killed region server:rs3.example.com,60020,1353367027826. Reported num of rs:6
12/11/19 23:23:25 INFO util.ChaosMonkey: Sleeping for:60000
12/11/19 23:24:25 INFO util.ChaosMonkey: Starting region server:rs3.example.com
12/11/19 23:24:25 INFO hbase.HBaseCluster: Starting RS on: rs3.example.com
12/11/19 23:24:25 INFO hbase.ClusterManager: Executing remote command: /homes/enis/code/hbase-0.94/bin/../bin/hbase-daemon.sh --config /homes/enis/code/hbase-0.94/bin/../conf start regionserver , hostname:rs3.example.com
12/11/19 23:24:26 INFO hbase.ClusterManager: Executed remote command, exit code:0 , output:starting regionserver, logging to /homes/enis/code/hbase-0.94/bin/../logs/hbase-enis-regionserver-rs3.example.com.out

12/11/19 23:24:27 INFO util.ChaosMonkey: Started region server:rs3.example.com,60020,1353367027826. Reported num of rs:6
</pre><p>

As you can see from the log, ChaosMonkey started the default PeriodicRandomActionPolicy, which is configured with all the available actions, and ran RestartActiveMaster and RestartRandomRs actions. ChaosMonkey tool, if run from command line, will keep on running until the process is killed.
</p></div></div></div><div class="section" title="16.8.&nbsp;Maven Build Commands"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="maven.build.commands"></a>16.8.&nbsp;Maven Build Commands</h2></div></div></div><p>All commands executed from the local HBase project directory.
       </p><p>Note: use Maven 3 (Maven 2 may work but we suggest you use Maven 3).
       </p><div class="section" title="16.8.1.&nbsp;Compile"><div class="titlepage"><div><div><h3 class="title"><a name="maven.build.commands.compile"></a>16.8.1.&nbsp;Compile</h3></div></div></div><pre class="programlisting">
mvn compile
          </pre></div><div class="section" title="16.8.2.&nbsp;Running all or individual Unit Tests"><div class="titlepage"><div><div><h3 class="title"><a name="maven.build.commands.unitall"></a>16.8.2.&nbsp;Running all or individual Unit Tests</h3></div></div></div><p>See the <a class="xref" href="#hbase.unittests.cmds" title="16.7.3.&nbsp;Running tests">Section&nbsp;16.7.3, &#8220;Running tests&#8221;</a> section
          above in <a class="xref" href="#hbase.unittests" title="16.7.2.&nbsp;Unit Tests">Section&nbsp;16.7.2, &#8220;Unit Tests&#8221;</a></p></div><div class="section" title="16.8.3.&nbsp;Building against various hadoop versions."><div class="titlepage"><div><div><h3 class="title"><a name="maven.build.hadoop"></a>16.8.3.&nbsp;Building against various hadoop versions.</h3></div></div></div><p>As of 0.96, Apache HBase supports building against Apache Hadoop versions: 1.0.3, 2.0.0-alpha and 3.0.0-SNAPSHOT.
	  By default, in 0.96 and earlier, we will build with Hadoop-1.0.x. 
          As of 0.98, Hadoop 1.x is deprecated and Hadoop 2.x is the default.
          To change the version to build against, add a hadoop.profile property when you invoke <span class="command"><strong>mvn</strong></span>:</p><pre class="programlisting">mvn -Dhadoop.profile=1.0 ...</pre><p>
         The above will build against whatever explicit hadoop 1.x version we have in our <code class="filename">pom.xml</code> as our '1.0' version.
         Tests may not all pass so you may need to pass <code class="code">-DskipTests</code> unless you are inclined to fix the failing tests.</p><div class="note" title="'dependencyManagement.dependencies.dependency.artifactId' for org.apache.hbase:${compat.module}:test-jar with value '${compat.module}' does not match a valid id pattern" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a name="maven.build.passing.default.profile"></a>'dependencyManagement.dependencies.dependency.artifactId' for org.apache.hbase:${compat.module}:test-jar with value '${compat.module}' does not match a valid id pattern</h3><p>You will see ERRORs like the above title if you pass the <span class="emphasis"><em>default</em></span> profile; e.g. if
you pass <span class="property">hadoop.profile=1.1</span> when building 0.96 or
<span class="property">hadoop.profile=2.0</span> when building hadoop 0.98; just drop the
hadoop.profile stipulation in this case to get your build to run again.  This seems to be a maven
pecularity that is probably fixable but we've not spent the time trying to figure it.</p></div><p>
         Similarly, for 3.0, you would just replace the profile value. Note that Hadoop-3.0.0-SNAPSHOT does not currently have a
         deployed maven artificat - you will need to build and install your own in your local maven repository if you want to run against this profile.
         </p><p>
         In earilier verions of Apache HBase, you can build against older versions of Apache Hadoop, notably, Hadoop 0.22.x and 0.23.x.
         If you are running, for example HBase-0.94 and wanted to build against Hadoop 0.23.x, you would run with:</p><pre class="programlisting">mvn -Dhadoop.profile=22 ...</pre></div></div><div class="section" title="16.9.&nbsp;Getting Involved"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="getting.involved"></a>16.9.&nbsp;Getting Involved</h2></div></div></div><p>Apache HBase gets better only when people contribute!
        </p><p>As Apache HBase is an Apache Software Foundation project, see <a class="xref" href="#asf" title="Appendix&nbsp;H.&nbsp;HBase and the Apache Software Foundation">Appendix&nbsp;H, <i>HBase and the Apache Software Foundation</i></a> for more information about how the ASF functions.
        </p><div class="section" title="16.9.1.&nbsp;Mailing Lists"><div class="titlepage"><div><div><h3 class="title"><a name="mailing.list"></a>16.9.1.&nbsp;Mailing Lists</h3></div></div></div><p>Sign up for the dev-list and the user-list.  See the
          <a class="link" href="http://hbase.apache.org/mail-lists.html" target="_top">mailing lists</a> page.
          Posing questions - and helping to answer other people's questions - is encouraged!
          There are varying levels of experience on both lists so patience and politeness are encouraged (and please
          stay on topic.)
          </p></div><div class="section" title="16.9.2.&nbsp;Jira"><div class="titlepage"><div><div><h3 class="title"><a name="jira"></a>16.9.2.&nbsp;Jira</h3></div></div></div><p>Check for existing issues in <a class="link" href="https://issues.apache.org/jira/browse/HBASE" target="_top">Jira</a>.
          If it's either a new feature request, enhancement, or a bug, file a ticket.
          </p><div class="section" title="16.9.2.1.&nbsp;Jira Priorities"><div class="titlepage"><div><div><h4 class="title"><a name="jira.priorities"></a>16.9.2.1.&nbsp;Jira Priorities</h4></div></div></div><p>The following is a guideline on setting Jira issue priorities:
                </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Blocker: Should only be used if the issue WILL cause data loss or cluster instability reliably.</li><li class="listitem">Critical: The issue described can cause data loss or cluster instability in some cases.</li><li class="listitem">Major: Important but not tragic issues, like updates to the client API that will add a lot of much-needed functionality or significant
                bugs that need to be fixed but that don't cause data loss.</li><li class="listitem">Minor: Useful enhancements and annoying but not damaging bugs.</li><li class="listitem">Trivial: Useful enhancements but generally cosmetic.</li></ul></div><p>
             </p></div><div class="section" title="16.9.2.2.&nbsp;Code Blocks in Jira Comments"><div class="titlepage"><div><div><h4 class="title"><a name="submitting.patches.jira.code"></a>16.9.2.2.&nbsp;Code Blocks in Jira Comments</h4></div></div></div><p>A commonly used macro in Jira is {code}. If you do this in a Jira comment...
</p><pre class="programlisting">
{code}
   code snippet
{code}
</pre><p>
              ... Jira will format the code snippet like code, instead of a regular comment.  It improves readability.
          </p></div></div></div><div class="section" title="16.10.&nbsp;Developing"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="developing"></a>16.10.&nbsp;Developing</h2></div></div></div><div class="section" title="16.10.1.&nbsp;Codelines"><div class="titlepage"><div><div><h3 class="title"><a name="codelines"></a>16.10.1.&nbsp;Codelines</h3></div></div></div><p>Most development is done on TRUNK.  However, there are branches for minor releases (e.g., 0.90.1, 0.90.2, and 0.90.3 are on the 0.90 branch).</p><p>If you have any questions on this just send an email to the dev dist-list.</p></div><div class="section" title="16.10.2.&nbsp;Unit Tests"><div class="titlepage"><div><div><h3 class="title"><a name="unit.tests"></a>16.10.2.&nbsp;Unit Tests</h3></div></div></div><p>In HBase we use <a class="link" href="http://junit.org" target="_top">JUnit</a> 4.
            If you need to run miniclusters of HDFS, ZooKeeper, HBase, or MapReduce testing,
            be sure to checkout the <code class="classname">HBaseTestingUtility</code>.
            Alex Baranau of Sematext describes how it can be used in
            <a class="link" href="http://blog.sematext.com/2010/08/30/hbase-case-study-using-hbasetestingutility-for-local-testing-development/" target="_top">HBase Case-Study: Using HBaseTestingUtility for Local Testing and Development</a> (2010).
          </p><div class="section" title="16.10.2.1.&nbsp;Mockito"><div class="titlepage"><div><div><h4 class="title"><a name="mockito"></a>16.10.2.1.&nbsp;Mockito</h4></div></div></div><p>Sometimes you don't need a full running server
              unit testing.  For example, some methods can make do with a
              a <code class="classname">org.apache.hadoop.hbase.Server</code> instance
              or a <code class="classname">org.apache.hadoop.hbase.master.MasterServices</code>
              Interface reference rather than a full-blown
              <code class="classname">org.apache.hadoop.hbase.master.HMaster</code>.
              In these cases, you maybe able to get away with a mocked
              <code class="classname">Server</code> instance.  For example:
              </p><pre class="programlisting">
              TODO...
              </pre><p>
           </p></div></div><div class="section" title="16.10.3.&nbsp;Code Standards"><div class="titlepage"><div><div><h3 class="title"><a name="code.standards"></a>16.10.3.&nbsp;Code Standards</h3></div></div></div><p>See <a class="xref" href="#eclipse.code.formatting" title="16.2.1.1.&nbsp;Code Formatting">Section&nbsp;16.2.1.1, &#8220;Code Formatting&#8221;</a> and <a class="xref" href="#common.patch.feedback" title="16.11.5.&nbsp;Common Patch Feedback">Section&nbsp;16.11.5, &#8220;Common Patch Feedback&#8221;</a>.
           </p><p>Also, please pay attention to the interface stability/audience classifications that you
           will see all over our code base.   They look like this at the head of the class:
           </p><pre class="programlisting">@InterfaceAudience.Public
@InterfaceStability.Stable</pre><p>
           </p><p>If the <code class="classname">InterfaceAudience</code> is <code class="varname">Private</code>,
           we can change the class (and we do not need to include a <code class="classname">InterfaceStability</code> mark).
           If a class is marked <code class="varname">Public</code> but its <code class="classname">InterfaceStability</code>
           is marked <code class="varname">Unstable</code>, we can change it. If it's
           marked <code class="varname">Public</code>/<code class="varname">Evolving</code>, we're allowed to change it
           but should try not to. If it's <code class="varname">Public</code> and <code class="varname">Stable</code>
           we can't change it without a deprecation path or with a really GREAT reason.</p><p>When you add new classes, mark them with the annotations above if publically accessible.
           If you are not cleared on how to mark your additions, ask up on the dev list.
           </p><p>This convention comes from our parent project Hadoop.</p></div><div class="section" title="16.10.4.&nbsp;Invariants"><div class="titlepage"><div><div><h3 class="title"><a name="design.invariants"></a>16.10.4.&nbsp;Invariants</h3></div></div></div><p>We don't have many but what we have we list below.  All are subject to challenge of
           course but until then, please hold to the rules of the road.
           </p><div class="section" title="16.10.4.1.&nbsp;No permanent state in ZooKeeper"><div class="titlepage"><div><div><h4 class="title"><a name="design.invariants.zk.data"></a>16.10.4.1.&nbsp;No permanent state in ZooKeeper</h4></div></div></div><p>ZooKeeper state should transient (treat it like memory). If deleted, hbase
          should be able to recover and essentially be in the same state<sup>[<a name="d366e13354" href="#ftn.d366e13354" class="footnote">35</a>]</sup>.
          </p></div></div><div class="section" title="16.10.5.&nbsp;Running In-Situ"><div class="titlepage"><div><div><h3 class="title"><a name="run.insitu"></a>16.10.5.&nbsp;Running In-Situ</h3></div></div></div><p>If you are developing Apache HBase, frequently it is useful to test your changes against a more-real cluster than what you find in unit tests. In this case, HBase can be run directly from the source in local-mode.
           All you need to do is run:
           </p><pre class="programlisting">${HBASE_HOME}/bin/start-hbase.sh</pre><p>
           This will spin up a full local-cluster, just as if you had packaged up HBase and installed it on your machine.
           </p><p>Keep in mind that you will need to have installed HBase into your local maven repository for the in-situ cluster to work properly. That is, you will need to run:</p><pre class="programlisting">mvn clean install -DskipTests</pre><p>to ensure that maven can find the correct classpath and dependencies. Generally, the above command
           is just a good thing to try running first, if maven is acting oddly.</p></div><div class="section" title="16.10.6.&nbsp;Adding Metrics"><div class="titlepage"><div><div><h3 class="title"><a name="add.metrics"></a>16.10.6.&nbsp;Adding Metrics</h3></div></div></div><p>
                  After adding a new feature a developer might want to add metrics.  HBase exposes
                  metrics using the Hadoop Metrics 2 system, so adding a new metric involves
                  exposing that metric to the hadoop system.  Unfortunately the API of metrics2
                  changed from hadoop 1 to hadoop 2.  In order to get around this a set of
                  interfaces and implementations have to be loaded at runtime.  To get an in-depth
                  look at the reasoning and structure of these classes you can read the blog post
                  located <a class="link" href="https://blogs.apache.org/hbase/entry/migration_to_the_new_metrics" target="_top">here</a>.
                  To add a metric to an existing MBean follow the short guide below:
              </p><div class="section" title="16.10.6.1.&nbsp;Add Metric name and Function to Hadoop Compat Interface."><div class="titlepage"><div><div><h4 class="title"><a name="d366e13383"></a>16.10.6.1.&nbsp;Add Metric name and Function to Hadoop Compat Interface.</h4></div></div></div><p>
                     Inside of the source interface the corresponds to where the metrics are
                     generated (eg MetricsMasterSource for things coming from HMaster) create new
                     static strings for metric name and description.  Then add a new method that
                     will be called to add new reading.
                  </p></div><div class="section" title="16.10.6.2.&nbsp;Add the Implementation to Both Hadoop 1 and Hadoop 2 Compat modules."><div class="titlepage"><div><div><h4 class="title"><a name="d366e13388"></a>16.10.6.2.&nbsp;Add the Implementation to Both Hadoop 1 and Hadoop 2 Compat modules.</h4></div></div></div><p>
                      Inside of the implementation of the source (eg. MetricsMasterSourceImpl
                      in the above example) create a new histogram, counter, gauge, or stat in the
                      init method.  Then in the method that was added to the interface wire up the
                      parameter passed in to the histogram.
                  </p><p>
                      Now add tests that make sure the data is correctly exported to the metrics 2
                      system. For this the MetricsAssertHelper is provided.
                  </p></div></div></div><div class="section" title="16.11.&nbsp;Submitting Patches"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="submitting.patches"></a>16.11.&nbsp;Submitting Patches</h2></div></div></div><p>If you are new to submitting patches to open source or new to submitting patches to Apache,
          I'd suggest you start by reading the <a class="link" href="http://commons.apache.org/patches.html" target="_top">On Contributing Patches</a>
          page from <a class="link" href="http://commons.apache.org/" target="_top">Apache Commons Project</a>.  Its a nice overview that
          applies equally to the Apache HBase Project.</p><div class="section" title="16.11.1.&nbsp;Create Patch"><div class="titlepage"><div><div><h3 class="title"><a name="submitting.patches.create"></a>16.11.1.&nbsp;Create Patch</h3></div></div></div><p>See the aforementioned Apache Commons link for how to make patches against a checked out subversion
          repository.  Patch files can also be easily generated from Eclipse, for example by selecting "Team -&gt; Create Patch".
          Patches can also be created by git diff and svn diff.
          </p><p>Please submit one patch-file per Jira.  For example, if multiple files are changed make sure the
          selected resource when generating the patch is a directory.  Patch files can reflect changes in multiple files. </p><p>
          Generating patches using git:<br>
              </p><pre class="programlisting">
$ git diff --no-prefix  &gt; HBASE_XXXX.patch
              </pre><p>
              Don't forget the 'no-prefix' option; and generate the diff from the root directory of project
      </p><p>Make sure you review <a class="xref" href="#eclipse.code.formatting" title="16.2.1.1.&nbsp;Code Formatting">Section&nbsp;16.2.1.1, &#8220;Code Formatting&#8221;</a> for code style. </p></div><div class="section" title="16.11.2.&nbsp;Patch File Naming"><div class="titlepage"><div><div><h3 class="title"><a name="submitting.patches.naming"></a>16.11.2.&nbsp;Patch File Naming</h3></div></div></div><p>The patch file should have the Apache HBase Jira ticket in the name.  For example, if a patch was submitted for <code class="filename">Foo.java</code>, then
          a patch file called <code class="filename">Foo_HBASE_XXXX.patch</code> would be acceptable where XXXX is the Apache HBase Jira number.
          </p><p>If you generating from a branch, then including the target branch in the filename is advised, e.g., <code class="filename">HBASE_XXXX-0.90.patch</code>.
          </p></div><div class="section" title="16.11.3.&nbsp;Unit Tests"><div class="titlepage"><div><div><h3 class="title"><a name="submitting.patches.tests"></a>16.11.3.&nbsp;Unit Tests</h3></div></div></div><p>Yes, please.  Please try to include unit tests with every code patch (and especially new classes and large changes).
            Make sure unit tests pass locally before submitting the patch.</p><p>Also, see <a class="xref" href="#mockito" title="16.10.2.1.&nbsp;Mockito">Section&nbsp;16.10.2.1, &#8220;Mockito&#8221;</a>.</p><p>If you are creating a new unit test class, notice how other unit test classes have classification/sizing
            annotations at the top and a static method on the end.  Be sure to include these in any new unit test files
            you generate.  See <a class="xref" href="#hbase.tests" title="16.7.&nbsp;Tests">Section&nbsp;16.7, &#8220;Tests&#8221;</a> for more on how the annotations work.
            </p></div><div class="section" title="16.11.4.&nbsp;Attach Patch to Jira"><div class="titlepage"><div><div><h3 class="title"><a name="submitting.patches.jira"></a>16.11.4.&nbsp;Attach Patch to Jira</h3></div></div></div><p>The patch should be attached to the associated Jira ticket "More Actions -&gt; Attach Files".  Make sure you click the
            ASF license inclusion, otherwise the patch can't be considered for inclusion.
            </p><p>Once attached to the ticket, click "Submit Patch" and
            the status of the ticket will change.  Committers will review submitted patches for inclusion into the codebase.  Please
            understand that not every patch may get committed, and that feedback will likely be provided on the patch.  Fear not, though,
            because the Apache HBase community is helpful!
            </p></div><div class="section" title="16.11.5.&nbsp;Common Patch Feedback"><div class="titlepage"><div><div><h3 class="title"><a name="common.patch.feedback"></a>16.11.5.&nbsp;Common Patch Feedback</h3></div></div></div><p>The following items are representative of common patch feedback. Your patch process will go faster if these are
          taken into account <span class="emphasis"><em>before</em></span> submission.
          </p><p>
          See the <a class="link" href="http://www.oracle.com/technetwork/java/codeconv-138413.html" target="_top">Java coding standards</a>
          for more information on coding conventions in Java.
          </p><div class="section" title="16.11.5.1.&nbsp;Space Invaders"><div class="titlepage"><div><div><h4 class="title"><a name="common.patch.feedback.space.invaders"></a>16.11.5.1.&nbsp;Space Invaders</h4></div></div></div><p>Rather than do this...
</p><pre class="programlisting">
if ( foo.equals( bar ) ) {     // don't do this
</pre><p>
			... do this instead...
</p><pre class="programlisting">
if (foo.equals(bar)) {
</pre><p>
          </p><p>Also, rather than do this...
</p><pre class="programlisting">
foo = barArray[ i ];     // don't do this
</pre><p>
			... do this instead...
</p><pre class="programlisting">
foo = barArray[i];
</pre><p>
          </p></div><div class="section" title="16.11.5.2.&nbsp;Auto Generated Code"><div class="titlepage"><div><div><h4 class="title"><a name="common.patch.feedback.autogen"></a>16.11.5.2.&nbsp;Auto Generated Code</h4></div></div></div><p>Auto-generated code in Eclipse often looks like this...
</p><pre class="programlisting">
 public void readFields(DataInput arg0) throws IOException {    // don't do this
   foo = arg0.readUTF();                                       // don't do this
</pre><p>
			... do this instead ...
</p><pre class="programlisting">
 public void readFields(DataInput di) throws IOException {
   foo = di.readUTF();
</pre><p>
           See the difference?  'arg0' is what Eclipse uses for arguments by default.
           </p></div><div class="section" title="16.11.5.3.&nbsp;Long Lines"><div class="titlepage"><div><div><h4 class="title"><a name="common.patch.feedback.longlines"></a>16.11.5.3.&nbsp;Long Lines</h4></div></div></div><p>
            Keep lines less than 100 characters.
</p><pre class="programlisting">
Bar bar = foo.veryLongMethodWithManyArguments(argument1, argument2, argument3, argument4, argument5, argument6, argument7, argument8, argument9);  // don't do this
</pre><p>
			... do something like this instead ...
</p><pre class="programlisting">
Bar bar = foo.veryLongMethodWithManyArguments(
 argument1, argument2, argument3,argument4, argument5, argument6, argument7, argument8, argument9);
</pre><p>
           </p></div><div class="section" title="16.11.5.4.&nbsp;Trailing Spaces"><div class="titlepage"><div><div><h4 class="title"><a name="common.patch.feedback.trailingspaces"></a>16.11.5.4.&nbsp;Trailing Spaces</h4></div></div></div><p>
            This happens more than people would imagine.
</p><pre class="programlisting">
Bar bar = foo.getBar();     &lt;--- imagine there's an extra space(s) after the semicolon instead of a line break.
</pre><p>
            Make sure there's a line-break after the end of your code, and also avoid lines that have nothing
            but whitespace.
            </p></div><div class="section" title="16.11.5.5.&nbsp;Implementing Writable"><div class="titlepage"><div><div><h4 class="title"><a name="common.patch.feedback.writable"></a>16.11.5.5.&nbsp;Implementing Writable</h4></div></div></div><div class="note" title="Applies pre-0.96 only" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Applies pre-0.96 only</h3><p>In 0.96, HBase moved to protobufs.  The below section on Writables
                    applies to 0.94.x and previous, not to 0.96 and beyond.
                </p></div><p>Every class returned by RegionServers must implement <code class="code">Writable</code>.  If you
            are creating a new class that needs to implement this interface, don't forget the default constructor.
            </p></div><div class="section" title="16.11.5.6.&nbsp;Javadoc"><div class="titlepage"><div><div><h4 class="title"><a name="common.patch.feedback.javadoc"></a>16.11.5.6.&nbsp;Javadoc</h4></div></div></div><p>This is also a very common feedback item.  Don't forget Javadoc!
                </p><p>Javadoc warnings are checked during precommit. If the precommit tool gives you a '-1',
                    please fix the javadoc issue. Your patch won't be committed if it adds such warnings.
                </p><p>
            </p></div><div class="section" title="16.11.5.7.&nbsp;Findbugs"><div class="titlepage"><div><div><h4 class="title"><a name="common.patch.feedback.findbugs"></a>16.11.5.7.&nbsp;Findbugs</h4></div></div></div><p>
                    Findbugs is used to detect common bugs pattern. As Javadoc, it is checked during
                    the precommit build up on Apache's Jenkins, and as with Javadoc, please fix them.
                    You can run findbugs locally with 'mvn findbugs:findbugs': it will generate the
                    findbugs files locally.  Sometimes, you may have to write code smarter than
                    Findbugs. You can annotate your code to tell Findbugs you know what you're
                    doing, by annotating your class with:
                    </p><pre class="programlisting">@edu.umd.cs.findbugs.annotations.SuppressWarnings(
                    value="HE_EQUALS_USE_HASHCODE",
                    justification="I know what I'm doing")</pre><p>
            </p><p>
                    Note that we're using the apache licensed version of the annotations.
            </p></div><div class="section" title="16.11.5.8.&nbsp;Javadoc - Useless Defaults"><div class="titlepage"><div><div><h4 class="title"><a name="common.patch.feedback.javadoc.defaults"></a>16.11.5.8.&nbsp;Javadoc - Useless Defaults</h4></div></div></div><p>Don't just leave the @param arguments the way your IDE generated them.  Don't do this...
</p><pre class="programlisting">
  /**
   *
   * @param bar             &lt;---- don't do this!!!!
   * @return                &lt;---- or this!!!!
   */
  public Foo getFoo(Bar bar);
</pre><p>
            ... either add something descriptive to the @param and @return lines, or just remove them.
            But the preference is to add something descriptive and useful.
            </p></div><div class="section" title="16.11.5.9.&nbsp;One Thing At A Time, Folks"><div class="titlepage"><div><div><h4 class="title"><a name="common.patch.feedback.onething"></a>16.11.5.9.&nbsp;One Thing At A Time, Folks</h4></div></div></div><p>If you submit a patch for one thing, don't do auto-reformatting or unrelated reformatting of code on a completely
            different area of code.
            </p><p>Likewise, don't add unrelated cleanup or refactorings outside the scope of your Jira.
            </p></div><div class="section" title="16.11.5.10.&nbsp;Ambigious Unit Tests"><div class="titlepage"><div><div><h4 class="title"><a name="common.patch.feedback.tests"></a>16.11.5.10.&nbsp;Ambigious Unit Tests</h4></div></div></div><p>Make sure that you're clear about what you are testing in your unit tests and why.
            </p></div></div><div class="section" title="16.11.6.&nbsp;Submitting a patch again"><div class="titlepage"><div><div><h3 class="title"><a name="d366e13575"></a>16.11.6.&nbsp;Submitting a patch again</h3></div></div></div><p>
                Sometimes committers ask for changes for a patch.  After incorporating the suggested/requested changes, follow the following process to submit the patch again.
            </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
                    Do not delete the old patch file
                </li><li class="listitem">
                    version your new patch file using a simple scheme like this: <br>
                    HBASE-{jira number}-{version}.patch <br>
                    e.g:
                    HBASE_XXXX-v2.patch
                </li><li class="listitem">
                    'Cancel Patch' on JIRA.. bug status will change back to Open
                </li><li class="listitem">
                    Attach new patch file (e.g. HBASE_XXXX-v2.patch) using 'Files --&gt; Attach'
                </li><li class="listitem">
                    Click on 'Submit Patch'.  Now the bug status will say 'Patch Available'.
                </li></ul></div>
            Committers will review the patch.  Rinse and repeat as many times as needed :-)
        </div><div class="section" title="16.11.7.&nbsp;Submitting incremental patches"><div class="titlepage"><div><div><h3 class="title"><a name="d366e13596"></a>16.11.7.&nbsp;Submitting incremental patches</h3></div></div></div><p>
                At times you may want to break a big change into mulitple patches.  Here is a sample work-flow using git
                </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
                        patch 1:
                        <div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
                                $ git diff --no-prefix &gt; HBASE_XXXX-1.patch
                            </li></ul></div></li><li class="listitem">
                        patch 2:
                        <div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
                                create a new git branch <br>
                                $ git checkout -b my_branch
                            </li><li class="listitem">
                                save your work
                                $ git add file1 file2 <br>
                                $ git commit -am 'saved after HBASE_XXXX-1.patch' <br>
                                now you have your own branch, that is different from remote master branch <br></li><li class="listitem">
                                make more changes...
                            </li><li class="listitem">
                                create second patch <br>
                                $ git diff --no-prefix &gt; HBASE_XXXX-2.patch
                            </li></ul></div></li></ul></div><p>
            </p></div><div class="section" title="16.11.8.&nbsp;ReviewBoard"><div class="titlepage"><div><div><h3 class="title"><a name="reviewboard"></a>16.11.8.&nbsp;ReviewBoard</h3></div></div></div><p>Larger patches should go through <a class="link" href="http://reviews.apache.org" target="_top">ReviewBoard</a>.
          </p><p>For more information on how to use ReviewBoard, see
           <a class="link" href="http://www.reviewboard.org/docs/manual/1.5/" target="_top">the ReviewBoard documentation</a>.
          </p></div><div class="section" title="16.11.9.&nbsp;Committing Patches"><div class="titlepage"><div><div><h3 class="title"><a name="committing.patches"></a>16.11.9.&nbsp;Committing Patches</h3></div></div></div><p>
          Committers do this.  See <a class="link" href="http://wiki.apache.org/hadoop/Hbase/HowToCommit" target="_top">How To Commit</a> in the Apache HBase wiki.
          </p><p>Commiters will also resolve the Jira, typically after the patch passes a build.
          </p><div class="section" title="16.11.9.1.&nbsp;Committers are responsible for making sure commits do not break the build or tests"><div class="titlepage"><div><div><h4 class="title"><a name="committer.tests"></a>16.11.9.1.&nbsp;Committers are responsible for making sure commits do not break the build or tests</h4></div></div></div><p>
              If a committer commits a patch it is their responsibility
              to make sure it passes the test suite.  It is helpful
              if contributors keep an eye out that their patch
              does not break the hbase build and/or tests but ultimately,
              a contributor cannot be expected to be up on the
              particular vagaries and interconnections that occur
              in a project like hbase.  A committer should.
            </p></div></div></div><div class="footnotes"><br><hr width="100" align="left"><div class="footnote"><p><sup>[<a id="ftn.d366e12583" href="#d366e12583" class="para">34</a>] </sup>Before 0.95.0, site and reference guide were at src/docbkx and src/site respectively</p></div><div class="footnote"><p><sup>[<a id="ftn.d366e13354" href="#d366e13354" class="para">35</a>] </sup>There are currently
          a few exceptions that we need to fix around whether a table is enabled or disabled</p></div></div></div><div class="chapter" title="Chapter&nbsp;17.&nbsp;ZooKeeper"><div class="titlepage"><div><div><h2 class="title"><a name="zookeeper"></a>Chapter&nbsp;17.&nbsp;ZooKeeper<a class="indexterm" name="d366e13661"></a></h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#d366e13785">17.1. Using existing ZooKeeper ensemble</a></span></dt><dt><span class="section"><a href="#zk.sasl.auth">17.2. SASL Authentication with ZooKeeper</a></span></dt><dd><dl><dt><span class="section"><a href="#d366e13850">17.2.1. Operating System Prerequisites</a></span></dt><dt><span class="section"><a href="#d366e13931">17.2.2. HBase-managed Zookeeper Configuration</a></span></dt><dt><span class="section"><a href="#d366e14006">17.2.3. External Zookeeper Configuration</a></span></dt><dt><span class="section"><a href="#d366e14065">17.2.4. Zookeeper Server Authentication Log Output</a></span></dt><dt><span class="section"><a href="#d366e14073">17.2.5. Zookeeper Client Authentication Log Output</a></span></dt><dt><span class="section"><a href="#d366e14081">17.2.6. Configuration from Scratch</a></span></dt><dt><span class="section"><a href="#d366e14090">17.2.7. Future improvements</a></span></dt></dl></dd></dl></div><p>A distributed Apache HBase installation depends on a running ZooKeeper cluster.
            All participating nodes and clients need to be able to access the
            running ZooKeeper ensemble. Apache HBase by default manages a ZooKeeper
            "cluster" for you. It will start and stop the ZooKeeper ensemble
            as part of the HBase start/stop process. You can also manage the
            ZooKeeper ensemble independent of HBase and just point HBase at
            the cluster it should use. To toggle HBase management of
            ZooKeeper, use the <code class="varname">HBASE_MANAGES_ZK</code> variable in
            <code class="filename">conf/hbase-env.sh</code>. This variable, which
            defaults to <code class="varname">true</code>, tells HBase whether to
            start/stop the ZooKeeper ensemble servers as part of HBase
            start/stop.</p><p>When HBase manages the ZooKeeper ensemble, you can specify
            ZooKeeper configuration using its native
            <code class="filename">zoo.cfg</code> file, or, the easier option is to
            just specify ZooKeeper options directly in
            <code class="filename">conf/hbase-site.xml</code>. A ZooKeeper
            configuration option can be set as a property in the HBase
            <code class="filename">hbase-site.xml</code> XML configuration file by
            prefacing the ZooKeeper option name with
            <code class="varname">hbase.zookeeper.property</code>. For example, the
            <code class="varname">clientPort</code> setting in ZooKeeper can be changed
            by setting the
            <code class="varname">hbase.zookeeper.property.clientPort</code> property.
            For all default values used by HBase, including ZooKeeper
            configuration, see <a class="xref" href="#hbase_default_configurations" title="2.3.1.1.&nbsp;HBase Default Configuration">Section&nbsp;2.3.1.1, &#8220;HBase Default Configuration&#8221;</a>. Look for the
            <code class="varname">hbase.zookeeper.property</code> prefix <sup>[<a name="d366e13700" href="#ftn.d366e13700" class="footnote">36</a>]</sup></p><p>You must at least list the ensemble servers in
            <code class="filename">hbase-site.xml</code> using the
            <code class="varname">hbase.zookeeper.quorum</code> property. This property
            defaults to a single ensemble member at
            <code class="varname">localhost</code> which is not suitable for a fully
            distributed HBase. (It binds to the local machine only and remote
            clients will not be able to connect). </p><div class="note" title="How many ZooKeepers should I run?" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a name="how_many_zks"></a>How many ZooKeepers should I run?</h3><p>You can run a ZooKeeper ensemble that comprises 1 node
                only but in production it is recommended that you run a
                ZooKeeper ensemble of 3, 5 or 7 machines; the more members an
                ensemble has, the more tolerant the ensemble is of host
                failures. Also, run an odd number of machines. In ZooKeeper,
                an even number of peers is supported, but it is normally not used
                because an even sized ensemble requires, proportionally, more peers
                to form a quorum than an odd sized ensemble requires. For example, an
                ensemble with 4 peers requires 3 to form a quorum, while an ensemble with
                5 also requires 3 to form a quorum. Thus, an ensemble of 5 allows 2 peers to
                fail, and thus is more fault tolerant than the ensemble of 4, which allows
                only 1 down peer.
                </p><p>Give each ZooKeeper server around 1GB of RAM, and if possible, its own
                dedicated disk (A dedicated disk is the best thing you can do
                to ensure a performant ZooKeeper ensemble). For very heavily
                loaded clusters, run ZooKeeper servers on separate machines
                from RegionServers (DataNodes and TaskTrackers).</p></div><p>For example, to have HBase manage a ZooKeeper quorum on
            nodes <span class="emphasis"><em>rs{1,2,3,4,5}.example.com</em></span>, bound to
            port 2222 (the default is 2181) ensure
            <code class="varname">HBASE_MANAGE_ZK</code> is commented out or set to
            <code class="varname">true</code> in <code class="filename">conf/hbase-env.sh</code>
            and then edit <code class="filename">conf/hbase-site.xml</code> and set
            <code class="varname">hbase.zookeeper.property.clientPort</code> and
            <code class="varname">hbase.zookeeper.quorum</code>. You should also set
            <code class="varname">hbase.zookeeper.property.dataDir</code> to other than
            the default as the default has ZooKeeper persist data under
            <code class="filename">/tmp</code> which is often cleared on system
            restart. In the example below we have ZooKeeper persist to
            <code class="filename">/user/local/zookeeper</code>. </p><pre class="programlisting">
  &lt;configuration&gt;
    ...
    &lt;property&gt;
      &lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt;
      &lt;value&gt;2222&lt;/value&gt;
      &lt;description&gt;Property from ZooKeeper's config zoo.cfg.
      The port at which the clients will connect.
      &lt;/description&gt;
    &lt;/property&gt;
    &lt;property&gt;
      &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;
      &lt;value&gt;rs1.example.com,rs2.example.com,rs3.example.com,rs4.example.com,rs5.example.com&lt;/value&gt;
      &lt;description&gt;Comma separated list of servers in the ZooKeeper Quorum.
      For example, "host1.mydomain.com,host2.mydomain.com,host3.mydomain.com".
      By default this is set to localhost for local and pseudo-distributed modes
      of operation. For a fully-distributed setup, this should be set to a full
      list of ZooKeeper quorum servers. If HBASE_MANAGES_ZK is set in hbase-env.sh
      this is the list of servers which we will start/stop ZooKeeper on.
      &lt;/description&gt;
    &lt;/property&gt;
    &lt;property&gt;
      &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;
      &lt;value&gt;/usr/local/zookeeper&lt;/value&gt;
      &lt;description&gt;Property from ZooKeeper's config zoo.cfg.
      The directory where the snapshot is stored.
      &lt;/description&gt;
    &lt;/property&gt;
    ...
  &lt;/configuration&gt;</pre><div class="caution" title="What verion of ZooKeeper should I use?" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a name="zk.version"></a>What verion of ZooKeeper should I use?</h3><p>The newer version, the better.  For example, some folks have been bitten by
          <a class="link" href="https://issues.apache.org/jira/browse/ZOOKEEPER-1277" target="_top">ZOOKEEPER-1277</a>.
          If running zookeeper 3.5+, you can ask hbase to make use of the new multi operation by
          enabling <a class="xref" href="#hbase.zookeeper.useMulti" title="hbase.zookeeper.useMulti"><code class="varname">hbase.zookeeper.useMulti</code></a>" in your <code class="filename">hbase-site.xml</code>.
      </p></div><div class="caution" title="ZooKeeper Maintenance" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">ZooKeeper Maintenance</h3><p>Be sure to set up the data dir cleaner described under
          <a class="link" href="http://zookeeper.apache.org/doc/r3.1.2/zookeeperAdmin.html#sc_maintenance" target="_top">Zookeeper Maintenance</a> else you could
          have 'interesting' problems a couple of months in; i.e. zookeeper could start
          dropping sessions if it has to run through a directory of hundreds of thousands of
          logs which is wont to do around leader reelection time -- a process rare but run on
      occasion whether because a machine is dropped or happens to hiccup.</p></div><div class="section" title="17.1.&nbsp;Using existing ZooKeeper ensemble"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="d366e13785"></a>17.1.&nbsp;Using existing ZooKeeper ensemble</h2></div></div></div><p>To point HBase at an existing ZooKeeper cluster, one that
              is not managed by HBase, set <code class="varname">HBASE_MANAGES_ZK</code>
              in <code class="filename">conf/hbase-env.sh</code> to false
              </p><pre class="programlisting">
  ...
  # Tell HBase whether it should manage its own instance of Zookeeper or not.
  export HBASE_MANAGES_ZK=false</pre><p> Next set ensemble locations
              and client port, if non-standard, in
              <code class="filename">hbase-site.xml</code>, or add a suitably
              configured <code class="filename">zoo.cfg</code> to HBase's
              <code class="filename">CLASSPATH</code>. HBase will prefer the
              configuration found in <code class="filename">zoo.cfg</code> over any
              settings in <code class="filename">hbase-site.xml</code>.</p><p>When HBase manages ZooKeeper, it will start/stop the
              ZooKeeper servers as a part of the regular start/stop scripts.
              If you would like to run ZooKeeper yourself, independent of
              HBase start/stop, you would do the following</p><pre class="programlisting">
${HBASE_HOME}/bin/hbase-daemons.sh {start,stop} zookeeper
</pre><p>Note that you can use HBase in this manner to spin up a
              ZooKeeper cluster, unrelated to HBase. Just make sure to set
              <code class="varname">HBASE_MANAGES_ZK</code> to <code class="varname">false</code>
              if you want it to stay up across HBase restarts so that when
              HBase shuts down, it doesn't take ZooKeeper down with it.</p><p>For more information about running a distinct ZooKeeper
              cluster, see the ZooKeeper <a class="link" href="http://hadoop.apache.org/zookeeper/docs/current/zookeeperStarted.html" target="_top">Getting
              Started Guide</a>.  Additionally, see the <a class="link" href="http://wiki.apache.org/hadoop/ZooKeeper/FAQ#A7" target="_top">ZooKeeper Wiki</a> or the
          <a class="link" href="http://zookeeper.apache.org/doc/r3.3.3/zookeeperAdmin.html#sc_zkMulitServerSetup" target="_top">ZooKeeper documentation</a>
          for more information on ZooKeeper sizing.
            </p></div><div class="section" title="17.2.&nbsp;SASL Authentication with ZooKeeper"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="zk.sasl.auth"></a>17.2.&nbsp;SASL Authentication with ZooKeeper</h2></div></div></div><p>Newer releases of Apache HBase (&gt;= 0.92) will
              support connecting to a ZooKeeper Quorum that supports
              SASL authentication (which is available in Zookeeper
              versions 3.4.0 or later).</p><p>This describes how to set up HBase to mutually
              authenticate with a ZooKeeper Quorum. ZooKeeper/HBase
              mutual authentication (<a class="link" href="https://issues.apache.org/jira/browse/HBASE-2418" target="_top">HBASE-2418</a>)
              is required as part of a complete secure HBase configuration
              (<a class="link" href="https://issues.apache.org/jira/browse/HBASE-3025" target="_top">HBASE-3025</a>).

              For simplicity of explication, this section ignores
              additional configuration required (Secure HDFS and Coprocessor
              configuration).  It's recommended to begin with an
              HBase-managed Zookeeper configuration (as opposed to a
              standalone Zookeeper quorum) for ease of learning.
              </p><div class="section" title="17.2.1.&nbsp;Operating System Prerequisites"><div class="titlepage"><div><div><h3 class="title"><a name="d366e13850"></a>17.2.1.&nbsp;Operating System Prerequisites</h3></div></div></div></div><p>
                  You need to have a working Kerberos KDC setup. For
                  each <code class="code">$HOST</code> that will run a ZooKeeper
                  server, you should have a principle
                  <code class="code">zookeeper/$HOST</code>.  For each such host,
                  add a service key (using the <code class="code">kadmin</code> or
                  <code class="code">kadmin.local</code> tool's <code class="code">ktadd</code>
                  command) for <code class="code">zookeeper/$HOST</code> and copy
                  this file to <code class="code">$HOST</code>, and make it
                  readable only to the user that will run zookeeper on
                  <code class="code">$HOST</code>. Note the location of this file,
                  which we will use below as
                  <code class="filename">$PATH_TO_ZOOKEEPER_KEYTAB</code>.
              </p><p>
                Similarly, for each <code class="code">$HOST</code> that will run
                an HBase server (master or regionserver), you should
                have a principle: <code class="code">hbase/$HOST</code>. For each
                host, add a keytab file called
                <code class="filename">hbase.keytab</code> containing a service
                key for <code class="code">hbase/$HOST</code>, copy this file to
                <code class="code">$HOST</code>, and make it readable only to the
                user that will run an HBase service on
                <code class="code">$HOST</code>. Note the location of this file,
                which we will use below as
                <code class="filename">$PATH_TO_HBASE_KEYTAB</code>.
              </p><p>
                Each user who will be an HBase client should also be
                given a Kerberos principal. This principal should
                usually have a password assigned to it (as opposed to,
                as with the HBase servers, a keytab file) which only
                this user knows. The client's principal's
                <code class="code">maxrenewlife</code> should be set so that it can
                be renewed enough so that the user can complete their
                HBase client processes. For example, if a user runs a
                long-running HBase client process that takes at most 3
                days, we might create this user's principal within
                <code class="code">kadmin</code> with: <code class="code">addprinc -maxrenewlife
                3days</code>. The Zookeeper client and server
                libraries manage their own ticket refreshment by
                running threads that wake up periodically to do the
                refreshment.
              </p><p>On each host that will run an HBase client
                (e.g. <code class="code">hbase shell</code>), add the following
                file to the HBase home directory's <code class="filename">conf</code>
                directory:</p><pre class="programlisting">
                  Client {
                    com.sun.security.auth.module.Krb5LoginModule required
                    useKeyTab=false
                    useTicketCache=true;
                  };
                </pre><p>We'll refer to this JAAS configuration file as
                <code class="filename">$CLIENT_CONF</code> below.</p><div class="section" title="17.2.2.&nbsp;HBase-managed Zookeeper Configuration"><div class="titlepage"><div><div><h3 class="title"><a name="d366e13931"></a>17.2.2.&nbsp;HBase-managed Zookeeper Configuration</h3></div></div></div><p>On each node that will run a zookeeper, a
                master, or a regionserver, create a <a class="link" href="http://docs.oracle.com/javase/1.4.2/docs/guide/security/jgss/tutorials/LoginConfigFile.html" target="_top">JAAS</a>
                configuration file in the conf directory of the node's
                <code class="filename">HBASE_HOME</code> directory that looks like the
                following:</p><pre class="programlisting">
                  Server {
                    com.sun.security.auth.module.Krb5LoginModule required
                    useKeyTab=true
                    keyTab="$PATH_TO_ZOOKEEPER_KEYTAB"
                    storeKey=true
                    useTicketCache=false
                    principal="zookeeper/$HOST";
                  };
                  Client {
                    com.sun.security.auth.module.Krb5LoginModule required
                    useKeyTab=true
                    useTicketCache=false
                    keyTab="$PATH_TO_HBASE_KEYTAB"
                    principal="hbase/$HOST";
                  };
                </pre>

                where the <code class="filename">$PATH_TO_HBASE_KEYTAB</code> and
                <code class="filename">$PATH_TO_ZOOKEEPER_KEYTAB</code> files are what
                you created above, and <code class="code">$HOST</code> is the hostname for that
                node.

                <p>The <code class="code">Server</code> section will be used by
                the Zookeeper quorum server, while the
                <code class="code">Client</code> section will be used by the HBase
                master and regionservers. The path to this file should
                be substituted for the text <code class="filename">$HBASE_SERVER_CONF</code>
                in the <code class="filename">hbase-env.sh</code>
                listing below.</p><p>
                  The path to this file should be substituted for the
                  text <code class="filename">$CLIENT_CONF</code> in the
                  <code class="filename">hbase-env.sh</code> listing below.
                </p><p>Modify your <code class="filename">hbase-env.sh</code> to include the
                following:</p><pre class="programlisting">
                  export HBASE_OPTS="-Djava.security.auth.login.config=$CLIENT_CONF"
                  export HBASE_MANAGES_ZK=true
                  export HBASE_ZOOKEEPER_OPTS="-Djava.security.auth.login.config=$HBASE_SERVER_CONF"
                  export HBASE_MASTER_OPTS="-Djava.security.auth.login.config=$HBASE_SERVER_CONF"
                  export HBASE_REGIONSERVER_OPTS="-Djava.security.auth.login.config=$HBASE_SERVER_CONF"
                </pre>

                where <code class="filename">$HBASE_SERVER_CONF</code> and
                <code class="filename">$CLIENT_CONF</code> are the full paths to the
                JAAS configuration files created above.

                <p>Modify your <code class="filename">hbase-site.xml</code> on each node
                that will run zookeeper, master or regionserver to contain:</p><pre class="programlisting">
                  &lt;configuration&gt;
                    &lt;property&gt;
                      &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;
                      &lt;value&gt;$ZK_NODES&lt;/value&gt;
                    &lt;/property&gt;
                    &lt;property&gt;
                      &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;
                      &lt;value&gt;true&lt;/value&gt;
                    &lt;/property&gt;
                    &lt;property&gt;
                      &lt;name&gt;hbase.zookeeper.property.authProvider.1&lt;/name&gt;
                      &lt;value&gt;org.apache.zookeeper.server.auth.SASLAuthenticationProvider&lt;/value&gt;
                    &lt;/property&gt;
                    &lt;property&gt;
                      &lt;name&gt;hbase.zookeeper.property.kerberos.removeHostFromPrincipal&lt;/name&gt;
                      &lt;value&gt;true&lt;/value&gt;
                    &lt;/property&gt;
                    &lt;property&gt;
                      &lt;name&gt;hbase.zookeeper.property.kerberos.removeRealmFromPrincipal&lt;/name&gt;
                      &lt;value&gt;true&lt;/value&gt;
                    &lt;/property&gt;
                  &lt;/configuration&gt;
                  </pre><p>where <code class="code">$ZK_NODES</code> is the
                comma-separated list of hostnames of the Zookeeper
                Quorum hosts.</p><p>Start your hbase cluster by running one or more
                of the following set of commands on the appropriate
                hosts:
                </p><pre class="programlisting">
                  bin/hbase zookeeper start
                  bin/hbase master start
                  bin/hbase regionserver start
                </pre></div><div class="section" title="17.2.3.&nbsp;External Zookeeper Configuration"><div class="titlepage"><div><div><h3 class="title"><a name="d366e14006"></a>17.2.3.&nbsp;External Zookeeper Configuration</h3></div></div></div><p>Add a JAAS configuration file that looks like:

                </p><pre class="programlisting">
                  Client {
                    com.sun.security.auth.module.Krb5LoginModule required
                    useKeyTab=true
                    useTicketCache=false
                    keyTab="$PATH_TO_HBASE_KEYTAB"
                    principal="hbase/$HOST";
                  };
                </pre><p>

                where the <code class="filename">$PATH_TO_HBASE_KEYTAB</code> is the keytab
                created above for HBase services to run on this host, and <code class="code">$HOST</code> is the
                hostname for that node. Put this in the HBase home's
                configuration directory. We'll refer to this file's
                full pathname as <code class="filename">$HBASE_SERVER_CONF</code> below.</p><p>Modify your hbase-env.sh to include the following:</p><pre class="programlisting">
                  export HBASE_OPTS="-Djava.security.auth.login.config=$CLIENT_CONF"
                  export HBASE_MANAGES_ZK=false
                  export HBASE_MASTER_OPTS="-Djava.security.auth.login.config=$HBASE_SERVER_CONF"
                  export HBASE_REGIONSERVER_OPTS="-Djava.security.auth.login.config=$HBASE_SERVER_CONF"
                </pre><p>Modify your <code class="filename">hbase-site.xml</code> on each node
                that will run a master or regionserver to contain:</p><pre class="programlisting">
                  &lt;configuration&gt;
                    &lt;property&gt;
                      &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;
                      &lt;value&gt;$ZK_NODES&lt;/value&gt;
                    &lt;/property&gt;
                    &lt;property&gt;
                      &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;
                      &lt;value&gt;true&lt;/value&gt;
                    &lt;/property&gt;
                  &lt;/configuration&gt;
                  
                </pre><p>where <code class="code">$ZK_NODES</code> is the
                comma-separated list of hostnames of the Zookeeper
                Quorum hosts.</p><p>
                  Add a <code class="filename">zoo.cfg</code> for each Zookeeper Quorum host containing:
                  </p><pre class="programlisting">
                      authProvider.1=org.apache.zookeeper.server.auth.SASLAuthenticationProvider
                      kerberos.removeHostFromPrincipal=true
                      kerberos.removeRealmFromPrincipal=true
                  </pre><p>

                  Also on each of these hosts, create a JAAS configuration file containing:

                  </p><pre class="programlisting">
                  Server {
                    com.sun.security.auth.module.Krb5LoginModule required
                    useKeyTab=true
                    keyTab="$PATH_TO_ZOOKEEPER_KEYTAB"
                    storeKey=true
                    useTicketCache=false
                    principal="zookeeper/$HOST";
                  };
                  </pre><p>

                  where <code class="code">$HOST</code> is the hostname of each
                  Quorum host. We will refer to the full pathname of
                  this file as <code class="filename">$ZK_SERVER_CONF</code> below.

                </p><p>
                  Start your Zookeepers on each Zookeeper Quorum host with:

                  </p><pre class="programlisting">
                    SERVER_JVMFLAGS="-Djava.security.auth.login.config=$ZK_SERVER_CONF" bin/zkServer start
                  </pre><p>

                </p><p>
                  Start your HBase cluster by running one or more of the following set of commands on the appropriate nodes:
                </p><pre class="programlisting">
                  bin/hbase master start
                  bin/hbase regionserver start
                </pre></div><div class="section" title="17.2.4.&nbsp;Zookeeper Server Authentication Log Output"><div class="titlepage"><div><div><h3 class="title"><a name="d366e14065"></a>17.2.4.&nbsp;Zookeeper Server Authentication Log Output</h3></div></div></div><p>If the configuration above is successful,
                you should see something similar to the following in
                your Zookeeper server logs:
                </p><pre class="programlisting">
11/12/05 22:43:39 INFO zookeeper.Login: successfully logged in.
11/12/05 22:43:39 INFO server.NIOServerCnxnFactory: binding to port 0.0.0.0/0.0.0.0:2181
11/12/05 22:43:39 INFO zookeeper.Login: TGT refresh thread started.
11/12/05 22:43:39 INFO zookeeper.Login: TGT valid starting at:        Mon Dec 05 22:43:39 UTC 2011
11/12/05 22:43:39 INFO zookeeper.Login: TGT expires:                  Tue Dec 06 22:43:39 UTC 2011
11/12/05 22:43:39 INFO zookeeper.Login: TGT refresh sleeping until: Tue Dec 06 18:36:42 UTC 2011
..
11/12/05 22:43:59 INFO auth.SaslServerCallbackHandler:
  Successfully authenticated client: authenticationID=hbase/ip-10-166-175-249.us-west-1.compute.internal@HADOOP.LOCALDOMAIN;
  authorizationID=hbase/ip-10-166-175-249.us-west-1.compute.internal@HADOOP.LOCALDOMAIN.
11/12/05 22:43:59 INFO auth.SaslServerCallbackHandler: Setting authorizedID: hbase
11/12/05 22:43:59 INFO server.ZooKeeperServer: adding SASL authorization for authorizationID: hbase
                </pre><p>

                </p></div><div class="section" title="17.2.5.&nbsp;Zookeeper Client Authentication Log Output"><div class="titlepage"><div><div><h3 class="title"><a name="d366e14073"></a>17.2.5.&nbsp;Zookeeper Client Authentication Log Output</h3></div></div></div><p>On the Zookeeper client side (HBase master or regionserver),
                you should see something similar to the following:

                </p><pre class="programlisting">
11/12/05 22:43:59 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=ip-10-166-175-249.us-west-1.compute.internal:2181 sessionTimeout=180000 watcher=master:60000
11/12/05 22:43:59 INFO zookeeper.ClientCnxn: Opening socket connection to server /10.166.175.249:2181
11/12/05 22:43:59 INFO zookeeper.RecoverableZooKeeper: The identifier of this process is 14851@ip-10-166-175-249
11/12/05 22:43:59 INFO zookeeper.Login: successfully logged in.
11/12/05 22:43:59 INFO client.ZooKeeperSaslClient: Client will use GSSAPI as SASL mechanism.
11/12/05 22:43:59 INFO zookeeper.Login: TGT refresh thread started.
11/12/05 22:43:59 INFO zookeeper.ClientCnxn: Socket connection established to ip-10-166-175-249.us-west-1.compute.internal/10.166.175.249:2181, initiating session
11/12/05 22:43:59 INFO zookeeper.Login: TGT valid starting at:        Mon Dec 05 22:43:59 UTC 2011
11/12/05 22:43:59 INFO zookeeper.Login: TGT expires:                  Tue Dec 06 22:43:59 UTC 2011
11/12/05 22:43:59 INFO zookeeper.Login: TGT refresh sleeping until: Tue Dec 06 18:30:37 UTC 2011
11/12/05 22:43:59 INFO zookeeper.ClientCnxn: Session establishment complete on server ip-10-166-175-249.us-west-1.compute.internal/10.166.175.249:2181, sessionid = 0x134106594320000, negotiated timeout = 180000
                </pre><p>
                </p></div><div class="section" title="17.2.6.&nbsp;Configuration from Scratch"><div class="titlepage"><div><div><h3 class="title"><a name="d366e14081"></a>17.2.6.&nbsp;Configuration from Scratch</h3></div></div></div>

                This has been tested on the current standard Amazon
                Linux AMI.  First setup KDC and principals as
                described above. Next checkout code and run a sanity
                check.

                <pre class="programlisting">
                git clone git://git.apache.org/hbase.git
                cd hbase
                mvn clean test -Dtest=TestZooKeeperACL
                </pre>

                Then configure HBase as described above.
                Manually edit target/cached_classpath.txt (see below)..

                <pre class="programlisting">
                bin/hbase zookeeper &amp;
                bin/hbase master &amp;
                bin/hbase regionserver &amp;
                </pre></div><div class="section" title="17.2.7.&nbsp;Future improvements"><div class="titlepage"><div><div><h3 class="title"><a name="d366e14090"></a>17.2.7.&nbsp;Future improvements</h3></div></div></div><div class="section" title="17.2.7.1.&nbsp;Fix target/cached_classpath.txt"><div class="titlepage"><div><div><h4 class="title"><a name="d366e14093"></a>17.2.7.1.&nbsp;Fix target/cached_classpath.txt</h4></div></div></div><p>
                You must override the standard hadoop-core jar file from the
                <code class="code">target/cached_classpath.txt</code>
                file with the version containing the HADOOP-7070 fix. You can use the following script to do this:

                </p><pre class="programlisting">
                  echo `find ~/.m2 -name "*hadoop-core*7070*SNAPSHOT.jar"` ':' `cat target/cached_classpath.txt` | sed 's/ //g' &gt; target/tmp.txt
                  mv target/tmp.txt target/cached_classpath.txt
                </pre><p>

                </p></div><div class="section" title="17.2.7.2.&nbsp;Set JAAS configuration programmatically"><div class="titlepage"><div><div><h4 class="title"><a name="d366e14104"></a>17.2.7.2.&nbsp;Set JAAS configuration
                  programmatically</h4></div></div></div>


                  This would avoid the need for a separate Hadoop jar
                  that fixes <a class="link" href="https://issues.apache.org/jira/browse/HADOOP-7070" target="_top">HADOOP-7070</a>.
                </div><div class="section" title="17.2.7.3.&nbsp;Elimination of kerberos.removeHostFromPrincipal and kerberos.removeRealmFromPrincipal"><div class="titlepage"><div><div><h4 class="title"><a name="d366e14111"></a>17.2.7.3.&nbsp;Elimination of
                  <code class="code">kerberos.removeHostFromPrincipal</code> and
                  <code class="code">kerberos.removeRealmFromPrincipal</code></h4></div></div></div></div></div></div><div class="footnotes"><br><hr width="100" align="left"><div class="footnote"><p><sup>[<a id="ftn.d366e13700" href="#d366e13700" class="para">36</a>] </sup>For the full list of ZooKeeper configurations, see
                ZooKeeper's <code class="filename">zoo.cfg</code>. HBase does not ship
                with a <code class="filename">zoo.cfg</code> so you will need to browse
                the <code class="filename">conf</code> directory in an appropriate
                ZooKeeper download.</p></div></div></div><div class="chapter" title="Chapter&nbsp;18.&nbsp;Apache HBase Coprocessors"><div class="titlepage"><div><div><h2 class="title"><a name="cp"></a>Chapter&nbsp;18.&nbsp;Apache HBase Coprocessors</h2></div></div></div><p>The idea of HBase coprocessors was inspired by Google's BigTable coprocessors. The <a class="link" href="https://blogs.apache.org/hbase/entry/coprocessor_introduction" target="_top">Apache HBase Blog on Coprocessor</a> is a very good documentation on that. It has detailed information about the coprocessor framework, terminology, management, and so on.
  </p></div><div class="chapter" title="Chapter&nbsp;19.&nbsp;Community"><div class="titlepage"><div><div><h2 class="title"><a name="community"></a>Chapter&nbsp;19.&nbsp;Community</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#decisions">19.1. Decisions</a></span></dt><dd><dl><dt><span class="section"><a href="#feature_branches">19.1.1. Feature Branches</a></span></dt><dt><span class="section"><a href="#patchplusonepolicy">19.1.2. Patch +1 Policy</a></span></dt><dt><span class="section"><a href="#hbase.fix.version.in.JIRA">19.1.3. How to set fix version in JIRA on issue resolve</a></span></dt><dt><span class="section"><a href="#hbase.when.to.close.JIRA">19.1.4. Policy on when to set a RESOLVED JIRA as CLOSED</a></span></dt><dt><span class="section"><a href="#no.permanent.state.in.zk">19.1.5. Only transient state in ZooKeeper!</a></span></dt></dl></dd><dt><span class="section"><a href="#community.roles">19.2. Community Roles</a></span></dt><dd><dl><dt><span class="section"><a href="#OWNER">19.2.1. Component Owner/Lieutenant</a></span></dt></dl></dd><dt><span class="section"><a href="#hbase.commit.msg.format">19.3. Commit Message format</a></span></dt></dl></div><div class="section" title="19.1.&nbsp;Decisions"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="decisions"></a>19.1.&nbsp;Decisions</h2></div></div></div><div class="section" title="19.1.1.&nbsp;Feature Branches"><div class="titlepage"><div><div><h3 class="title"><a name="feature_branches"></a>19.1.1.&nbsp;Feature Branches</h3></div></div></div><p>Feature Branches are easy to make.  You do not have to be a committer to make one.  Just request the name of your branch be added to JIRA up on the
        developer's mailing list and a committer will add it for you.  Thereafter you can file issues against your feature branch in Apache HBase JIRA.  Your code you
        keep elsewhere -- it should be public so it can be observed -- and you can update dev mailing list on progress.   When the feature is ready for commit,
        3 +1s from committers will get your feature merged<sup>[<a name="d366e14141" href="#ftn.d366e14141" class="footnote">37</a>]</sup>
        </p></div><div class="section" title="19.1.2.&nbsp;Patch +1 Policy"><div class="titlepage"><div><div><h3 class="title"><a name="patchplusonepolicy"></a>19.1.2.&nbsp;Patch +1 Policy</h3></div></div></div><p>
The below policy is something we put in place 09/2012.  It is a
suggested policy rather than a hard requirement.  We want to try it
first to see if it works before we cast it in stone.
        </p><p>
Apache HBase is made of
<a class="link" href="https://issues.apache.org/jira/browse/HBASE#selectedTab=com.atlassian.jira.plugin.system.project%3Acomponents-panel" target="_top">components</a>.
Components have one or more <a class="xref" href="#OWNER" title="19.2.1.&nbsp;Component Owner/Lieutenant">Section&nbsp;19.2.1, &#8220;Component Owner/Lieutenant&#8221;</a>s.  See the 'Description' field on the
<a class="link" href="https://issues.apache.org/jira/browse/HBASE#selectedTab=com.atlassian.jira.plugin.system.project%3Acomponents-panel" target="_top">components</a>
JIRA page for who the current owners are by component.
</p><p>
Patches that fit within the scope of a single Apache HBase component require,
at least, a +1 by one of the component's owners before commit. If
owners are absent -- busy or otherwise -- two +1s by non-owners will
suffice.
</p><p>
Patches that span components need at least two +1s before they can be
committed, preferably +1s by owners of components touched by the
x-component patch (TODO: This needs tightening up but I think fine for
first pass).
</p><p>
Any -1 on a patch by anyone vetos a patch; it cannot be committed
until the justification for the -1 is addressed.
</p></div><div class="section" title="19.1.3.&nbsp;How to set fix version in JIRA on issue resolve"><div class="titlepage"><div><div><h3 class="title"><a name="hbase.fix.version.in.JIRA"></a>19.1.3.&nbsp;How to set fix version in JIRA on issue resolve</h3></div></div></div><p>Here is how <a class="link" href="http://search-hadoop.com/m/azemIi5RCJ1" target="_top">we agreed</a> to set versions in JIRA when we
              resolve an issue.  If trunk is going to be 0.98.0 then:
              </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p>
              Commit only to trunk: Mark with 0.98
                </p></li><li class="listitem"><p>
              Commit to 0.95 and trunk : Mark with 0.98, and 0.95.x
                </p></li><li class="listitem"><p>
              Commit to 0.94.x and 0.95, and trunk: Mark with 0.98, 0.95.x, and 0.94.x
                </p></li><li class="listitem"><p>
              Commit to 89-fb: Mark with 89-fb.
                </p></li><li class="listitem"><p>
              Commit site fixes: no version
                </p></li></ul></div><p>
          </p></div><div class="section" title="19.1.4.&nbsp;Policy on when to set a RESOLVED JIRA as CLOSED"><div class="titlepage"><div><div><h3 class="title"><a name="hbase.when.to.close.JIRA"></a>19.1.4.&nbsp;Policy on when to set a RESOLVED JIRA as CLOSED</h3></div></div></div><p>We <a class="link" href="http://search-hadoop.com/m/4cIKs1iwXMS1" target="_top">agreed</a>
              that for issues that list multiple releases in their <span class="emphasis"><em>Fix Version/s</em></span> field,
              CLOSE the issue on the release of any of the versions listed; subsequent change
              to the issue must happen in a new JIRA.
          </p></div><div class="section" title="19.1.5.&nbsp;Only transient state in ZooKeeper!"><div class="titlepage"><div><div><h3 class="title"><a name="no.permanent.state.in.zk"></a>19.1.5.&nbsp;Only transient state in ZooKeeper!</h3></div></div></div><p>
You should be able to kill the data in zookeeper and hbase should ride over it recreating the zk content as it goes.
This is an old adage around these parts.  We just made note of it now.  We also are currently in violation of this
basic tenet -- replication at least keeps permanent state in zk -- but we are working to undo this breaking of a
golden rule.
          </p></div></div><div class="section" title="19.2.&nbsp;Community Roles"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="community.roles"></a>19.2.&nbsp;Community Roles</h2></div></div></div><div class="section" title="19.2.1.&nbsp;Component Owner/Lieutenant"><div class="titlepage"><div><div><h3 class="title"><a name="OWNER"></a>19.2.1.&nbsp;Component Owner/Lieutenant</h3></div></div></div><p>
Component owners are listed in the description field on this Apache HBase JIRA <a class="link" href="https://issues.apache.org/jira/browse/HBASE#selectedTab=com.atlassian.jira.plugin.system.project%3Acomponents-panel" target="_top">components</a>
page.  The owners are listed in the 'Description' field rather than in the 'Component
Lead' field because the latter only allows us list one individual
whereas it is encouraged that components have multiple owners.
        </p><p>
Owners or component lieutenants are volunteers who are (usually, but not necessarily) expert in
their component domain and may have an agenda on how they think their
Apache HBase component should evolve.
</p><p>
Duties include:
</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
Owners will try and review patches that land within their component's scope.
</p></li><li class="listitem"><p>
If applicable, if an owner has an agenda, they will publish their
goals or the design toward which they are driving their component
</p></li></ol></div><p>
</p><p>
If you would like to be volunteer as a component owner, just write the
dev list and we'll sign you up. Owners do not need to be committers.
</p></div></div><div class="section" title="19.3.&nbsp;Commit Message format"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="hbase.commit.msg.format"></a>19.3.&nbsp;Commit Message format</h2></div></div></div><p>We <a class="link" href="http://search-hadoop.com/m/Gwxwl10cFHa1" target="_top">agreed</a>
          to the following SVN commit message format:
</p><pre class="programlisting">HBASE-xxxxx &lt;title&gt;. (&lt;contributor&gt;)</pre><p>
If the person making the commit is the contributor, leave off the '(&lt;contributor&gt;)' element.
          </p></div><div class="footnotes"><br><hr width="100" align="left"><div class="footnote"><p><sup>[<a id="ftn.d366e14141" href="#d366e14141" class="para">37</a>] </sup>See <a class="link" href="http://search-hadoop.com/m/asM982C5FkS1" target="_top">HBase, mail # dev - Thoughts about large feature dev branches</a></p></div></div></div><div class="appendix" title="Appendix&nbsp;A.&nbsp;FAQ"><div class="titlepage"><div><div><h2 class="title"><a name="faq"></a>Appendix&nbsp;A.&nbsp;FAQ</h2></div></div></div><div class="qandaset" title="Frequently Asked Questions"><a name="d366e14248"></a><dl><dt>A.1.  <a href="#d366e14249">General</a></dt><dd><dl><dt> <a href="#d366e14252">When should I use HBase?</a></dt><dt> <a href="#d366e14261">Are there other HBase FAQs?</a></dt><dt> <a href="#faq.sql">Does HBase support SQL?</a></dt><dt> <a href="#d366e14283">How can I find examples of NoSQL/HBase?</a></dt><dt> <a href="#d366e14292">What is the history of HBase?</a></dt></dl></dd><dt>A.2.  <a href="#faq.arch">Architecture</a></dt><dd><dl><dt> <a href="#faq.arch.regions">How does HBase handle Region-RegionServer assignment and locality?</a></dt></dl></dd><dt>A.3.  <a href="#faq.config">Configuration</a></dt><dd><dl><dt> <a href="#faq.config.started">How can I get started with my first cluster?</a></dt><dt> <a href="#faq.config.started">Where can I learn about the rest of the configuration options?</a></dt></dl></dd><dt>A.4.  <a href="#faq.design">Schema Design / Data Access</a></dt><dd><dl><dt> <a href="#faq.design.schema">How should I design my schema in HBase?</a></dt><dt> <a href="#d366e14348">
                    How can I store (fill in the blank) in HBase?
            </a></dt><dt> <a href="#secondary.indices">
                    How can I handle secondary indexes in HBase?
            </a></dt><dt> <a href="#faq.changing.rowkeys">Can I change a table's rowkeys?</a></dt><dt> <a href="#faq.apis">What APIs does HBase support?</a></dt></dl></dd><dt>A.5.  <a href="#faq.mapreduce">MapReduce</a></dt><dd><dl><dt> <a href="#faq.mapreduce.use">How can I use MapReduce with HBase?</a></dt></dl></dd><dt>A.6.  <a href="#d366e14400">Performance and Troubleshooting</a></dt><dd><dl><dt> <a href="#d366e14403">
                   How can I improve HBase cluster performance?
            </a></dt><dt> <a href="#d366e14412">
                    How can I troubleshoot my HBase cluster?
            </a></dt></dl></dd><dt>A.7.  <a href="#ec2">Amazon EC2</a></dt><dd><dl><dt> <a href="#d366e14424">
            I am running HBase on Amazon EC2 and...
            </a></dt></dl></dd><dt>A.8.  <a href="#d366e14435">Operations</a></dt><dd><dl><dt> <a href="#d366e14438">
                    How do I manage my HBase cluster?
            </a></dt><dt> <a href="#d366e14447">
                    How do I back up my HBase cluster?
            </a></dt></dl></dd><dt>A.9.  <a href="#d366e14456">HBase in Action</a></dt><dd><dl><dt> <a href="#d366e14459">Where can I find interesting videos and presentations on HBase?</a></dt></dl></dd></dl><table border="0" width="100%" summary="Q and A Set"><col align="left" width="1%"><col><tbody><tr class="qandadiv"><td align="left" valign="top" colspan="2"><h3 class="title"><a name="d366e14249"></a>A.1. General</h3></td></tr><tr class="toc"><td align="left" valign="top" colspan="2"><dl><dt> <a href="#d366e14252">When should I use HBase?</a></dt><dt> <a href="#d366e14261">Are there other HBase FAQs?</a></dt><dt> <a href="#faq.sql">Does HBase support SQL?</a></dt><dt> <a href="#d366e14283">How can I find examples of NoSQL/HBase?</a></dt><dt> <a href="#d366e14292">What is the history of HBase?</a></dt></dl></td></tr><tr class="question"><td align="left" valign="top"><a name="d366e14252"></a><a name="d366e14253"></a></td><td align="left" valign="top"><p>When should I use HBase?</p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>See the <a class="xref" href="#arch.overview" title="9.1.&nbsp;Overview">Section&nbsp;9.1, &#8220;Overview&#8221;</a> in the Architecture chapter.
                </p></td></tr><tr class="question"><td align="left" valign="top"><a name="d366e14261"></a><a name="d366e14262"></a></td><td align="left" valign="top"><p>Are there other HBase FAQs?</p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>
              See the FAQ that is up on the wiki, <a class="link" href="http://wiki.apache.org/hadoop/Hbase/FAQ" target="_top">HBase Wiki FAQ</a>.
                </p></td></tr><tr class="question"><td align="left" valign="top"><a name="faq.sql"></a><a name="d366e14272"></a></td><td align="left" valign="top"><p>Does HBase support SQL?</p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>
                    Not really.  SQL-ish support for HBase via <a class="link" href="http://hive.apache.org/" target="_top">Hive</a> is in development, however Hive is based on MapReduce which is not generally suitable for low-latency requests.
                    See the <a class="xref" href="#datamodel" title="Chapter&nbsp;5.&nbsp;Data Model">Chapter&nbsp;5, <i>Data Model</i></a> section for examples on the HBase client.
                </p></td></tr><tr class="question"><td align="left" valign="top"><a name="d366e14283"></a><a name="d366e14284"></a></td><td align="left" valign="top"><p>How can I find examples of NoSQL/HBase?</p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>See the link to the BigTable paper in <a class="xref" href="#other.info" title="Appendix&nbsp;F.&nbsp;Other Information About HBase">Appendix&nbsp;F, <i>Other Information About HBase</i></a> in the appendix, as
                well as the other papers.
                </p></td></tr><tr class="question"><td align="left" valign="top"><a name="d366e14292"></a><a name="d366e14293"></a></td><td align="left" valign="top"><p>What is the history of HBase?</p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>See <a class="xref" href="#hbase.history" title="Appendix&nbsp;G.&nbsp;HBase History">Appendix&nbsp;G, <i>HBase History</i></a>.
                </p></td></tr><tr class="qandadiv"><td align="left" valign="top" colspan="2"><h3 class="title"><a name="faq.arch"></a>A.2. Architecture</h3></td></tr><tr class="toc"><td align="left" valign="top" colspan="2"><dl><dt> <a href="#faq.arch.regions">How does HBase handle Region-RegionServer assignment and locality?</a></dt></dl></td></tr><tr class="question"><td align="left" valign="top"><a name="faq.arch.regions"></a><a name="d366e14305"></a></td><td align="left" valign="top"><p>How does HBase handle Region-RegionServer assignment and locality?</p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>
                    See <a class="xref" href="#regions.arch" title="9.7.&nbsp;Regions">Section&nbsp;9.7, &#8220;Regions&#8221;</a>.
                </p></td></tr><tr class="qandadiv"><td align="left" valign="top" colspan="2"><h3 class="title"><a name="faq.config"></a>A.3. Configuration</h3></td></tr><tr class="toc"><td align="left" valign="top" colspan="2"><dl><dt> <a href="#faq.config.started">How can I get started with my first cluster?</a></dt><dt> <a href="#faq.config.started">Where can I learn about the rest of the configuration options?</a></dt></dl></td></tr><tr class="question"><td align="left" valign="top"><a name="faq.config.started"></a><a name="d366e14317"></a></td><td align="left" valign="top"><p>How can I get started with my first cluster?</p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>
                    See <a class="xref" href="#quickstart" title="1.2.&nbsp;Quick Start">Section&nbsp;1.2, &#8220;Quick Start&#8221;</a>.
                </p></td></tr><tr class="question"><td align="left" valign="top"><a name="faq.config.started"></a><a name="d366e14326"></a></td><td align="left" valign="top"><p>Where can I learn about the rest of the configuration options?</p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>
                    See <a class="xref" href="#configuration" title="Chapter&nbsp;2.&nbsp;Apache HBase Configuration">Chapter&nbsp;2, <i>Apache HBase Configuration</i></a>.
                </p></td></tr><tr class="qandadiv"><td align="left" valign="top" colspan="2"><h3 class="title"><a name="faq.design"></a>A.4. Schema Design / Data Access</h3></td></tr><tr class="toc"><td align="left" valign="top" colspan="2"><dl><dt> <a href="#faq.design.schema">How should I design my schema in HBase?</a></dt><dt> <a href="#d366e14348">
                    How can I store (fill in the blank) in HBase?
            </a></dt><dt> <a href="#secondary.indices">
                    How can I handle secondary indexes in HBase?
            </a></dt><dt> <a href="#faq.changing.rowkeys">Can I change a table's rowkeys?</a></dt><dt> <a href="#faq.apis">What APIs does HBase support?</a></dt></dl></td></tr><tr class="question"><td align="left" valign="top"><a name="faq.design.schema"></a><a name="d366e14338"></a></td><td align="left" valign="top"><p>How should I design my schema in HBase?</p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>
                    See <a class="xref" href="#datamodel" title="Chapter&nbsp;5.&nbsp;Data Model">Chapter&nbsp;5, <i>Data Model</i></a> and <a class="xref" href="#schema" title="Chapter&nbsp;6.&nbsp;HBase and Schema Design">Chapter&nbsp;6, <i>HBase and Schema Design</i></a>
                </p></td></tr><tr class="question"><td align="left" valign="top"><a name="d366e14348"></a><a name="d366e14349"></a></td><td align="left" valign="top"><p>
                    How can I store (fill in the blank) in HBase?
            </p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>
                See <a class="xref" href="#supported.datatypes" title="6.5.&nbsp; Supported Datatypes">Section&nbsp;6.5, &#8220;
  Supported Datatypes
  &#8221;</a>.
                </p></td></tr><tr class="question"><td align="left" valign="top"><a name="secondary.indices"></a><a name="d366e14358"></a></td><td align="left" valign="top"><p>
                    How can I handle secondary indexes in HBase?
            </p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>
                See <a class="xref" href="#secondary.indexes" title="6.9.&nbsp; Secondary Indexes and Alternate Query Paths">Section&nbsp;6.9, &#8220;
  Secondary Indexes and Alternate Query Paths
  &#8221;</a>
                </p></td></tr><tr class="question"><td align="left" valign="top"><a name="faq.changing.rowkeys"></a><a name="d366e14367"></a></td><td align="left" valign="top"><p>Can I change a table's rowkeys?</p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>
                    This is a very common quesiton.  You can't.  See <a class="xref" href="#changing.rowkeys" title="6.3.5.&nbsp;Immutability of Rowkeys">Section&nbsp;6.3.5, &#8220;Immutability of Rowkeys&#8221;</a>.
                </p></td></tr><tr class="question"><td align="left" valign="top"><a name="faq.apis"></a><a name="d366e14376"></a></td><td align="left" valign="top"><p>What APIs does HBase support?</p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>
                    See <a class="xref" href="#datamodel" title="Chapter&nbsp;5.&nbsp;Data Model">Chapter&nbsp;5, <i>Data Model</i></a>, <a class="xref" href="#client" title="9.3.&nbsp;Client">Section&nbsp;9.3, &#8220;Client&#8221;</a> and <a class="xref" href="#nonjava.jvm" title="10.1.&nbsp;Non-Java Languages Talking to the JVM">Section&nbsp;10.1, &#8220;Non-Java Languages Talking to the JVM&#8221;</a>.
                </p></td></tr><tr class="qandadiv"><td align="left" valign="top" colspan="2"><h3 class="title"><a name="faq.mapreduce"></a>A.5. MapReduce</h3></td></tr><tr class="toc"><td align="left" valign="top" colspan="2"><dl><dt> <a href="#faq.mapreduce.use">How can I use MapReduce with HBase?</a></dt></dl></td></tr><tr class="question"><td align="left" valign="top"><a name="faq.mapreduce.use"></a><a name="d366e14392"></a></td><td align="left" valign="top"><p>How can I use MapReduce with HBase?</p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>
                    See <a class="xref" href="#mapreduce" title="Chapter&nbsp;7.&nbsp;HBase and MapReduce">Chapter&nbsp;7, <i>HBase and MapReduce</i></a>
                </p></td></tr><tr class="qandadiv"><td align="left" valign="top" colspan="2"><h3 class="title"><a name="d366e14400"></a>A.6. Performance and Troubleshooting</h3></td></tr><tr class="toc"><td align="left" valign="top" colspan="2"><dl><dt> <a href="#d366e14403">
                   How can I improve HBase cluster performance?
            </a></dt><dt> <a href="#d366e14412">
                    How can I troubleshoot my HBase cluster?
            </a></dt></dl></td></tr><tr class="question"><td align="left" valign="top"><a name="d366e14403"></a><a name="d366e14404"></a></td><td align="left" valign="top"><p>
                   How can I improve HBase cluster performance?
            </p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>
                See <a class="xref" href="#performance" title="Chapter&nbsp;12.&nbsp;Apache HBase Performance Tuning">Chapter&nbsp;12, <i>Apache HBase Performance Tuning</i></a>.
                </p></td></tr><tr class="question"><td align="left" valign="top"><a name="d366e14412"></a><a name="d366e14413"></a></td><td align="left" valign="top"><p>
                    How can I troubleshoot my HBase cluster?
            </p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>
                See <a class="xref" href="#trouble" title="Chapter&nbsp;13.&nbsp;Troubleshooting and Debugging Apache HBase">Chapter&nbsp;13, <i>Troubleshooting and Debugging Apache HBase</i></a>.
                </p></td></tr><tr class="qandadiv"><td align="left" valign="top" colspan="2"><h3 class="title"><a name="ec2"></a>A.7. Amazon EC2</h3></td></tr><tr class="toc"><td align="left" valign="top" colspan="2"><dl><dt> <a href="#d366e14424">
            I am running HBase on Amazon EC2 and...
            </a></dt></dl></td></tr><tr class="question"><td align="left" valign="top"><a name="d366e14424"></a><a name="d366e14425"></a></td><td align="left" valign="top"><p>
            I am running HBase on Amazon EC2 and...
            </p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>
 	            EC2 issues are a special case.  See Troubleshooting <a class="xref" href="#trouble.ec2" title="13.12.&nbsp;Amazon EC2">Section&nbsp;13.12, &#8220;Amazon EC2&#8221;</a> and Performance <a class="xref" href="#perf.ec2" title="12.12.&nbsp;Amazon EC2">Section&nbsp;12.12, &#8220;Amazon EC2&#8221;</a> sections.
               </p></td></tr><tr class="qandadiv"><td align="left" valign="top" colspan="2"><h3 class="title"><a name="d366e14435"></a>A.8. Operations</h3></td></tr><tr class="toc"><td align="left" valign="top" colspan="2"><dl><dt> <a href="#d366e14438">
                    How do I manage my HBase cluster?
            </a></dt><dt> <a href="#d366e14447">
                    How do I back up my HBase cluster?
            </a></dt></dl></td></tr><tr class="question"><td align="left" valign="top"><a name="d366e14438"></a><a name="d366e14439"></a></td><td align="left" valign="top"><p>
                    How do I manage my HBase cluster?
            </p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>
                    See <a class="xref" href="#ops_mgt" title="Chapter&nbsp;15.&nbsp;Apache HBase Operational Management">Chapter&nbsp;15, <i>Apache HBase Operational Management</i></a>
                </p></td></tr><tr class="question"><td align="left" valign="top"><a name="d366e14447"></a><a name="d366e14448"></a></td><td align="left" valign="top"><p>
                    How do I back up my HBase cluster?
            </p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>
                    See <a class="xref" href="#ops.backup" title="15.7.&nbsp;HBase Backup">Section&nbsp;15.7, &#8220;HBase Backup&#8221;</a>
                </p></td></tr><tr class="qandadiv"><td align="left" valign="top" colspan="2"><h3 class="title"><a name="d366e14456"></a>A.9. HBase in Action</h3></td></tr><tr class="toc"><td align="left" valign="top" colspan="2"><dl><dt> <a href="#d366e14459">Where can I find interesting videos and presentations on HBase?</a></dt></dl></td></tr><tr class="question"><td align="left" valign="top"><a name="d366e14459"></a><a name="d366e14460"></a></td><td align="left" valign="top"><p>Where can I find interesting videos and presentations on HBase?</p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>
                    See <a class="xref" href="#other.info" title="Appendix&nbsp;F.&nbsp;Other Information About HBase">Appendix&nbsp;F, <i>Other Information About HBase</i></a>
                </p></td></tr></tbody></table></div></div><div class="appendix" title="Appendix&nbsp;B.&nbsp;hbck In Depth"><div class="titlepage"><div><div><h2 class="title"><a name="hbck.in.depth"></a>Appendix&nbsp;B.&nbsp;hbck In Depth</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#d366e14473">B.1. Running hbck to identify inconsistencies</a></span></dt><dt><span class="section"><a href="#d366e14489">B.2. Inconsistencies</a></span></dt><dt><span class="section"><a href="#d366e14508">B.3. Localized repairs</a></span></dt><dt><span class="section"><a href="#d366e14546">B.4. Region Overlap Repairs</a></span></dt><dd><dl><dt><span class="section"><a href="#d366e14585">B.4.1. Special cases: Meta is not properly assigned</a></span></dt><dt><span class="section"><a href="#d366e14594">B.4.2. Special cases: HBase version file is missing</a></span></dt><dt><span class="section"><a href="#d366e14601">B.4.3. Special case: Root and META are corrupt.</a></span></dt><dt><span class="section"><a href="#d366e14608">B.4.4. Special cases: Offline split parent</a></span></dt></dl></dd></dl></div><p>HBaseFsck (hbck) is a tool for checking for region consistency and table integrity problems
and repairing a corrupted HBase. It works in two basic modes -- a read-only inconsistency
identifying mode and a multi-phase read-write repair mode.
	</p><div class="section" title="B.1.&nbsp;Running hbck to identify inconsistencies"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="d366e14473"></a>B.1.&nbsp;Running hbck to identify inconsistencies</h2></div></div></div>
To check to see if your HBase cluster has corruptions, run hbck against your HBase cluster:
<pre class="programlisting">
$ ./bin/hbase hbck
</pre><p>
At the end of the commands output it prints OK or tells you the number of INCONSISTENCIES
present. You may also want to run run hbck a few times because some inconsistencies can be
transient (e.g. cluster is starting up or a region is splitting). Operationally you may want to run
hbck regularly and setup alert (e.g. via nagios) if it repeatedly reports inconsistencies .
A run of hbck will report a list of inconsistencies along with a brief description of the regions and
tables affected. The using the <code class="code">-details</code> option will report more details including a representative
listing of all the splits present in all the tables.
	</p><pre class="programlisting">
$ ./bin/hbase hbck -details
</pre>
If you just want to know if some tables are corrupted, you can limit hbck to identify inconsistencies
in only specific tables. For example the following command would only attempt to check table
TableFoo and TableBar. The benefit is that hbck will run in less time.
<pre class="programlisting">
$ ./bin/hbase hbck TableFoo TableBar
</pre></div><div class="section" title="B.2.&nbsp;Inconsistencies"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="d366e14489"></a>B.2.&nbsp;Inconsistencies</h2></div></div></div><p>
	If after several runs, inconsistencies continue to be reported, you may have encountered a
corruption. These should be rare, but in the event they occur newer versions of HBase include
the hbck tool enabled with automatic repair options.
	</p><p>
	There are two invariants that when violated create inconsistencies in HBase:
	</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">HBase&#8217;s region consistency invariant is satisfied if every region is assigned and
deployed on exactly one region server, and all places where this state kept is in
accordance.
	</li><li class="listitem">HBase&#8217;s table integrity invariant is satisfied if for each table, every possible row key
resolves to exactly one region.
	</li></ul></div><p>
Repairs generally work in three phases -- a read-only information gathering phase that identifies
inconsistencies, a table integrity repair phase that restores the table integrity invariant, and then
finally a region consistency repair phase that restores the region consistency invariant.
Starting from version 0.90.0, hbck could detect region consistency problems report on a subset
of possible table integrity problems. It also included the ability to automatically fix the most
common inconsistency, region assignment and deployment consistency problems. This repair
could be done by using the <code class="code">-fix</code> command line option. These problems close regions if they are
open on the wrong server or on multiple region servers and also assigns regions to region
servers if they are not open.
</p><p>
Starting from HBase versions 0.90.7, 0.92.2 and 0.94.0, several new command line options are
introduced to aid repairing a corrupted HBase. This hbck sometimes goes by the nickname
&#8220;uberhbck&#8221;. Each particular version of uber hbck is compatible with the HBase&#8217;s of the same
major version (0.90.7 uberhbck can repair a 0.90.4). However, versions &lt;=0.90.6 and versions
&lt;=0.92.1 may require restarting the master or failing over to a backup master.
</p></div><div class="section" title="B.3.&nbsp;Localized repairs"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="d366e14508"></a>B.3.&nbsp;Localized repairs</h2></div></div></div><p>
	When repairing a corrupted HBase, it is best to repair the lowest risk inconsistencies first.
These are generally region consistency repairs -- localized single region repairs, that only modify
in-memory data, ephemeral zookeeper data, or patch holes in the META table.
Region consistency requires that the HBase instance has the state of the region&#8217;s data in HDFS
(.regioninfo files), the region&#8217;s row in the .META. table., and region&#8217;s deployment/assignments on
region servers and the master in accordance. Options for repairing region consistency include:
	</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><code class="code">-fixAssignments</code> (equivalent to the 0.90 <code class="code">-fix</code> option) repairs unassigned, incorrectly
assigned or multiply assigned regions.
		</li><li class="listitem"><code class="code">-fixMeta</code> which removes meta rows when corresponding regions are not present in
HDFS and adds new meta rows if they regions are present in HDFS while not in META.
		</li></ul></div><p>
	To fix deployment and assignment problems you can run this command:
</p><pre class="programlisting">
$ ./bin/hbase hbck -fixAssignments
</pre>
To fix deployment and assignment problems as well as repairing incorrect meta rows you can
run this command:.
<pre class="programlisting">
$ ./bin/hbase hbck -fixAssignments -fixMeta
</pre>
There are a few classes of table integrity problems that are low risk repairs. The first two are
degenerate (startkey == endkey) regions and backwards regions (startkey &gt; endkey). These are
automatically handled by sidelining the data to a temporary directory (/hbck/xxxx).
The third low-risk class is hdfs region holes. This can be repaired by using the:
	<div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><code class="code">-fixHdfsHoles</code> option for fabricating new empty regions on the file system.
If holes are detected you can use -fixHdfsHoles and should include -fixMeta and -fixAssignments to make the new region consistent.
		</li></ul></div><pre class="programlisting">
$ ./bin/hbase hbck -fixAssignments -fixMeta -fixHdfsHoles
</pre>
Since this is a common operation, we&#8217;ve added a the <code class="code">-repairHoles</code> flag that is equivalent to the
previous command:
<pre class="programlisting">
$ ./bin/hbase hbck -repairHoles
</pre>
If inconsistencies still remain after these steps, you most likely have table integrity problems
related to orphaned or overlapping regions.
	</div><div class="section" title="B.4.&nbsp;Region Overlap Repairs"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="d366e14546"></a>B.4.&nbsp;Region Overlap Repairs</h2></div></div></div>
Table integrity problems can require repairs that deal with overlaps. This is a riskier operation
because it requires modifications to the file system, requires some decision making, and may
require some manual steps. For these repairs it is best to analyze the output of a <code class="code">hbck -details</code>
run so that you isolate repairs attempts only upon problems the checks identify. Because this is
riskier, there are safeguard that should be used to limit the scope of the repairs.
WARNING: This is a relatively new and have only been tested on online but idle HBase instances
(no reads/writes). Use at your own risk in an active production environment!
The options for repairing table integrity violations include:
	<div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><code class="code">-fixHdfsOrphans</code> option for &#8220;adopting&#8221; a region directory that is missing a region
metadata file (the .regioninfo file).
		</li><li class="listitem"><code class="code">-fixHdfsOverlaps</code> ability for fixing overlapping regions
		</li></ul></div>
When repairing overlapping regions, a region&#8217;s data can be modified on the file system in two
ways: 1) by merging regions into a larger region or 2) by sidelining regions by moving data to
&#8220;sideline&#8221; directory where data could be restored later. Merging a large number of regions is
technically correct but could result in an extremely large region that requires series of costly
compactions and splitting operations. In these cases, it is probably better to sideline the regions
that overlap with the most other regions (likely the largest ranges) so that merges can happen on
a more reasonable scale. Since these sidelined regions are already laid out in HBase&#8217;s native
directory and HFile format, they can be restored by using HBase&#8217;s bulk load mechanism.
The default safeguard thresholds are conservative. These options let you override the default
thresholds and to enable the large region sidelining feature.
	<div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><code class="code">-maxMerge &lt;n&gt;</code> maximum number of overlapping regions to merge
		</li><li class="listitem"><code class="code">-sidelineBigOverlaps</code> if more than maxMerge regions are overlapping, sideline attempt
to sideline the regions overlapping with the most other regions.
		</li><li class="listitem"><code class="code">-maxOverlapsToSideline &lt;n&gt;</code> if sidelining large overlapping regions, sideline at most n
regions.
		</li></ul></div>

Since often times you would just want to get the tables repaired, you can use this option to turn
on all repair options:
	<div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><code class="code">-repair</code> includes all the region consistency options and only the hole repairing table
integrity options.
		</li></ul></div>
Finally, there are safeguards to limit repairs to only specific tables. For example the following
command would only attempt to check and repair table TableFoo and TableBar.
<pre class="programlisting">
$ ./bin/hbase hbck -repair TableFoo TableBar
</pre><div class="section" title="B.4.1.&nbsp;Special cases: Meta is not properly assigned"><div class="titlepage"><div><div><h3 class="title"><a name="d366e14585"></a>B.4.1.&nbsp;Special cases: Meta is not properly assigned</h3></div></div></div>
There are a few special cases that hbck can handle as well.
Sometimes the meta table&#8217;s only region is inconsistently assigned or deployed. In this case
there is a special <code class="code">-fixMetaOnly</code> option that can try to fix meta assignments.
<pre class="programlisting">
$ ./bin/hbase hbck -fixMetaOnly -fixAssignments
</pre></div><div class="section" title="B.4.2.&nbsp;Special cases: HBase version file is missing"><div class="titlepage"><div><div><h3 class="title"><a name="d366e14594"></a>B.4.2.&nbsp;Special cases: HBase version file is missing</h3></div></div></div>
HBase&#8217;s data on the file system requires a version file in order to start. If this flie is missing, you
can use the <code class="code">-fixVersionFile</code> option to fabricating a new HBase version file. This assumes that
the version of hbck you are running is the appropriate version for the HBase cluster.
	</div><div class="section" title="B.4.3.&nbsp;Special case: Root and META are corrupt."><div class="titlepage"><div><div><h3 class="title"><a name="d366e14601"></a>B.4.3.&nbsp;Special case: Root and META are corrupt.</h3></div></div></div>
The most drastic corruption scenario is the case where the ROOT or META is corrupted and
HBase will not start. In this case you can use the OfflineMetaRepair tool create new ROOT
and META regions and tables.
This tool assumes that HBase is offline. It then marches through the existing HBase home
directory, loads as much information from region metadata files (.regioninfo files) as possible
from the file system. If the region metadata has proper table integrity, it sidelines the original root
and meta table directories, and builds new ones with pointers to the region directories and their
data.
<pre class="programlisting">
$ ./bin/hbase org.apache.hadoop.hbase.util.hbck.OfflineMetaRepair
</pre>
NOTE: This tool is not as clever as uberhbck but can be used to bootstrap repairs that uberhbck
can complete.
If the tool succeeds you should be able to start hbase and run online repairs if necessary.
	</div><div class="section" title="B.4.4.&nbsp;Special cases: Offline split parent"><div class="titlepage"><div><div><h3 class="title"><a name="d366e14608"></a>B.4.4.&nbsp;Special cases: Offline split parent</h3></div></div></div><p>
Once a region is split, the offline parent will be cleaned up automatically. Sometimes, daughter regions
are split again before their parents are cleaned up. HBase can clean up parents in the right order. However,
there could be some lingering offline split parents sometimes. They are in META, in HDFS, and not deployed.
But HBase can't clean them up. In this case, you can use the <code class="code">-fixSplitParents</code> option to reset
them in META to be online and not split. Therefore, hbck can merge them with other regions if fixing
overlapping regions option is used.
    </p><p>
This option should not normally be used, and it is not in <code class="code">-fixAll</code>.
    </p></div></div></div><div class="appendix" title="Appendix&nbsp;C.&nbsp;Compression In HBase"><div class="titlepage"><div><div><h2 class="title"><a name="compression"></a>Appendix&nbsp;C.&nbsp;Compression In HBase<a class="indexterm" name="d366e14624"></a></h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#compression.test">C.1. CompressionTest Tool</a></span></dt><dt><span class="section"><a href="#hbase.regionserver.codecs">C.2. 
    <code class="varname">
    hbase.regionserver.codecs
    </code>
    </a></span></dt><dt><span class="section"><a href="#lzo.compression">C.3. 
    LZO
    </a></span></dt><dt><span class="section"><a href="#gzip.compression">C.4. 
    GZIP
    </a></span></dt><dt><span class="section"><a href="#snappy.compression">C.5. 
    SNAPPY
    </a></span></dt><dd><dl><dt><span class="section"><a href="#snappy.compression.installation">C.5.1. 
    Installation
    </a></span></dt></dl></dd><dt><span class="section"><a href="#changing.compression">C.6. Changing Compression Schemes</a></span></dt></dl></div><p>There are a bunch of compression options in HBase.  There is some helpful discussion
        to be found in <a class="link" href="http://search-hadoop.com/m/lL12B1PFVhp1" target="_top">Documenting Guidance on compression and codecs</a>.
    </p><div class="section" title="C.1.&nbsp;CompressionTest Tool"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="compression.test"></a>C.1.&nbsp;CompressionTest Tool</h2></div></div></div><p>
    HBase includes a tool to test compression is set up properly.
    To run it, type <code class="code">/bin/hbase org.apache.hadoop.hbase.util.CompressionTest</code>.
    This will emit usage on how to run the tool.
    </p><div class="note" title="You need to restart regionserver for it to pick up fixed codecs!" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">You need to restart regionserver for it to pick up fixed codecs!</h3><p>Be aware that the regionserver caches the result of the compression check it runs
            ahead of each region open.  This means
        that you will have to restart the regionserver for it to notice that you have fixed
    any codec issues.</p></div></div><div class="section" title="C.2.&nbsp; hbase.regionserver.codecs"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="hbase.regionserver.codecs"></a>C.2.&nbsp;
    <code class="varname">
    hbase.regionserver.codecs
    </code>
    </h2></div></div></div><p>
    To have a RegionServer test a set of codecs and fail-to-start if any
    code is missing or misinstalled, add the configuration
    <code class="varname">
    hbase.regionserver.codecs
    </code>
    to your <code class="filename">hbase-site.xml</code> with a value of
    codecs to test on startup.  For example if the
    <code class="varname">
    hbase.regionserver.codecs
    </code> value is <code class="code">lzo,gz</code> and if lzo is not present
    or improperly installed, the misconfigured RegionServer will fail
    to start.
    </p><p>
    Administrators might make use of this facility to guard against
    the case where a new server is added to cluster but the cluster
    requires install of a particular coded.
    </p></div><div class="section" title="C.3.&nbsp; LZO"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="lzo.compression"></a>C.3.&nbsp;
    LZO
    </h2></div></div></div><p>Unfortunately, HBase cannot ship with LZO because of
      the licensing issues; HBase is Apache-licensed, LZO is GPL.
      Therefore LZO install is to be done post-HBase install.
      See the <a class="link" href="http://wiki.apache.org/hadoop/UsingLzoCompression" target="_top">Using LZO Compression</a>
      wiki page for how to make LZO work with HBase.
      </p><p>A common problem users run into when using LZO is that while initial
      setup of the cluster runs smooth, a month goes by and some sysadmin goes to
      add a machine to the cluster only they'll have forgotten to do the LZO
      fixup on the new machine.  In versions since HBase 0.90.0, we should
      fail in a way that makes it plain what the problem is, but maybe not. </p><p>See <a class="xref" href="#hbase.regionserver.codecs" title="C.2.&nbsp; hbase.regionserver.codecs">Section&nbsp;C.2, &#8220;
    <code class="varname">
    hbase.regionserver.codecs
    </code>
    &#8221;</a>
      for a feature to help protect against failed LZO install.</p></div><div class="section" title="C.4.&nbsp; GZIP"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="gzip.compression"></a>C.4.&nbsp;
    GZIP
    </h2></div></div></div><p>
    GZIP will generally compress better than LZO though slower.
    For some setups, better compression may be preferred.
    Java will use java's GZIP unless the native Hadoop libs are
    available on the CLASSPATH; in this case it will use native
    compressors instead (If the native libs are NOT present,
    you will see lots of <span class="emphasis"><em>Got brand-new compressor</em></span>
    reports in your logs; see <a class="xref" href="#">???</a>).
    </p></div><div class="section" title="C.5.&nbsp; SNAPPY"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="snappy.compression"></a>C.5.&nbsp;
    SNAPPY
    </h2></div></div></div><p>
        If snappy is installed, HBase can make use of it (courtesy of
        <a class="link" href="http://code.google.com/p/hadoop-snappy/" target="_top">hadoop-snappy</a>
        <sup>[<a name="d366e14699" href="#ftn.d366e14699" class="footnote">38</a>]</sup>).

        </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
                    Build and install <a class="link" href="http://code.google.com/p/snappy/" target="_top">snappy</a> on all nodes
                    of your cluster (see below).  HBase nor Hadoop cannot include snappy because of licensing issues (The
                    hadoop libhadoop.so under its native dir does not include snappy; of note, the shipped .so
                    may be for 32-bit architectures -- this fact has tripped up folks in the past with them thinking
                    it 64-bit).  The notes below are about installing snappy for HBase use.  You may want snappy
                    available in your hadoop context also.  That is not covered here.
                    HBase and Hadoop find the snappy .so in different locations currently: Hadoop picks those files in
                    <code class="filename">./lib</code> while HBase finds the .so in <code class="filename">./lib/[PLATFORM]</code>.
                </p></li><li class="listitem"><p>
        Use CompressionTest to verify snappy support is enabled and the libs can be loaded ON ALL NODES of your cluster:
        </p><pre class="programlisting">$ hbase org.apache.hadoop.hbase.util.CompressionTest hdfs://host/path/to/hbase snappy</pre><p>
                </p></li><li class="listitem"><p>
        Create a column family with snappy compression and verify it in the hbase shell:
        </p><pre class="programlisting">$ hbase&gt; create 't1', { NAME =&gt; 'cf1', COMPRESSION =&gt; 'SNAPPY' }
hbase&gt; describe 't1'</pre><p>
        In the output of the "describe" command, you need to ensure it lists "COMPRESSION =&gt; 'SNAPPY'"
                </p></li></ol></div><p>

    </p><div class="section" title="C.5.1.&nbsp; Installation"><div class="titlepage"><div><div><h3 class="title"><a name="snappy.compression.installation"></a>C.5.1.&nbsp;
    Installation
    </h3></div></div></div><p>Snappy is used by hbase to compress HFiles on flush and when compacting.
    </p><p>
        You will find the snappy library file under the .libs directory from your Snappy build (For example
        /home/hbase/snappy-1.0.5/.libs/). The file is called libsnappy.so.1.x.x where 1.x.x is the version of the snappy
        code you are building. You can either copy this file into your hbase lib directory -- under lib/native/PLATFORM --
        naming the file as libsnappy.so,
        or simply create a symbolic link to it (See ./bin/hbase for how it does library path for native libs).
    </p><p>
        The second file you need is the hadoop native library. You will find this file in your hadoop installation directory
        under lib/native/Linux-amd64-64/ or lib/native/Linux-i386-32/. The file you are looking for is libhadoop.so.1.x.x.
        Again, you can simply copy this file or link to it from under hbase in lib/native/PLATFORM (e.g. Linux-amd64-64, etc.),
        using the name libhadoop.so.
    </p><p>
        At the end of the installation, you should have both libsnappy.so and libhadoop.so links or files present into
        lib/native/Linux-amd64-64 or into lib/native/Linux-i386-32 (where the last part of the directory path is the
        PLATFORM you built and rare running the native lib on)
    </p><p>To point hbase at snappy support, in hbase-env.sh set
        </p><pre class="programlisting">export HBASE_LIBRARY_PATH=/pathtoyourhadoop/lib/native/Linux-amd64-64</pre><p>
        In <code class="filename">/pathtoyourhadoop/lib/native/Linux-amd64-64</code> you should have something like:
        </p><pre class="programlisting">
        libsnappy.a
        libsnappy.so
        libsnappy.so.1
        libsnappy.so.1.1.2
    </pre><p>
    </p></div></div><div class="section" title="C.6.&nbsp;Changing Compression Schemes"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="changing.compression"></a>C.6.&nbsp;Changing Compression Schemes</h2></div></div></div><p>A frequent question on the dist-list is how to change compression schemes for ColumnFamilies.  This is actually quite simple,
      and can be done via an alter command.  Because the compression scheme is encoded at the block-level in StoreFiles, the table does
      <span class="emphasis"><em>not</em></span> need to be re-created and the data does <span class="emphasis"><em>not</em></span> copied somewhere else.  Just make sure
      the old codec is still available until you are sure that all of the old StoreFiles have been compacted.
      </p></div><div class="footnotes"><br><hr width="100" align="left"><div class="footnote"><p><sup>[<a id="ftn.d366e14699" href="#d366e14699" class="para">38</a>] </sup>See <a class="link" href="http://search-hadoop.com/m/Ds8d51c263B1/%2522Hadoop-Snappy+in+synch+with+Hadoop+trunk%2522&amp;subj=Hadoop+Snappy+in+synch+with+Hadoop+trunk" target="_top">Alejandro's note</a> up on the list on difference between Snappy in Hadoop
        and Snappy in HBase</p></div></div></div><div class="appendix" title="Appendix&nbsp;D.&nbsp;YCSB: The Yahoo! Cloud Serving Benchmark and HBase"><div class="titlepage"><div><div><h2 class="title"><a name="d366e14765"></a>Appendix&nbsp;D.&nbsp;<a class="link" href="https://github.com/brianfrankcooper/YCSB/" target="_top">YCSB: The Yahoo! Cloud Serving Benchmark</a> and HBase</h2></div></div></div><p>TODO: Describe how YCSB is poor for putting up a decent cluster load.</p><p>TODO: Describe setup of YCSB for HBase.  In particular, presplit your tables before you start
          a run.  See <a class="link" href="https://issues.apache.org/jira/browse/HBASE-4163" target="_top">HBASE-4163 Create Split Strategy for YCSB Benchmark</a>
          for why and a little shell command for how to do it.</p><p>Ted Dunning redid YCSB so it's mavenized and added facility for verifying workloads.  See <a class="link" href="https://github.com/tdunning/YCSB" target="_top">Ted Dunning's YCSB</a>.</p></div><div class="appendix" title="Appendix&nbsp;E.&nbsp;HFile format version 2"><div class="titlepage"><div><div><h2 class="title"><a name="hfilev2"></a>Appendix&nbsp;E.&nbsp;HFile format version 2</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#d366e14785">E.1. Motivation </a></span></dt><dt><span class="section"><a href="#d366e14798">E.2. HFile format version 1 overview </a></span></dt><dd><dl><dt><span class="section"><a href="#d366e14820">E.2.1.  Block index format in version 1 </a></span></dt></dl></dd><dt><span class="section"><a href="#d366e14844">E.3. 
      HBase file format with inline blocks (version 2)
      </a></span></dt><dd><dl><dt><span class="section"><a href="#d366e14847">E.3.1.  Overview</a></span></dt><dt><span class="section"><a href="#d366e14862">E.3.2. Unified version 2 block format</a></span></dt><dt><span class="section"><a href="#d366e14931">E.3.3.  Block index in version 2</a></span></dt><dt><span class="section"><a href="#d366e14956">E.3.4. 
      Root block index format in version 2</a></span></dt><dt><span class="section"><a href="#d366e15009">E.3.5. 
      Non-root block index format in version 2</a></span></dt><dt><span class="section"><a href="#d366e15034">E.3.6. 
      Bloom filters in version 2</a></span></dt><dt><span class="section"><a href="#d366e15071">E.3.7. File Info format in versions 1 and 2</a></span></dt><dt><span class="section"><a href="#d366e15117">E.3.8. 
      Fixed file trailer format differences between versions 1 and 2</a></span></dt><dt><span class="section"><a href="#d366e15260">E.3.9. getShortMidpointKey(an optimization for data index block)</a></span></dt></dl></dd></dl></div><div class="section" title="E.1.&nbsp;Motivation"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="d366e14785"></a>E.1.&nbsp;Motivation </h2></div></div></div><p>Note:  this feature was introduced in HBase 0.92</p><p>We found it necessary to revise the HFile format after encountering high memory usage and slow startup times caused by large Bloom filters and block indexes in the region server. Bloom filters can get as large as 100 MB per HFile, which adds up to 2 GB when aggregated over 20 regions. Block indexes can grow as large as 6 GB in aggregate size over the same set of regions. A region is not considered opened until all of its block index data is loaded. Large Bloom filters produce a different performance problem: the first get request that requires a Bloom filter lookup will incur the latency of loading the entire Bloom filter bit array.</p><p>To speed up region server startup we break Bloom filters and block indexes into multiple blocks and write those blocks out as they fill up, which also reduces the HFile writer&#8217;s memory footprint. In the Bloom filter case, &#8220;filling up a block&#8221; means accumulating enough keys to efficiently utilize a fixed-size bit array, and in the block index case we accumulate an &#8220;index block&#8221; of the desired size. Bloom filter blocks and index blocks (we call these &#8220;inline blocks&#8221;) become interspersed with data blocks, and as a side effect we can no longer rely on the difference between block offsets to determine data block length, as it was done in version 1.</p><p>HFile is a low-level file format by design, and it should not deal with application-specific details such as Bloom filters, which are handled at StoreFile level. Therefore, we call Bloom filter blocks in an HFile "inline" blocks. We also supply HFile with an interface to write those inline blocks. </p><p>Another format modification aimed at reducing the region server startup time is to use a contiguous &#8220;load-on-open&#8221; section that has to be loaded in memory at the time an HFile is being opened. Currently, as an HFile opens, there are separate seek operations to read the trailer, data/meta indexes, and file info. To read the Bloom filter, there are two more seek operations for its &#8220;data&#8221; and &#8220;meta&#8221; portions. In version 2, we seek once to read the trailer and seek again to read everything else we need to open the file from a contiguous block.</p></div><div class="section" title="E.2.&nbsp;HFile format version 1 overview"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="d366e14798"></a>E.2.&nbsp;HFile format version 1 overview </h2></div></div></div><p>As we will be discussing the changes we are making to the HFile format, it is useful to give a short overview of the previous (HFile version 1) format. An HFile in the existing format is structured as follows:
           <span class="inlinemediaobject"><img src="images/hfile.png" align="middle" alt="HFile Version 1"></span>
           <sup>[<a name="d366e14813" href="#ftn.d366e14813" class="footnote">39</a>]</sup>
       </p><div class="section" title="E.2.1.&nbsp; Block index format in version 1"><div class="titlepage"><div><div><h3 class="title"><a name="d366e14820"></a>E.2.1.&nbsp; Block index format in version 1 </h3></div></div></div><p>The block index in version 1 is very straightforward. For each entry, it contains: </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Offset (long)</p></li><li class="listitem"><p>Uncompressed size (int)</p></li><li class="listitem"><p>Key (a serialized byte array written using Bytes.writeByteArray) </p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p>Key length as a variable-length integer (VInt)
                  </p></li><li class="listitem"><p>
                     Key bytes
                 </p></li></ol></div></li></ol></div><p>The number of entries in the block index is stored in the fixed file trailer, and has to be passed in to the method that reads the block index. One of the limitations of the block index in version 1 is that it does not provide the compressed size of a block, which turns out to be necessary for decompression. Therefore, the HFile reader has to infer this compressed size from the offset difference between blocks. We fix this limitation in version 2, where we store on-disk block size instead of uncompressed size, and get uncompressed size from the block header.</p></div></div><div class="section" title="E.3.&nbsp; HBase file format with inline blocks (version 2)"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="d366e14844"></a>E.3.&nbsp;
      HBase file format with inline blocks (version 2)
      </h2></div></div></div><div class="section" title="E.3.1.&nbsp; Overview"><div class="titlepage"><div><div><h3 class="title"><a name="d366e14847"></a>E.3.1.&nbsp; Overview</h3></div></div></div><p>The version of HBase introducing the above features reads both version 1 and 2 HFiles, but only writes version 2 HFiles. A version 2 HFile is structured as follows:
           <span class="inlinemediaobject"><img src="images/hfilev2.png" align="middle" alt="HFile Version 2"></span>

   </p></div><div class="section" title="E.3.2.&nbsp;Unified version 2 block format"><div class="titlepage"><div><div><h3 class="title"><a name="d366e14862"></a>E.3.2.&nbsp;Unified version 2 block format</h3></div></div></div><p>In the version 2 every block in the data section contains the following fields: </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>8 bytes: Block type, a sequence of bytes equivalent to version 1's "magic records". Supported block types are: </p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p>DATA &#8211; data blocks
                  </p></li><li class="listitem"><p>
                     LEAF_INDEX &#8211; leaf-level index blocks in a multi-level-block-index
                 </p></li><li class="listitem"><p>
                     BLOOM_CHUNK &#8211; Bloom filter chunks
                  </p></li><li class="listitem"><p>
                     META &#8211; meta blocks (not used for Bloom filters in version 2 anymore)
                  </p></li><li class="listitem"><p>
                     INTERMEDIATE_INDEX &#8211; intermediate-level index blocks in a multi-level blockindex
                  </p></li><li class="listitem"><p>
                     ROOT_INDEX &#8211; root&gt;level index blocks in a multi&gt;level block index
                  </p></li><li class="listitem"><p>
                     FILE_INFO &#8211; the &#8220;file info&#8221; block, a small key&gt;value map of metadata
                  </p></li><li class="listitem"><p>
                     BLOOM_META &#8211; a Bloom filter metadata block in the load&gt;on&gt;open section
                  </p></li><li class="listitem"><p>
                     TRAILER &#8211; a fixed&gt;size file trailer. As opposed to the above, this is not an
                     HFile v2 block but a fixed&gt;size (for each HFile version) data structure
                  </p></li><li class="listitem"><p>
                      INDEX_V1 &#8211; this block type is only used for legacy HFile v1 block
                  </p></li></ol></div></li><li class="listitem"><p>Compressed size of the block's data, not including the header (int).
         </p><p>
Can be used for skipping the current data block when scanning HFile data.
                  </p></li><li class="listitem"><p>Uncompressed size of the block's data, not including the header (int)</p><p>
 This is equal to the compressed size if the compression algorithm is NON
                  </p></li><li class="listitem"><p>File offset of the previous block of the same type (long)</p><p>
 Can be used for seeking to the previous data/index block
                  </p></li><li class="listitem"><p>Compressed data (or uncompressed data if the compression algorithm is NONE).</p></li></ol></div><p>The above format of blocks is used in the following HFile sections:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Scanned block section. The section is named so because it contains all data blocks that need to be read when an HFile is scanned sequentially. &nbsp;Also contains leaf block index and Bloom chunk blocks. </p></li><li class="listitem"><p>Non-scanned block section. This section still contains unified-format v2 blocks but it does not have to be read when doing a sequential scan. This section contains &#8220;meta&#8221; blocks and intermediate-level index blocks.
         </p></li></ol></div><p>We are supporting &#8220;meta&#8221; blocks in version 2 the same way they were supported in version 1, even though we do not store Bloom filter data in these blocks anymore. </p></div><div class="section" title="E.3.3.&nbsp; Block index in version 2"><div class="titlepage"><div><div><h3 class="title"><a name="d366e14931"></a>E.3.3.&nbsp; Block index in version 2</h3></div></div></div><p>There are three types of block indexes in HFile version 2, stored in two different formats (root and non-root): </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Data index &#8212; version 2 multi-level block index, consisting of:</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p>
 Version 2 root index, stored in the data block index section of the file
             </p></li><li class="listitem"><p>
Optionally, version 2 intermediate levels, stored in the non%root format in   the data index section of the file.    Intermediate levels can only be present if leaf level blocks are present
             </p></li><li class="listitem"><p>
Optionally, version 2 leaf levels, stored in the non%root format inline with   data blocks
             </p></li></ol></div></li><li class="listitem"><p>Meta index &#8212; version 2 root index format only, stored in the meta index section of the file</p></li><li class="listitem"><p>Bloom index &#8212; version 2 root index format only, stored in the &#8220;load-on-open&#8221; section as part of Bloom filter metadata.</p></li></ol></div></div><div class="section" title="E.3.4.&nbsp; Root block index format in version 2"><div class="titlepage"><div><div><h3 class="title"><a name="d366e14956"></a>E.3.4.&nbsp;
      Root block index format in version 2</h3></div></div></div><p>This format applies to:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Root level of the version 2 data index</p></li><li class="listitem"><p>Entire meta and Bloom indexes in version 2, which are always single-level. </p></li></ol></div><p>A version 2 root index block is a sequence of entries of the following format, similar to entries of a version 1 block index, but storing on-disk size instead of uncompressed size. </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Offset (long) </p><p>
This offset may point to a data block or to a deeper&gt;level index block.
             </p></li><li class="listitem"><p>On-disk size (int) </p></li><li class="listitem"><p>Key (a serialized byte array stored using Bytes.writeByteArray) </p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p>Key (VInt)
             </p></li><li class="listitem"><p>Key bytes
             </p></li></ol></div></li></ol></div><p>A single-level version 2 block index consists of just a single root index block. To read a root index block of version 2, one needs to know the number of entries. For the data index and the meta index the number of entries is stored in the trailer, and for the Bloom index it is stored in the compound Bloom filter metadata.</p><p>For a multi-level block index we also store the following fields in the root index block in the load-on-open section of the HFile, in addition to the data structure described above:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Middle leaf index block offset</p></li><li class="listitem"><p>Middle leaf block on-disk size (meaning the leaf index block containing the reference to the &#8220;middle&#8221; data block of the file) </p></li><li class="listitem"><p>The index of the mid-key (defined below) in the middle leaf-level block.</p></li></ol></div><p></p><p>These additional fields are used to efficiently retrieve the mid-key of the HFile used in HFile splits, which we define as the first key of the block with a zero-based index of (n &#8211; 1) / 2, if the total number of blocks in the HFile is n. This definition is consistent with how the mid-key was determined in HFile version 1, and is reasonable in general, because blocks are likely to be the same size on average, but we don&#8217;t have any estimates on individual key/value pair sizes. </p><p></p><p>When writing a version 2 HFile, the total number of data blocks pointed to by every leaf-level index block is kept track of. When we finish writing and the total number of leaf-level blocks is determined, it is clear which leaf-level block contains the mid-key, and the fields listed above are computed. &nbsp;When reading the HFile and the mid-key is requested, we retrieve the middle leaf index block (potentially from the block cache) and get the mid-key value from the appropriate position inside that leaf block.</p></div><div class="section" title="E.3.5.&nbsp; Non-root block index format in version 2"><div class="titlepage"><div><div><h3 class="title"><a name="d366e15009"></a>E.3.5.&nbsp;
      Non-root block index format in version 2</h3></div></div></div><p>This format applies to intermediate-level and leaf index blocks of a version 2 multi-level data block index. Every non-root index block is structured as follows. </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>numEntries: the number of entries (int). </p></li><li class="listitem"><p>entryOffsets: the &#8220;secondary index&#8221; of offsets of entries in the block, to facilitate a quick binary search on the key (numEntries + 1 int values). The last value is the total length of all entries in this index block. For example, in a non-root index block with entry sizes 60, 80, 50 the &#8220;secondary index&#8221; will contain the following int array: {0, 60, 140, 190}.</p></li><li class="listitem"><p>Entries. Each entry contains: </p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p>
Offset of the block referenced by this entry in the file (long)
             </p></li><li class="listitem"><p>
On&gt;disk size of the referenced block (int)
             </p></li><li class="listitem"><p>
Key. The length can be calculated from entryOffsets.
             </p></li></ol></div></li></ol></div></div><div class="section" title="E.3.6.&nbsp; Bloom filters in version 2"><div class="titlepage"><div><div><h3 class="title"><a name="d366e15034"></a>E.3.6.&nbsp;
      Bloom filters in version 2</h3></div></div></div><p>In contrast with version 1, in a version 2 HFile Bloom filter metadata is stored in the load-on-open section of the HFile for quick startup. </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>A compound Bloom filter. </p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p>
 Bloom filter version = 3 (int). There used to be a DynamicByteBloomFilter class that had the Bloom   filter version number 2
             </p></li><li class="listitem"><p>
The total byte size of all compound Bloom filter chunks (long)
             </p></li><li class="listitem"><p>
 Number of hash functions (int
             </p></li><li class="listitem"><p>
Type of hash functions (int)
             </p></li><li class="listitem"><p>
The total key count inserted into the Bloom filter (long)
             </p></li><li class="listitem"><p>
The maximum total number of keys in the Bloom filter (long)
             </p></li><li class="listitem"><p>
The number of chunks (int)
             </p></li><li class="listitem"><p>
Comparator class used for Bloom filter keys, a UTF&gt;8 encoded string stored   using Bytes.writeByteArray
             </p></li><li class="listitem"><p>
 Bloom block index in the version 2 root block index format
             </p></li></ol></div></li></ol></div></div><div class="section" title="E.3.7.&nbsp;File Info format in versions 1 and 2"><div class="titlepage"><div><div><h3 class="title"><a name="d366e15071"></a>E.3.7.&nbsp;File Info format in versions 1 and 2</h3></div></div></div><p>The file info block is a serialized <a class="ulink" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/io/HbaseMapWritable.html" target="_top">HbaseMapWritable</a> (essentially a map from byte arrays to byte arrays) with the following keys, among others. StoreFile-level logic adds more keys to this.</p><div class="informaltable"><table border="1"><colgroup><col><col></colgroup><tbody><tr><td>
               <p>hfile.LASTKEY </p>
            </td><td>
               <p>The last key of the file (byte array) </p>
            </td></tr><tr><td>
               <p>hfile.AVG_KEY_LEN </p>
            </td><td>
               <p>The average key length in the file (int) </p>
            </td></tr><tr><td>
               <p>hfile.AVG_VALUE_LEN </p>
            </td><td>
               <p>The average value length in the file (int) </p>
            </td></tr></tbody></table></div><p>File info format did not change in version 2. However, we moved the file info to the final section of the file, which can be loaded as one block at the time the HFile is being opened. Also, we do not store comparator in the version 2 file info anymore. Instead, we store it in the fixed file trailer. This is because we need to know the comparator at the time of parsing the load-on-open section of the HFile.</p></div><div class="section" title="E.3.8.&nbsp; Fixed file trailer format differences between versions 1 and 2"><div class="titlepage"><div><div><h3 class="title"><a name="d366e15117"></a>E.3.8.&nbsp;
      Fixed file trailer format differences between versions 1 and 2</h3></div></div></div><p>The following table shows common and different fields between fixed file trailers in versions 1 and 2. Note that the size of the trailer is different depending on the version, so it is &#8220;fixed&#8221; only within one version. However, the version is always stored as the last four-byte integer in the file. </p><p></p><div class="informaltable"><table border="1"><colgroup><col class="c1"><col class="c2"></colgroup><tbody><tr><td>
               <p>Version 1 </p>
            </td><td>
               <p>Version 2 </p>
            </td></tr><tr><td colspan="2" align="center">
               <p>File info offset (long) </p>
            </td></tr><tr><td>
               <p>Data index offset (long) </p>
            </td><td>
                <p>loadOnOpenOffset (long)</p>
                <p><span class="emphasis"><em>The offset of the section that we need toload when opening the file.</em></span></p>
            </td></tr><tr><td colspan="2" align="center">
               <p>Number of data index entries (int) </p>
            </td></tr><tr><td>
               <p>metaIndexOffset (long)</p>
               <p>This field is not being used by the version 1 reader, so we removed it from version 2.</p>
            </td><td>
               <p>uncompressedDataIndexSize (long)</p>
               <p>The total uncompressed size of the whole data block index, including root-level, intermediate-level, and leaf-level blocks.</p>
            </td></tr><tr><td colspan="2" align="center">
               <p>Number of meta index entries (int) </p>
            </td></tr><tr><td colspan="2" align="center">
               <p>Total uncompressed bytes (long) </p>
            </td></tr><tr><td>
               <p>numEntries (int) </p>
            </td><td>
               <p>numEntries (long) </p>
            </td></tr><tr><td colspan="2" align="center">
               <p>Compression codec: 0 = LZO, 1 = GZ, 2 = NONE (int) </p>
            </td></tr><tr><td>
               <p></p>
            </td><td>
               <p>The number of levels in the data block index (int) </p>
            </td></tr><tr><td>
               <p></p>
            </td><td>
               <p>firstDataBlockOffset (long)</p>
               <p>The offset of the first first data block. Used when scanning. </p>
            </td></tr><tr><td>
               <p></p>
            </td><td>
               <p>lastDataBlockEnd (long)</p>
               <p>The offset of the first byte after the last key/value data block. We don't need to go beyond this offset when scanning. </p>
            </td></tr><tr><td>
               <p>Version: 1 (int) </p>
            </td><td>
               <p>Version: 2 (int) </p>
            </td></tr></tbody></table></div><p></p></div><div class="section" title="E.3.9.&nbsp;getShortMidpointKey(an optimization for data index block)"><div class="titlepage"><div><div><h3 class="title"><a name="d366e15260"></a>E.3.9.&nbsp;getShortMidpointKey(an optimization for data index block)</h3></div></div></div><p>Note: this optimization was introduced in HBase 0.95+</p><p>HFiles contain many blocks that contain a range of sorted Cells. Each cell has a key. To save IO when reading Cells, the HFile also has an index that maps a Cell's start key to the offset of the beginning of a particular block. Prior to this optimization, HBase would use the key of the first cell in each data block as the index key.</p><p>In HBASE-7845, we generate a new key that is lexicographically larger than the last key of the previous block and lexicographically equal or smaller than the start key of the current block. While actual keys can potentially be very long, this "fake key" or "virtual key" can be much shorter. For example, if the stop key of previous block is "the quick brown fox", the start key of current block is "the who", we could use "the r" as our virtual key in our hfile index.</p><p>There are two benefits to this:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><div class="section" title="1.&nbsp;"><div class="titlepage"></div>having shorter keys reduces the hfile index size, (allowing us to keep more indexes in memory), and</div></li><li class="listitem"><div class="section" title="1.&nbsp;"><div class="titlepage"></div>using something closer to the end key of the previous block allows us to avoid a potential extra IO when the target key lives in between the "virtual key" and the key of the first element in the target block.</div></li></ul></div><p>This optimization (implemented by the getShortMidpointKey method) is inspired by LevelDB's ByteWiseComparatorImpl::FindShortestSeparator() and FindShortSuccessor().</p></div></div><div class="footnotes"><br><hr width="100" align="left"><div class="footnote"><p><sup>[<a id="ftn.d366e14813" href="#d366e14813" class="para">39</a>] </sup>Image courtesy of Lars George, <a class="link" href="http://www.larsgeorge.com/2009/10/hbase-architecture-101-storage.html" target="_top">hbase-architecture-101-storage.html</a>.</p></div></div></div><div class="appendix" title="Appendix&nbsp;F.&nbsp;Other Information About HBase"><div class="titlepage"><div><div><h2 class="title"><a name="other.info"></a>Appendix&nbsp;F.&nbsp;Other Information About HBase</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#other.info.videos">F.1. HBase Videos</a></span></dt><dt><span class="section"><a href="#other.info.pres">F.2. HBase Presentations (Slides)</a></span></dt><dt><span class="section"><a href="#other.info.papers">F.3. HBase Papers</a></span></dt><dt><span class="section"><a href="#other.info.sites">F.4. HBase Sites</a></span></dt><dt><span class="section"><a href="#other.info.books">F.5. HBase Books</a></span></dt><dt><span class="section"><a href="#other.info.books.hadoop">F.6. Hadoop Books</a></span></dt></dl></div><div class="section" title="F.1.&nbsp;HBase Videos"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="other.info.videos"></a>F.1.&nbsp;HBase Videos</h2></div></div></div><p>Introduction to HBase
            </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><a class="link" href="http://www.cloudera.com/content/cloudera/en/resources/library/presentation/chicago_data_summit_apache_hbase_an_introduction_todd_lipcon.html" target="_top">Introduction to HBase</a> by Todd Lipcon (Chicago Data Summit 2011).
			  </li><li class="listitem"><a class="link" href="http://www.cloudera.com/videos/intorduction-hbase-todd-lipcon" target="_top">Introduction to HBase</a> by Todd Lipcon (2010).
			  </li></ul></div><p>
         </p><p><a class="link" href="http://www.cloudera.com/videos/hadoop-world-2011-presentation-video-building-realtime-big-data-services-at-facebook-with-hadoop-and-hbase" target="_top">Building Real Time Services at Facebook with HBase</a> by Jonathan Gray (Hadoop World 2011).
         </p><p><a class="link" href="http://www.cloudera.com/videos/hw10_video_how_stumbleupon_built_and_advertising_platform_using_hbase_and_hadoop" target="_top">HBase and Hadoop, Mixing Real-Time and Batch Processing at StumbleUpon</a> by JD Cryans (Hadoop World 2010).
         </p></div><div class="section" title="F.2.&nbsp;HBase Presentations (Slides)"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="other.info.pres"></a>F.2.&nbsp;HBase Presentations (Slides)</h2></div></div></div><p><a class="link" href="http://www.cloudera.com/content/cloudera/en/resources/library/hadoopworld/hadoop-world-2011-presentation-video-advanced-hbase-schema-design.html" target="_top">Advanced HBase Schema Design</a> by Lars George (Hadoop World 2011).
         </p><p><a class="link" href="http://www.slideshare.net/cloudera/chicago-data-summit-apache-hbase-an-introduction" target="_top">Introduction to HBase</a> by Todd Lipcon (Chicago Data Summit 2011).
         </p><p><a class="link" href="http://www.slideshare.net/cloudera/hw09-practical-h-base-getting-the-most-from-your-h-base-install" target="_top">Getting The Most From Your HBase Install</a> by Ryan Rawson, Jonathan Gray (Hadoop World 2009).
         </p></div><div class="section" title="F.3.&nbsp;HBase Papers"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="other.info.papers"></a>F.3.&nbsp;HBase Papers</h2></div></div></div><p><a class="link" href="http://research.google.com/archive/bigtable.html" target="_top">BigTable</a> by Google (2006).
         </p><p><a class="link" href="http://www.larsgeorge.com/2010/05/hbase-file-locality-in-hdfs.html" target="_top">HBase and HDFS Locality</a> by Lars George (2010).
         </p><p><a class="link" href="http://ianvarley.com/UT/MR/Varley_MastersReport_Full_2009-08-07.pdf" target="_top">No Relation: The Mixed Blessings of Non-Relational Databases</a> by Ian Varley (2009).
         </p></div><div class="section" title="F.4.&nbsp;HBase Sites"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="other.info.sites"></a>F.4.&nbsp;HBase Sites</h2></div></div></div><p><a class="link" href="http://www.cloudera.com/blog/category/hbase/" target="_top">Cloudera's HBase Blog</a> has a lot of links to useful HBase information.
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><a class="link" href="http://www.cloudera.com/blog/2010/04/cap-confusion-problems-with-partition-tolerance/" target="_top">CAP Confusion</a> is a relevant entry for background information on
			distributed storage systems.
			</li></ul></div><p>
         </p><p><a class="link" href="http://wiki.apache.org/hadoop/HBase/HBasePresentations" target="_top">HBase Wiki</a> has a page with a number of presentations.
         </p><p><a class="link" href="http://refcardz.dzone.com/refcardz/hbase" target="_top">HBase RefCard</a> from DZone.
         </p></div><div class="section" title="F.5.&nbsp;HBase Books"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="other.info.books"></a>F.5.&nbsp;HBase Books</h2></div></div></div><p><a class="link" href="http://shop.oreilly.com/product/0636920014348.do" target="_top">HBase:  The Definitive Guide</a> by Lars George.
         </p></div><div class="section" title="F.6.&nbsp;Hadoop Books"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="other.info.books.hadoop"></a>F.6.&nbsp;Hadoop Books</h2></div></div></div><p><a class="link" href="http://shop.oreilly.com/product/9780596521981.do" target="_top">Hadoop:  The Definitive Guide</a> by Tom White.
         </p></div></div><div class="appendix" title="Appendix&nbsp;G.&nbsp;HBase History"><div class="titlepage"><div><div><h2 class="title"><a name="hbase.history"></a>Appendix&nbsp;G.&nbsp;HBase History</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">2006:  <a class="link" href="http://research.google.com/archive/bigtable.html" target="_top">BigTable</a> paper published by Google.
	  </li><li class="listitem">2006 (end of year):  HBase development starts.
	  </li><li class="listitem">2008:  HBase becomes Hadoop sub-project.
	  </li><li class="listitem">2010:  HBase becomes Apache top-level project.
	  </li></ul></div></div><div class="appendix" title="Appendix&nbsp;H.&nbsp;HBase and the Apache Software Foundation"><div class="titlepage"><div><div><h2 class="title"><a name="asf"></a>Appendix&nbsp;H.&nbsp;HBase and the Apache Software Foundation</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#asf.devprocess">H.1. ASF Development Process</a></span></dt><dt><span class="section"><a href="#asf.reporting">H.2. ASF Board Reporting</a></span></dt></dl></div><p>HBase is a project in the Apache Software Foundation and as such there are responsibilities to the ASF to ensure
    a healthy project.</p><div class="section" title="H.1.&nbsp;ASF Development Process"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="asf.devprocess"></a>H.1.&nbsp;ASF Development Process</h2></div></div></div><p>See the <a class="link" href="http://www.apache.org/dev/#committers" target="_top">Apache Development Process page</a>
        for all sorts of information on how the ASF is structured (e.g., PMC, committers, contributors), to tips on contributing
        and getting involved, and how open-source works at ASF.
        </p></div><div class="section" title="H.2.&nbsp;ASF Board Reporting"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="asf.reporting"></a>H.2.&nbsp;ASF Board Reporting</h2></div></div></div><p>Once a quarter, each project in the ASF portfolio submits a report to the ASF board.  This is done by the HBase project
         lead and the committers.  See <a class="link" href="http://www.apache.org/foundation/board/reporting" target="_top">ASF board reporting</a> for more information.
         </p></div></div><div class="appendix" title="Appendix&nbsp;I.&nbsp;Enabling Dapper-like Tracing in HBase"><div class="titlepage"><div><div><h2 class="title"><a name="tracing"></a>Appendix&nbsp;I.&nbsp;Enabling Dapper-like Tracing in HBase</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#tracing.spanreceivers">I.1. SpanReceivers</a></span></dt><dt><span class="section"><a href="#tracing.client.modifications">I.2. Client Modifications</a></span></dt><dt><span class="section"><a href="#tracing.client.shell">I.3. Tracing from HBase Shell</a></span></dt></dl></div><p>
    <a class="link" href="https://issues.apache.org/jira/browse/HBASE-6449" target="_top">HBASE-6449</a>
    added support for tracing requests through HBase, using the open source tracing library,
    <a class="link" href="http://github.com/cloudera/htrace" target="_top">HTrace</a>.
    Setting up tracing is quite simple,
    however it currently requires some very minor changes to your client code
    (it would not be very difficult to remove this requirement).
  </p><div class="section" title="I.1.&nbsp;SpanReceivers"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="tracing.spanreceivers"></a>I.1.&nbsp;SpanReceivers</h2></div></div></div><p>
      The tracing system works by collecting information in structs called 'Spans'.
      It is up to you to choose how you want to receive this information
      by implementing the <code class="classname">SpanReceiver</code> interface,
      which defines one method:
</p><pre class="programlisting">
  public void receiveSpan(Span span);
</pre><p>
      This method serves as a callback whenever a span is completed.
      HTrace allows you to use as many SpanReceivers as you want
      so you can easily send trace information to multiple destinations.
    </p><p>
      Configure what SpanReceivers you'd like to us
      by putting a comma separated list of the
      fully-qualified class name of classes implementing
      <code class="classname">SpanReceiver</code> in <code class="filename">hbase-site.xml</code>
      property: <code class="varname">hbase.trace.spanreceiver.classes</code>.
    </p><p>
      HTrace includes a <code class="classname">LocalFileSpanReceiver</code>
      that writes all span information to local files in a JSON-based format.
      The <code class="classname">LocalFileSpanReceiver</code>
      looks in <code class="filename">hbase-site.xml</code>
      for a <code class="varname">hbase.local-file-span-receiver.path</code>
      property with a value describing the name of the file
      to which nodes should write their span information.
</p><pre class="programlisting">
  &lt;property&gt;
    &lt;name&gt;hbase.trace.spanreceiver.classes&lt;/name&gt;
    &lt;value&gt;org.htrace.impl.LocalFileSpanReceiver&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.local-file-span-receiver.path&lt;/name&gt;
    &lt;value&gt;/var/log/hbase/htrace.out&lt;/value&gt;
  &lt;/property&gt;
</pre><p>
    </p><p>
      HTrace also includes a <code class="classname">ZipkinSpanReceiver</code>
      that converts all span information to
      <a class="link" href="http://github.com/twitter/zipkin" target="_top">Zipkin</a>
      span format and send them to Zipkin server.
      You need to install htrace-zipkin jar and add it to your HBase classpath
      in order to use this receiver.
      The <code class="classname">ZipkinSpanReceiver</code>
      looks in <code class="filename">hbase-site.xml</code>
      for a <code class="varname">hbase.zipkin.collector-hostname</code>
      and <code class="varname">hbase.zipkin.collector-port</code>
      property with a value describing the Zipkin server
      to which span information are sent.
</p><pre class="programlisting">
  &lt;property&gt;
    &lt;name&gt;hbase.trace.spanreceiver.classes&lt;/name&gt;
    &lt;value&gt;org.htrace.impl.ZipkinSpanReceiver&lt;/value&gt;
  &lt;/property&gt; 
  &lt;property&gt;
    &lt;name&gt;hbase.zipkin.collector-hostname&lt;/name&gt;
    &lt;value&gt;localhost&lt;/value&gt;
  &lt;/property&gt; 
  &lt;property&gt;
    &lt;name&gt;hbase.zipkin.collector-port&lt;/name&gt;
    &lt;value&gt;9410&lt;/value&gt;
  &lt;/property&gt; 
</pre><p>
    </p><p>
      If you do not want to use the included span receivers,
      you are encouraged to write your own receiver
      (take a look at <code class="classname">LocalFileSpanReceiver</code> for an example).
      If you think others would benefit from your receiver,
      file a JIRA or send a pull request to
      <a class="link" href="http://github.com/cloudera/htrace" target="_top">HTrace</a>.
    </p></div><div class="section" title="I.2.&nbsp;Client Modifications"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="tracing.client.modifications"></a>I.2.&nbsp;Client Modifications</h2></div></div></div><p>
      In order to turn on tracing in your client code,
      you must initialize the module sending spans to receiver
      once per client process.
      (Because <code class="classname">SpanReceiverHost</code> is included in hbase-server jar,
      you need it on the client classpath in order to run this example.)
</p><pre class="programlisting">
  private SpanReceiverHost spanReceiverHost;
  
  ...
  
    Configuration conf = HBaseConfiguration.create();
    SpanReceiverHost spanReceiverHost = SpanReceiverHost.getInstance(conf);
</pre><p>
      Then you simply start tracing span before requests you think are interesting,
      and close it when the request is done.
      For example, if you wanted to trace all of your get operations,
      you change this:
</p><pre class="programlisting">
  HTable table = new HTable(conf, "t1");
  Get get = new Get(Bytes.toBytes("r1"));
  Result res = table.get(get);
</pre><p>
      into:
</p><pre class="programlisting">
  TraceScope ts = Trace.startSpan("Gets", Sampler.ALWAYS);
  try {
    HTable table = new HTable(conf, "t1");
    Get get = new Get(Bytes.toBytes("r1"));
    Result res = table.get(get);
  } finally {
    ts.close();
  }
</pre><p>
      If you wanted to trace half of your 'get' operations, you would pass in:
</p><pre class="programlisting">
  new ProbabilitySampler(0.5)
</pre><p>
      in lieu of <code class="varname">Sampler.ALWAYS</code>
      to <code class="classname">Trace.startSpan()</code>.
      See the HTrace <code class="filename">README</code> for more information on Samplers.
    </p></div><div class="section" title="I.3.&nbsp;Tracing from HBase Shell"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="tracing.client.shell"></a>I.3.&nbsp;Tracing from HBase Shell</h2></div></div></div><p>
      You can use <span class="command"><strong>trace</strong></span> command
      for tracing requests from HBase Shell.
      <span class="command"><strong>trace 'start'</strong></span> command turns on tracing and 
      <span class="command"><strong>trace 'stop'</strong></span> command turns off tracing.
</p><pre class="programlisting">
  hbase(main):001:0&gt; trace 'start'
  hbase(main):002:0&gt; put 'test', 'row1', 'f:', 'val1'   # traced commands
  hbase(main):003:0&gt; trace 'stop'
</pre><p>
    </p><p>
      <span class="command"><strong>trace 'start'</strong></span> and 
      <span class="command"><strong>trace 'stop'</strong></span> always
      returns boolean value representing 
      if or not there is ongoing tracing.
      As a result, <span class="command"><strong>trace 'stop'</strong></span>
      returns false on suceess.
      <span class="command"><strong>trace 'status'</strong></span>
      just returns if or not tracing is turned on.
</p><pre class="programlisting">
  hbase(main):001:0&gt; trace 'start'
  =&gt; true
  
  hbase(main):002:0&gt; trace 'status'
  =&gt; true
  
  hbase(main):003:0&gt; trace 'stop'
  =&gt; false
  
  hbase(main):004:0&gt; trace 'status'
  =&gt; false
</pre><p>
    </p></div></div><div class="appendix" title="Appendix&nbsp;J.&nbsp;0.95 RPC Specification"><div class="titlepage"><div><div><h2 class="title"><a name="hbase.rpc"></a>Appendix&nbsp;J.&nbsp;0.95 RPC Specification</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#d366e15573">J.1. Goals</a></span></dt><dt><span class="section"><a href="#d366e15586">J.2. TODO</a></span></dt><dt><span class="section"><a href="#d366e15602">J.3. RPC</a></span></dt><dd><dl><dt><span class="section"><a href="#d366e15616">J.3.1. Connection Setup</a></span></dt><dt><span class="section"><a href="#d366e15648">J.3.2. Request</a></span></dt><dt><span class="section"><a href="#d366e15681">J.3.3. Response</a></span></dt><dt><span class="section"><a href="#d366e15711">J.3.4. Exceptions</a></span></dt><dt><span class="section"><a href="#d366e15718">J.3.5. CellBlocks</a></span></dt></dl></dd><dt><span class="section"><a href="#d366e15723">J.4. Notes</a></span></dt><dd><dl><dt><span class="section"><a href="#d366e15726">J.4.1. Constraints</a></span></dt><dt><span class="section"><a href="#d366e15731">J.4.2. One fat pb request or header+param</a></span></dt><dt><span class="section"><a href="#rpc.configs">J.4.3. RPC Configurations</a></span></dt></dl></dd></dl></div><p>In 0.95, all client/server communication is done with
      <a class="link" href="https://code.google.com/p/protobuf/" target="_top">protobuf&#8217;ed</a> Messages rather than with
      <a class="link" href="http://hadoop.apache.org/docs/current/api/org/apache/hadoop/io/Writable.html" target="_top">Hadoop Writables</a>.
      Our RPC wire format therefore changes.
      This document describes the client/server request/response protocol and our new RPC wire-format.</p><p></p><p>For what RPC is like in 0.94 and previous,
      see Beno&icirc;t/Tsuna&#8217;s <a class="link" href="https://github.com/OpenTSDB/asynchbase/blob/master/src/HBaseRpc.java#L164" target="_top">Unofficial Hadoop / HBase RPC protocol documentation</a>.
      For more background on how we arrived at this spec., see
      <a class="link" href="https://docs.google.com/document/d/1WCKwgaLDqBw2vpux0jPsAu2WPTRISob7HGCO8YhfDTA/edit#" target="_top">HBase RPC: WIP</a></p><p></p><div class="section" title="J.1.&nbsp;Goals"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="d366e15573"></a>J.1.&nbsp;Goals</h2></div></div></div><p>
      </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>A wire-format we can evolve</p></li><li class="listitem"><p>A format that does not require our rewriting server core or
                  radically changing its current architecture (for later).</p></li></ol></div><p>
  </p></div><div class="section" title="J.2.&nbsp;TODO"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="d366e15586"></a>J.2.&nbsp;TODO</h2></div></div></div><p>
      </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>List of problems with currently specified format and where
                  we would like to go in a version2, etc. For example, what would we
                  have to change if anything to move server async or to support
                  streaming/chunking?</p></li><li class="listitem"><p>Diagram on how it works</p></li><li class="listitem"><p>A grammar that succinctly describes the wire-format. Currently
                  we have these words and the content of the rpc protobuf idl but
                  a grammar for the back and forth would help with groking rpc.  Also,
                  a little state machine on client/server interactions would help
              with understanding (and ensuring correct implementation).</p></li></ol></div><p>
  </p></div><div class="section" title="J.3.&nbsp;RPC"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="d366e15602"></a>J.3.&nbsp;RPC</h2></div></div></div><p>The client will send setup information on connection establish.
      Thereafter, the client invokes methods against the remote server sending a protobuf Message and receiving a protobuf Message in response.
      Communication is synchronous.  All back and forth is preceded by an int that has the total length of the request/response.
      Optionally, Cells(KeyValues) can be passed outside of protobufs in follow-behind Cell blocks (because
      <a class="link" href="https://docs.google.com/document/d/1WEtrq-JTIUhlnlnvA0oYRLp0F8MKpEBeBSCFcQiacdw/edit#" target="_top">we can&#8217;t protobuf megabytes of KeyValues</a> or Cells).
      These CellBlocks are encoded and optionally compressed.</p><p></p><p>For more detail on the protobufs involved, see the
      <a class="link" href="http://svn.apache.org/viewvc/hbase/trunk/hbase-protocol/src/main/protobuf/RPC.proto?view=markup" target="_top">RPC.proto</a> file in trunk.</p><div class="section" title="J.3.1.&nbsp;Connection Setup"><div class="titlepage"><div><div><h3 class="title"><a name="d366e15616"></a>J.3.1.&nbsp;Connection Setup</h3></div></div></div><p>Client initiates connection.</p><div class="section" title="J.3.1.1.&nbsp;Client"><div class="titlepage"><div><div><h4 class="title"><a name="d366e15621"></a>J.3.1.1.&nbsp;Client</h4></div></div></div><p>On connection setup, client sends a preamble followed by a connection header.
      </p><div class="section" title="J.3.1.1.1.&nbsp;<preamble&gt;"><div class="titlepage"><div><div><h5 class="title"><a name="d366e15626"></a>J.3.1.1.1.&nbsp;&lt;preamble&gt;</h5></div></div></div><pre class="programlisting">&lt;MAGIC 4 byte integer&gt; &lt;1 byte RPC Format Version&gt; &lt;1 byte auth type&gt;<sup>[<a name="d366e15632" href="#ftn.d366e15632" class="footnote">40</a>]</sup></pre><p>E.g.: HBas0x000x50 -- 4 bytes of MAGIC -- &#8216;HBas&#8217; -- plus one-byte of version, 0 in this case, and one byte, 0x50 (SIMPLE). of an auth type.</p></div><div class="section" title="J.3.1.1.2.&nbsp;<Protobuf ConnectionHeader Message&gt;"><div class="titlepage"><div><div><h5 class="title"><a name="d366e15637"></a>J.3.1.1.2.&nbsp;&lt;Protobuf ConnectionHeader Message&gt;</h5></div></div></div><p>Has user info, and &#8220;protocol&#8221;, as well as the encoders and compression the client will use sending CellBlocks.
          CellBlock encoders and compressors are for the life of the connection.
          CellBlock encoders implement org.apache.hadoop.hbase.codec.Codec.
          CellBlocks may then also be compressed.
          Compressors implement org.apache.hadoop.io.compress.CompressionCodec.
          This protobuf is written using writeDelimited so is prefaced by a pb varint
          with its serialized length</p></div></div><div class="section" title="J.3.1.2.&nbsp;Server"><div class="titlepage"><div><div><h4 class="title"><a name="d366e15643"></a>J.3.1.2.&nbsp;Server</h4></div></div></div><p>After client sends preamble and connection header,
          server does NOT respond if successful connection setup.
          No response means server is READY to accept requests and to give out response.
      If the version or authentication in the preamble is not agreeable or the server has trouble parsing the preamble,
      it will throw a org.apache.hadoop.hbase.ipc.FatalConnectionException explaining the error and will then disconnect.
      If the client in the connection header -- i.e. the protobuf&#8217;d Message that comes after the connection preamble -- asks for for a
      Service the server does not support or a codec the server does not have, again we throw a FatalConnectionException with explanation.</p></div></div><div class="section" title="J.3.2.&nbsp;Request"><div class="titlepage"><div><div><h3 class="title"><a name="d366e15648"></a>J.3.2.&nbsp;Request</h3></div></div></div><p>After a Connection has been set up, client makes requests.  Server responds.</p><p>A request is made up of a protobuf RequestHeader followed by a protobuf Message parameter.
          The header includes the method name and optionally, metadata on the optional CellBlock that may be following.
          The parameter type suits the method being invoked: i.e. if we are doing a getRegionInfo request,
          the protobuf Message param will be an instance of GetRegionInfoRequest.
          The response will be a GetRegionInfoResponse.
          The CellBlock is optionally used ferrying the bulk of the RPC data: i.e Cells/KeyValues.</p><p></p><div class="section" title="J.3.2.1.&nbsp;Request Parts"><div class="titlepage"><div><div><h4 class="title"><a name="d366e15656"></a>J.3.2.1.&nbsp;Request Parts</h4></div></div></div><div class="section" title="J.3.2.1.1.&nbsp;<Total Length&gt;"><div class="titlepage"><div><div><h5 class="title"><a name="d366e15659"></a>J.3.2.1.1.&nbsp;&lt;Total Length&gt;</h5></div></div></div><p>The request is prefaced by an int that holds the total length of what follows.</p></div><div class="section" title="J.3.2.1.2.&nbsp;<Protobuf RequestHeader Message&gt;"><div class="titlepage"><div><div><h5 class="title"><a name="d366e15664"></a>J.3.2.1.2.&nbsp;&lt;Protobuf RequestHeader Message&gt;</h5></div></div></div><p>Will have call.id, trace.id, and method name, etc. including optional Metadata on the Cell block IFF one is following.
              Data is protobuf&#8217;d inline in this pb Message or optionally comes in the following CellBlock</p></div><div class="section" title="J.3.2.1.3.&nbsp;<Protobuf Param Message&gt;"><div class="titlepage"><div><div><h5 class="title"><a name="d366e15669"></a>J.3.2.1.3.&nbsp;&lt;Protobuf Param Message&gt;</h5></div></div></div><p>If the method being invoked is getRegionInfo, if you study the Service descriptor for the client to regionserver protocol,
              you will find that the request sends a GetRegionInfoRequest protobuf Message param in this position.</p></div><div class="section" title="J.3.2.1.4.&nbsp;<CellBlock&gt;"><div class="titlepage"><div><div><h5 class="title"><a name="d366e15674"></a>J.3.2.1.4.&nbsp;&lt;CellBlock&gt;</h5></div></div></div><p>An encoded and optionally compressed Cell block.</p></div></div></div><div class="section" title="J.3.3.&nbsp;Response"><div class="titlepage"><div><div><h3 class="title"><a name="d366e15681"></a>J.3.3.&nbsp;Response</h3></div></div></div><p>Same as Request, it is a protobuf ResponseHeader followed by a protobuf Message response where the Message response type suits the method invoked.
          Bulk of the data may come in a following CellBlock.</p><div class="section" title="J.3.3.1.&nbsp;Response Parts"><div class="titlepage"><div><div><h4 class="title"><a name="d366e15686"></a>J.3.3.1.&nbsp;Response Parts</h4></div></div></div><div class="section" title="J.3.3.1.1.&nbsp;<Total Length&gt;"><div class="titlepage"><div><div><h5 class="title"><a name="d366e15689"></a>J.3.3.1.1.&nbsp;&lt;Total Length&gt;</h5></div></div></div><p>The response is prefaced by an int that holds the total length of what follows.</p></div><div class="section" title="J.3.3.1.2.&nbsp;<Protobuf ResponseHeader Message&gt;"><div class="titlepage"><div><div><h5 class="title"><a name="d366e15694"></a>J.3.3.1.2.&nbsp;&lt;Protobuf ResponseHeader Message&gt;</h5></div></div></div><p>Will have call.id, etc. Will include exception if failed processing. &nbsp;Optionally includes metadata on optional, IFF there is a CellBlock following.</p></div><div class="section" title="J.3.3.1.3.&nbsp;<Protobuf Response Message&gt;"><div class="titlepage"><div><div><h5 class="title"><a name="d366e15699"></a>J.3.3.1.3.&nbsp;&lt;Protobuf Response Message&gt;</h5></div></div></div><p>Return or may be nothing if exception. If the method being invoked is getRegionInfo, if you study the Service descriptor for the client to regionserver protocol,
          you will find that the response sends a GetRegionInfoResponse protobuf Message param in this position.</p></div><div class="section" title="J.3.3.1.4.&nbsp;<CellBlock&gt;"><div class="titlepage"><div><div><h5 class="title"><a name="d366e15704"></a>J.3.3.1.4.&nbsp;&lt;CellBlock&gt;</h5></div></div></div><p>An encoded and optionally compressed Cell block.</p></div></div></div><div class="section" title="J.3.4.&nbsp;Exceptions"><div class="titlepage"><div><div><h3 class="title"><a name="d366e15711"></a>J.3.4.&nbsp;Exceptions</h3></div></div></div><p>There are two distinct types.
          There is the request failed which is encapsulated inside the response header for the response.
          The connection stays open to receive new requests.
          The second type, the FatalConnectionException, kills the connection.</p><p>Exceptions can carry extra information.
          See the ExceptionResponse protobuf type.
          It has a flag to indicate do-no-retry as well as other miscellaneous payload to help improve client responsiveness.</p></div><div class="section" title="J.3.5.&nbsp;CellBlocks"><div class="titlepage"><div><div><h3 class="title"><a name="d366e15718"></a>J.3.5.&nbsp;CellBlocks</h3></div></div></div><p>These are not versioned.
          Server can do the codec or it cannot.
          If new version of a codec with say, tighter encoding, then give it a new class name.
          Codecs will live on the server for all time so old clients can connect.</p></div></div><div class="section" title="J.4.&nbsp;Notes"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="d366e15723"></a>J.4.&nbsp;Notes</h2></div></div></div><div class="section" title="J.4.1.&nbsp;Constraints"><div class="titlepage"><div><div><h3 class="title"><a name="d366e15726"></a>J.4.1.&nbsp;Constraints</h3></div></div></div><p>In some part, current wire-format -- i.e. all requests and responses preceeded by a length -- has been dictated by current server non-async architecture.</p></div><div class="section" title="J.4.2.&nbsp;One fat pb request or header+param"><div class="titlepage"><div><div><h3 class="title"><a name="d366e15731"></a>J.4.2.&nbsp;One fat pb request or header+param</h3></div></div></div><p>We went with pb header followed by pb param making a request and a pb header followed by pb response for now.
          Doing header+param rather than a single protobuf Message with both header and param content:</p><p>
  </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Is closer to what we currently have</p></li><li class="listitem"><p>Having a single fat pb requires extra copying putting the already pb&#8217;d param into the body of the fat request pb (and same making result)</p></li><li class="listitem"><p>We can decide whether to accept the request or not before we read the param; for example, the request might be low priority. &nbsp;As is, we read header+param in one go as server is currently implemented so this is a TODO.</p></li></ol></div><p>
  </p><p>The advantages are minor. &nbsp;If later, fat request has clear advantage, can roll out a v2 later.</p></div><div class="section" title="J.4.3.&nbsp;RPC Configurations"><div class="titlepage"><div><div><h3 class="title"><a name="rpc.configs"></a>J.4.3.&nbsp;RPC Configurations</h3></div></div></div><div class="section" title="J.4.3.1.&nbsp;CellBlock Codecs"><div class="titlepage"><div><div><h4 class="title"><a name="d366e15754"></a>J.4.3.1.&nbsp;CellBlock Codecs</h4></div></div></div><p>To enable a codec other than the default <code class="classname">KeyValueCodec</code>,
          set <code class="varname">hbase.client.rpc.codec</code>
          to the name of the Codec class to use.  Codec must implement hbase's <code class="classname">Codec</code> Interface.  After connection setup,
          all passed cellblocks will be sent with this codec.  The server will return cellblocks using this same codec as long
          as the codec is on the servers' CLASSPATH (else you will get <code class="classname">UnsupportedCellCodecException</code>).</p><p>To change the default codec, set <code class="varname">hbase.client.default.rpc.codec</code>.
      </p><p>To disable cellblocks completely and to go pure protobuf, set the default to the
          empty String and do not specify a codec in your Configuration.  So, set <code class="varname">hbase.client.default.rpc.codec</code>
          to the empty string and do not set <code class="varname">hbase.client.rpc.codec</code>.
          This will cause the client to connect to the server with no codec specified.
          If a server sees no codec, it will return all responses in pure protobuf.
          Running pure protobuf all the time will be slower than running with cellblocks.
      </p></div><div class="section" title="J.4.3.2.&nbsp;Compression"><div class="titlepage"><div><div><h4 class="title"><a name="d366e15784"></a>J.4.3.2.&nbsp;Compression</h4></div></div></div><p>Uses hadoops compression codecs.  To enable compressing of passed CellBlocks, set <code class="varname">hbase.client.rpc.compressor</code>
          to the name of the Compressor to use.  Compressor must implement Hadoops' CompressionCodec Interface.  After connection setup,
          all passed cellblocks will be sent compressed.  The server will return cellblocks compressed using this same compressor as long
          as the compressor is on its CLASSPATH (else you will get <code class="classname">UnsupportedCompressionCodecException</code>).</p></div></div></div><div class="footnotes"><br><hr width="100" align="left"><div class="footnote"><p><sup>[<a id="ftn.d366e15632" href="#d366e15632" class="para">40</a>] </sup> We need the auth method spec. here so the connection header is encoded if auth enabled.</p></div></div></div><div class="index" title="Index"><div class="titlepage"><div><div><h2 class="title"><a name="book_index"></a>Index</h2></div></div></div><div class="index"><div class="indexdiv"><h3>C</h3><dl><dt>Cells, <a class="indexterm" href="#cells">Cells</a></dt><dt>Column Family, <a class="indexterm" href="#columnfamily">Column Family</a></dt><dt>Column Family Qualifier, <a class="indexterm" href="#columnfamily">Column Family</a></dt><dt>Compression, <a class="indexterm" href="#compression">Compression In HBase</a></dt></dl></div><div class="indexdiv"><h3>H</h3><dl><dt>Hadoop, <a class="indexterm" href="#hadoop">Hadoop</a></dt></dl></div><div class="indexdiv"><h3>I</h3><dl><dt>IntegrationTests, <a class="indexterm" href="#hbase.unittests.integration">Integration Tests</a></dt></dl></div><div class="indexdiv"><h3>L</h3><dl><dt>LargeTests, <a class="indexterm" href="#hbase.unittests.large">Large Tests</a></dt></dl></div><div class="indexdiv"><h3>M</h3><dl><dt>MediumTests, <a class="indexterm" href="#hbase.unittests.medium">Medium Tests</a></dt><dt>MSLAB, <a class="indexterm" href="#gcpause">Long GC pauses</a></dt></dl></div><div class="indexdiv"><h3>N</h3><dl><dt>nproc, <a class="indexterm" href="#ulimit">
          ulimit
            and
          nproc
        </a></dt></dl></div><div class="indexdiv"><h3>S</h3><dl><dt>SmallTests, <a class="indexterm" href="#hbase.unittests.small">Small Tests</a></dt></dl></div><div class="indexdiv"><h3>T</h3><dl><dt>Test Resource Checker, <a class="indexterm" href="#hbase.unittests.resource.checker">Test Resource Checker</a></dt></dl></div><div class="indexdiv"><h3>U</h3><dl><dt>ulimit, <a class="indexterm" href="#ulimit">
          ulimit
            and
          nproc
        </a></dt></dl></div><div class="indexdiv"><h3>V</h3><dl><dt>Versions, <a class="indexterm" href="#versions">Versions</a></dt></dl></div><div class="indexdiv"><h3>X</h3><dl><dt>xcievers, <a class="indexterm" href="#dfs.datanode.max.xcievers">dfs.datanode.max.xcievers</a></dt></dl></div><div class="indexdiv"><h3>Z</h3><dl><dt>ZooKeeper, <a class="indexterm" href="#zookeeper">ZooKeeper</a></dt></dl></div></div></div></div><div id="disqus_thread"></div><script type="text/javascript">
    var disqus_shortname = 'hbase'; // required: replace example with your forum shortname
    var disqus_url = 'http://hbase.apache.org/book';
    var disqus_identifier = 'book';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript><a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a></body></html>