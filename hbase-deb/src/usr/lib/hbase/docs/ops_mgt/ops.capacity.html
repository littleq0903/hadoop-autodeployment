<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
   <title>1.9.&nbsp;Capacity Planning and Region Sizing</title><link rel="stylesheet" type="text/css" href="../css/freebsd_docbook.css"><meta name="generator" content="DocBook XSL-NS Stylesheets V1.76.1"><link rel="home" href="ops_mgt.html" title="Chapter&nbsp;1.&nbsp;Apache HBase Operational Management"><link rel="up" href="ops_mgt.html" title="Chapter&nbsp;1.&nbsp;Apache HBase Operational Management"><link rel="prev" href="ops.snapshots.html" title="1.8.&nbsp;HBase Snapshots"><link rel="next" href="table.rename.html" title="1.10.&nbsp;Table Rename"></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><div class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="3" align="center">1.9.&nbsp;Capacity Planning and Region Sizing</th></tr><tr><td width="20%" align="left"><a accesskey="p" href="ops.snapshots.html">Prev</a>&nbsp;</td><th width="60%" align="center">&nbsp;</th><td width="20%" align="right">&nbsp;<a accesskey="n" href="table.rename.html">Next</a></td></tr></table><hr></div><div class="section" title="1.9.&nbsp;Capacity Planning and Region Sizing"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="ops.capacity"></a>1.9.&nbsp;Capacity Planning and Region Sizing</h2></div></div></div><p>There are several considerations when planning the capacity for an HBase cluster and performing the initial configuration. Start with a solid understanding of how HBase handles data internally.</p><div class="section" title="1.9.1.&nbsp;Node count and hardware/VM configuration"><div class="titlepage"><div><div><h3 class="title"><a name="ops.capacity.nodes"></a>1.9.1.&nbsp;Node count and hardware/VM configuration</h3></div></div></div><div class="section" title="1.9.1.1.&nbsp;Physical data size"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.nodes.datasize"></a>1.9.1.1.&nbsp;Physical data size</h4></div></div></div><p>Physical data size on disk is distinct from logical size of your data and is affected by the following:
</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Increased by HBase overhead
<div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">See <a class="xref" href="">???</a> and <a class="xref" href="">???</a>. At least 24 bytes per key-value (cell), can be more. Small keys/values means more relative overhead.</li><li class="listitem">KeyValue instances are aggregated into blocks, which are indexed. Indexes also have to be stored. Blocksize is configurable on a per-ColumnFamily basis. See <a class="xref" href="">???</a>.</li></ul></div></li><li class="listitem">Decreased by <a class="xref" href="">???</a> and data block encoding, depending on data. See also <a class="ulink" href="http://search-hadoop.com/m/lL12B1PFVhp1" target="_top">this thread</a>. You might want to test what compression and encoding (if any) make sense for your data.</li><li class="listitem">Increased by size of region server <a class="xref" href="">???</a> (usually fixed and negligible - less than half of RS memory size, per RS).</li><li class="listitem">Increased by HDFS replication - usually x3.</li></ul></div><p>Aside from the disk space necessary to store the data, one RS may not be able to serve arbitrarily large amounts of data due to some practical limits on region count and size (see <a class="xref" href="ops.capacity.html#ops.capacity.regions" title="1.9.2.&nbsp;Determining region count and size">below</a>).</p></div><div class="section" title="1.9.1.2.&nbsp;Read/Write throughput"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.nodes.throughput"></a>1.9.1.2.&nbsp;Read/Write throughput</h4></div></div></div><p>Number of nodes can also be driven by required thoughput for reads and/or writes. The  throughput one can get per node depends a lot on data (esp. key/value sizes) and request patterns, as well as node and system configuration. Planning should be done for peak load if it is likely that the load would be the main driver of the increase of the node count. PerformanceEvaluation and <a class="xref" href="">???</a> tools can be used to test single node or a test cluster.</p><p>For write, usually 5-15Mb/s per RS can be expected, since every region server has only one active WAL. There's no good estimate for reads, as it depends vastly on data, requests, and cache hit rate. <a class="xref" href="">???</a> might be helpful.</p></div><div class="section" title="1.9.1.3.&nbsp;JVM GC limitations"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.nodes.gc"></a>1.9.1.3.&nbsp;JVM GC limitations</h4></div></div></div><p>RS cannot currently utilize very large heap due to cost of GC. There's also no good way of running multiple RS-es per server (other than running several VMs per machine). Thus, ~20-24Gb or less memory dedicated to one RS is recommended. GC tuning is required for large heap sizes. See <a class="xref" href="">???</a>, <a class="xref" href="">???</a> and elsewhere (TODO: where?)</p></div></div><div class="section" title="1.9.2.&nbsp;Determining region count and size"><div class="titlepage"><div><div><h3 class="title"><a name="ops.capacity.regions"></a>1.9.2.&nbsp;Determining region count and size</h3></div></div></div><p>Generally less regions makes for a smoother running cluster (you can always manually split the big regions later (if necessary) to spread the data, or request load, over the cluster); 20-200 regions per RS is a reasonable range. The number of regions cannot be configured directly (unless you go for fully <a class="xref" href="">???</a>); adjust the region size to achieve the target region size given table size.</p><p>When configuring regions for multiple tables, note that most region settings can be set on a per-table basis via <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HTableDescriptor.html" target="_top">HTableDescriptor</a>, as well as shell commands. These settings will override the ones in <code class="varname">hbase-site.xml</code>. That is useful if your tables have different workloads/use cases.</p><p>Also note that in the discussion of region sizes here, <span class="bold"><strong>HDFS replication factor is not (and should not be) taken into account, whereas other factors <a class="xref" href="ops.capacity.html#ops.capacity.nodes.datasize" title="1.9.1.1.&nbsp;Physical data size">above</a> should be.</strong></span> So, if your data is compressed and replicated 3 ways by HDFS, "9 Gb region" means 9 Gb of compressed data. HDFS replication factor only affects your disk usage and is invisible to most HBase code.</p><div class="section" title="1.9.2.1.&nbsp;Number of regions per RS - upper bound"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.regions.count"></a>1.9.2.1.&nbsp;Number of regions per RS - upper bound</h4></div></div></div><p>In production scenarios, where you have a lot of data, you are normally concerned with the maximum number of regions you can have per server. <a class="xref" href="">???</a> has technical discussion on the subject; in short, maximum number of regions is mostly determined by memstore memory usage. Each region has its own memstores; these grow up to a configurable size; usually in 128-256Mb range, see <a class="xref" href="">???</a>. There's one memstore per column family (so there's only one per region if there's one CF in the table). RS dedicates some fraction of total memory (see <a class="xref" href="">???</a>) to region memstores. If this memory is exceeded (too much memstore usage), undesirable consequences such as unresponsive server, or later compaction storms, can result. Thus, a good starting point for the number of regions per RS (assuming one table) is </p><pre class="programlisting">(RS memory)*(total memstore fraction)/((memstore size)*(# column families))</pre><p>
E.g. if RS has 16Gb RAM, with default settings, it is 16384*0.4/128 ~ 51 regions per RS is a starting point. The formula can be extended to multiple tables; if they all have the same configuration, just use total number of families.</p><p>This number can be adjusted; the formula above assumes all your regions are filled at approximately the same rate. If only a fraction of your regions are going to be actively written to, you can divide the result by that fraction to get a larger region count. Then, even if all regions are written to, all region memstores are not filled evenly, and eventually jitter appears even if they are (due to limited number of concurrent flushes). Thus, one can have as many as 2-3 times more regions than the starting point; however, increased numbers carry increased risk.</p><p>For write-heavy workload, memstore fraction can be increased in configuration at the expense of block cache; this will also allow one to have more regions.</p></div><div class="section" title="1.9.2.2.&nbsp;Number of regions per RS - lower bound"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.regions.mincount"></a>1.9.2.2.&nbsp;Number of regions per RS - lower bound</h4></div></div></div><p>HBase scales by having regions across many servers. Thus if you have 2 regions for 16GB data, on a 20 node machine your data will be concentrated on just a few machines - nearly the entire        cluster will be idle. This really can't be stressed enough, since a common problem is loading 200MB data into HBase and then wondering why your awesome 10 node cluster isn't doing anything.</p><p>On the other hand, if you have a very large amount of data, you may also want to go for a larger number of regions to avoid having regions that are too large.</p></div><div class="section" title="1.9.2.3.&nbsp;Maximum region size"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.regions.size"></a>1.9.2.3.&nbsp;Maximum region size</h4></div></div></div><p>For large tables in production scenarios, maximum region size is mostly limited by compactions - very large compactions, esp. major, can degrade cluster performance. Currently, the recommended maximum region size is 10-20Gb, and 5-10Gb is optimal. For older 0.90.x codebase, the upper-bound of regionsize is about 4Gb, with a default of 256Mb.</p><p>The size at which the region is split into two is generally configured via <a class="xref" href="">???</a>; for details, see <a class="xref" href="">???</a>.</p><p>If you cannot estimate the size of your tables well, when starting off, it's probably best to stick to the default region size, perhaps going smaller for hot tables (or manually split hot regions to spread the load over the cluster), or go with larger region sizes if your cell sizes tend to be largish (100k and up).</p><p>In HBase 0.98, experimental stripe compactions feature was added that would allow for larger regions, especially for log data. See <a class="xref" href="">???</a>.</p></div><div class="section" title="1.9.2.4.&nbsp;Total data size per region server"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.regions.total"></a>1.9.2.4.&nbsp;Total data size per region server</h4></div></div></div><p>According to above numbers for region size and number of regions per region server, in an optimistic estimate 10 GB x 100 regions per RS will give up to 1TB served per region server, which is in line with some of the reported multi-PB use cases. However, it is important to think about the data vs cache size ratio at the RS level. With 1TB of data per server and 10 GB block cache, only 1% of the data will be cached, which may barely cover all block indices.</p></div></div><div class="section" title="1.9.3.&nbsp;Initial configuration and tuning"><div class="titlepage"><div><div><h3 class="title"><a name="ops.capacity.config"></a>1.9.3.&nbsp;Initial configuration and tuning</h3></div></div></div><p>First, see <a class="xref" href="">???</a>. Note that some configurations, more than others, depend on specific scenarios. Pay special attention to 
</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><a class="xref" href="">???</a> - request handler thread count, vital for high-throughput workloads.</li><li class="listitem"><a class="xref" href="">???</a> - the blocking number of WAL files depends on your memstore configuration and should be set accordingly to prevent potential blocking when doing high volume of writes.</li></ul></div><p>Then, there are some considerations when setting up your cluster and tables.</p><div class="section" title="1.9.3.1.&nbsp;Compactions"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.config.compactions"></a>1.9.3.1.&nbsp;Compactions</h4></div></div></div><p>Depending on read/write volume and latency requirements, optimal compaction settings may be different. See <a class="xref" href="">???</a> for some details.</p><p>When provisioning for large data sizes, however, it's good to keep in mind that compactions can affect write throughput. Thus, for write-intensive workloads, you may opt for less frequent compactions and more store files per regions. Minimum number of files for compactions (<code class="varname">hbase.hstore.compaction.min</code>) can be set to higher value; <a class="xref" href="">???</a> should also be increased, as more files might accumulate in such case. You may also consider manually managing compactions: <a class="xref" href="">???</a></p></div><div class="section" title="1.9.3.2.&nbsp;Pre-splitting the table"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.config.presplit"></a>1.9.3.2.&nbsp;Pre-splitting the table</h4></div></div></div><p>Based on the target number of the regions per RS (see <a class="xref" href="ops.capacity.html#ops.capacity.regions.count" title="1.9.2.1.&nbsp;Number of regions per RS - upper bound">above</a>) and number of RSes, one can pre-split the table at creation time. This would both avoid some costly splitting as the table starts to fill up, and ensure that the table starts out already distributed across many servers.</p><p>If the table is expected to grow large enough to justify that, at least one region per RS should be created. It is not recommended to split immediately into the full target number of regions (e.g. 50 * number of RSes), but a low intermediate value can be chosen. For multiple tables, it is recommended to be conservative with presplitting (e.g. pre-split 1 region per RS at most), especially if you don't know how much each table will grow. If you split too much, you may end up with too many regions, with some tables having too many small regions.</p><p>For pre-splitting howto, see <a class="xref" href="">???</a>.</p></div></div></div><div id="disqus_thread"></div><script type="text/javascript">
    var disqus_shortname = 'hbase'; // required: replace example with your forum shortname
    var disqus_url = 'http://hbase.apache.org/book';
    var disqus_identifier = 'ops.capacity';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript><a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a><div class="navfooter"><hr><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="ops.snapshots.html">Prev</a>&nbsp;</td><td width="20%" align="center">&nbsp;</td><td width="40%" align="right">&nbsp;<a accesskey="n" href="table.rename.html">Next</a></td></tr><tr><td width="40%" align="left" valign="top">1.8.&nbsp;HBase Snapshots&nbsp;</td><td width="20%" align="center"><a accesskey="h" href="ops_mgt.html">Home</a></td><td width="40%" align="right" valign="top">&nbsp;1.10.&nbsp;Table Rename</td></tr></table></div></body></html>