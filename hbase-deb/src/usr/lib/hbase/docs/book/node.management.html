<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
   <title>15.3.&nbsp;Node Management</title><link rel="stylesheet" type="text/css" href="../css/freebsd_docbook.css"><meta name="generator" content="DocBook XSL-NS Stylesheets V1.76.1"><link rel="home" href="book.html" title="The Apache HBase&#153; Reference Guide"><link rel="up" href="ops_mgt.html" title="Chapter&nbsp;15.&nbsp;Apache HBase Operational Management"><link rel="prev" href="ops.regionmgt.html" title="15.2.&nbsp;Region Management"><link rel="next" href="hbase_metrics.html" title="15.4.&nbsp;HBase Metrics"></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><div class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="3" align="center">15.3.&nbsp;Node Management</th></tr><tr><td width="20%" align="left"><a accesskey="p" href="ops.regionmgt.html">Prev</a>&nbsp;</td><th width="60%" align="center">Chapter&nbsp;15.&nbsp;Apache HBase Operational Management</th><td width="20%" align="right">&nbsp;<a accesskey="n" href="hbase_metrics.html">Next</a></td></tr></table><hr></div><div class="section" title="15.3.&nbsp;Node Management"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="node.management"></a>15.3.&nbsp;Node Management</h2></div></div></div><div class="section" title="15.3.1.&nbsp;Node Decommission"><div class="titlepage"><div><div><h3 class="title"><a name="decommission"></a>15.3.1.&nbsp;Node Decommission</h3></div></div></div><p>You can stop an individual RegionServer by running the following
            script in the HBase directory on the particular  node:
            </p><pre class="programlisting">$ ./bin/hbase-daemon.sh stop regionserver</pre><p>
            The RegionServer will first close all regions and then shut itself down.
            On shutdown, the RegionServer's ephemeral node in ZooKeeper will expire.
            The master will notice the RegionServer gone and will treat it as
            a 'crashed' server; it will reassign the nodes the RegionServer was carrying.
            </p><div class="note" title="Disable the Load Balancer before Decommissioning a node" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Disable the Load Balancer before Decommissioning a node</h3><p>If the load balancer runs while a node is shutting down, then
                 there could be contention between the Load Balancer and the
                 Master's recovery of the just decommissioned RegionServer.
                 Avoid any problems by disabling the balancer first.
                 See <a class="xref" href="node.management.html#lb" title="Load Balancer">Load Balancer</a> below.
             </p></div><p>
        </p><p>
        A downside to the above stop of a RegionServer is that regions could be offline for
        a good period of time.  Regions are closed in order.  If many regions on the server, the
        first region to close may not be back online until all regions close and after the master
        notices the RegionServer's znode gone.  In Apache HBase 0.90.2, we added facility for having
        a node gradually shed its load and then shutdown itself down. Apache HBase 0.90.2 added the
            <code class="filename">graceful_stop.sh</code> script.  Here is its usage:
            </p><pre class="programlisting">$ ./bin/graceful_stop.sh
Usage: graceful_stop.sh [--config &amp;conf-dir&gt;] [--restart] [--reload] [--thrift] [--rest] &amp;hostname&gt;
 thrift      If we should stop/start thrift before/after the hbase stop/start
 rest        If we should stop/start rest before/after the hbase stop/start
 restart     If we should restart after graceful stop
 reload      Move offloaded regions back on to the stopped server
 debug       Move offloaded regions back on to the stopped server
 hostname    Hostname of server we are to stop</pre><p>
        </p><p>
            To decommission a loaded RegionServer, run the following:
            </p><pre class="programlisting">$ ./bin/graceful_stop.sh HOSTNAME</pre><p>
            where <code class="varname">HOSTNAME</code> is the host carrying the RegionServer
            you would decommission.
            </p><div class="note" title="On HOSTNAME" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">On <code class="varname">HOSTNAME</code></h3><p>The <code class="varname">HOSTNAME</code> passed to <code class="filename">graceful_stop.sh</code>
            must match the hostname that hbase is using to identify RegionServers.
            Check the list of RegionServers in the master UI for how HBase is
            referring to servers. Its usually hostname but can also be FQDN.
            Whatever HBase is using, this is what you should pass the
            <code class="filename">graceful_stop.sh</code> decommission
            script.  If you pass IPs, the script is not yet smart enough to make
            a hostname (or FQDN) of it and so it will fail when it checks if server is
            currently running; the graceful unloading of regions will not run.
            </p></div><p> The <code class="filename">graceful_stop.sh</code> script will move the regions off the
            decommissioned RegionServer one at a time to minimize region churn.
            It will verify the region deployed in the new location before it
            will moves the next region and so on until the decommissioned server
            is carrying zero regions.  At this point, the <code class="filename">graceful_stop.sh</code>
            tells the RegionServer <span class="command"><strong>stop</strong></span>.  The master will at this point notice the
            RegionServer gone but all regions will have already been redeployed
            and because the RegionServer went down cleanly, there will be no
            WAL logs to split.
            </p><div class="note" title="Load Balancer" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a name="lb"></a>Load Balancer</h3><p>
                It is assumed that the Region Load Balancer is disabled while the
                <span class="command"><strong>graceful_stop</strong></span> script runs (otherwise the balancer
                and the decommission script will end up fighting over region deployments).
                Use the shell to disable the balancer:
                </p><pre class="programlisting">hbase(main):001:0&gt; balance_switch false
true
0 row(s) in 0.3590 seconds</pre><p>
This turns the balancer OFF.  To reenable, do:
                </p><pre class="programlisting">hbase(main):001:0&gt; balance_switch true
false
0 row(s) in 0.3590 seconds</pre><p>
            </p><p>The <span class="command"><strong>graceful_stop</strong></span> will check the balancer
                and if enabled, will turn it off before it goes to work.  If it
                exits prematurely because of error, it will not have reset the
                balancer.  Hence, it is better to manage the balancer apart from
                <span class="command"><strong>graceful_stop</strong></span> reenabling it after you are done
                w/ graceful_stop.
            </p></div><p>
        </p><div class="section" title="15.3.1.1.&nbsp;Decommissioning several Regions Servers concurrently"><div class="titlepage"><div><div><h4 class="title"><a name="draining.servers"></a>15.3.1.1.&nbsp;Decommissioning several Regions Servers concurrently</h4></div></div></div><p>If you have a large cluster, you may want to
            decommission more than one machine at a time by gracefully
            stopping mutiple RegionServers concurrently.
            To gracefully drain multiple regionservers at the
	    same time, RegionServers can be put into a "draining"
	    state.  This is done by marking a RegionServer as a
	    draining node by creating an entry in ZooKeeper under the
        <code class="filename">hbase_root/draining</code> znode.  This znode has format
        </p><pre class="programlisting">name,port,startcode</pre><p> just like the regionserver entries
        under <code class="filename">hbase_root/rs</code> znode.
	    </p><p>Without this facility, decommissioning mulitple nodes
	    may be non-optimal because regions that are being drained
	    from one region server may be moved to other regionservers that
	    are also draining.  Marking RegionServers to be in the
        draining state prevents this from happening<sup>[<a name="d366e11363" href="#ftn.d366e11363" class="footnote">33</a>]</sup>.
	    </p></div><div class="section" title="15.3.1.2.&nbsp;Bad or Failing Disk"><div class="titlepage"><div><div><h4 class="title"><a name="bad.disk"></a>15.3.1.2.&nbsp;Bad or Failing Disk</h4></div></div></div><p>It is good having <a class="xref" href="important_configurations.html#dfs.datanode.failed.volumes.tolerated" title="2.5.2.2.1.&nbsp;dfs.datanode.failed.volumes.tolerated">Section&nbsp;2.5.2.2.1, &#8220;dfs.datanode.failed.volumes.tolerated&#8221;</a> set if you have a decent number of disks
            per machine for the case where a disk plain dies.  But usually disks do the "John Wayne" -- i.e. take a while
            to go down spewing errors in <code class="filename">dmesg</code> -- or for some reason, run much slower than their
            companions.  In this case you want to decommission the disk.  You have two options.  You can
            <a class="link" href="http://wiki.apache.org/hadoop/FAQ#I_want_to_make_a_large_cluster_smaller_by_taking_out_a_bunch_of_nodes_simultaneously._How_can_this_be_done.3F" target="_top">decommission the datanode</a>
            or, less disruptive in that only the bad disks data will be rereplicated, can stop the datanode,
            unmount the bad volume (You can't umount a volume while the datanode is using it), and then restart the
            datanode (presuming you have set dfs.datanode.failed.volumes.tolerated &gt; 0).  The regionserver will
            throw some errors in its logs as it recalibrates where to get its data from -- it will likely
            roll its WAL log too -- but in general but for some latency spikes, it should keep on chugging.
            </p><div class="note" title="Short Circuit Reads" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Short Circuit Reads</h3><p>If you are doing short-circuit reads, you will have to move the regions off the regionserver
                    before you stop the datanode; when short-circuiting reading, though chmod'd so regionserver cannot
                    have access, because it already has the files open, it will be able to keep reading the file blocks
                    from the bad disk even though the datanode is down.  Move the regions back after you restart the
                datanode.</p></div><p>
            </p></div></div><div class="section" title="15.3.2.&nbsp;Rolling Restart"><div class="titlepage"><div><div><h3 class="title"><a name="rolling"></a>15.3.2.&nbsp;Rolling Restart</h3></div></div></div><p>
            You can also ask this script to restart a RegionServer after the shutdown
            AND move its old regions back into place.  The latter you might do to
            retain data locality.  A primitive rolling restart might be effected by
            running something like the following:
            </p><pre class="programlisting">$ for i in `cat conf/regionservers|sort`; do ./bin/graceful_stop.sh --restart --reload --debug $i; done &amp;&gt; /tmp/log.txt &amp;
            </pre><p>
            Tail the output of <code class="filename">/tmp/log.txt</code> to follow the scripts
            progress. The above does RegionServers only.  The script will also disable the
            load balancer before moving the regions.  You'd need to do the master
            update separately.  Do it before you run the above script.
            Here is a pseudo-script for how you might craft a rolling restart script:
            </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Untar your release, make sure of its configuration and
                        then rsync it across the cluster. If this is 0.90.2, patch it
                        with HBASE-3744 and HBASE-3756.
                    </p></li><li class="listitem"><p>Run hbck to ensure the cluster consistent
                        </p><pre class="programlisting">$ ./bin/hbase hbck</pre><p>
                    Effect repairs if inconsistent.
                    </p></li><li class="listitem"><p>Restart the Master: </p><pre class="programlisting">$ ./bin/hbase-daemon.sh stop master; ./bin/hbase-daemon.sh start master</pre><p>
                    </p></li><li class="listitem"><p>Run the <code class="filename">graceful_stop.sh</code> script per RegionServer.  For example:
            </p><pre class="programlisting">$ for i in `cat conf/regionservers|sort`; do ./bin/graceful_stop.sh --restart --reload --debug $i; done &amp;&gt; /tmp/log.txt &amp;
            </pre><p>
                     If you are running thrift or rest servers on the RegionServer, pass --thrift or --rest options (See usage
                     for <code class="filename">graceful_stop.sh</code> script).
                 </p></li><li class="listitem"><p>Restart the Master again.  This will clear out dead servers list and reenable the balancer.
                    </p></li><li class="listitem"><p>Run hbck to ensure the cluster is consistent.
                    </p></li></ol></div><p>
        </p><p>It is important to drain HBase regions slowly when
	restarting regionservers. Otherwise, multiple regions go
	offline simultaneously as they are re-assigned to other
	nodes. Depending on your usage patterns, this might not be
	desirable.
	</p></div><div class="section" title="15.3.3.&nbsp;Adding a New Node"><div class="titlepage"><div><div><h3 class="title"><a name="adding.new.node"></a>15.3.3.&nbsp;Adding a New Node</h3></div></div></div><p>Adding a new regionserver in HBase is essentially free, you simply start it like this:
              </p><pre class="programlisting">$ ./bin/hbase-daemon.sh start regionserver</pre><p>
              and it will register itself with the master. Ideally you also started a DataNode on the same
              machine so that the RS can eventually start to have local files. If you rely on ssh to start your
              daemons, don't forget to add the new hostname in <code class="filename">conf/regionservers</code> on the master.
        </p><p>At this point the region server isn't serving data because no regions have moved to it yet. If the balancer is
              enabled, it will start moving regions to the new RS. On a small/medium cluster this can have a very adverse effect
              on latency as a lot of regions will be offline at the same time. It is thus recommended to disable the balancer
              the same way it's done when decommissioning a node and move the regions manually (or even better, using a script
              that moves them one by one).
        </p><p>The moved regions will all have 0% locality and won't have any blocks in cache so the region server will have
              to use the network to serve requests. Apart from resulting in higher latency, it may also be able to use all of
              your network card's capacity. For practical purposes, consider that a standard 1GigE NIC won't be able to read
              much more than <span class="emphasis"><em>100MB/s</em></span>. In this case, or if you are in a OLAP environment and require having
              locality, then it is recommended to major compact the moved regions.
        </p></div><div class="footnotes"><br><hr width="100" align="left"><div class="footnote"><p><sup>[<a id="ftn.d366e11363" href="#d366e11363" class="para">33</a>] </sup>See
	    this <a class="link" href="http://inchoate-clatter.blogspot.com/2012/03/hbase-ops-automation.html" target="_top">blog
            post</a> for more details.</p></div></div></div><div id="disqus_thread"></div><script type="text/javascript">
    var disqus_shortname = 'hbase'; // required: replace example with your forum shortname
    var disqus_url = 'http://hbase.apache.org/book';
    var disqus_identifier = 'node.management';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript><a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a><div class="navfooter"><hr><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="ops.regionmgt.html">Prev</a>&nbsp;</td><td width="20%" align="center"><a accesskey="u" href="ops_mgt.html">Up</a></td><td width="40%" align="right">&nbsp;<a accesskey="n" href="hbase_metrics.html">Next</a></td></tr><tr><td width="40%" align="left" valign="top">15.2.&nbsp;Region Management&nbsp;</td><td width="20%" align="center"><a accesskey="h" href="book.html">Home</a></td><td width="40%" align="right" valign="top">&nbsp;15.4.&nbsp;HBase Metrics</td></tr></table></div></body></html>