<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
   <title>12.9.&nbsp;Reading from HBase</title><link rel="stylesheet" type="text/css" href="../css/freebsd_docbook.css"><meta name="generator" content="DocBook XSL-NS Stylesheets V1.76.1"><link rel="home" href="book.html" title="The Apache HBase&#153; Reference Guide"><link rel="up" href="performance.html" title="Chapter&nbsp;12.&nbsp;Apache HBase Performance Tuning"><link rel="prev" href="perf.writing.html" title="12.8.&nbsp;Writing to HBase"><link rel="next" href="perf.deleting.html" title="12.10.&nbsp;Deleting from HBase"></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><div class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="3" align="center">12.9.&nbsp;Reading from HBase</th></tr><tr><td width="20%" align="left"><a accesskey="p" href="perf.writing.html">Prev</a>&nbsp;</td><th width="60%" align="center">Chapter&nbsp;12.&nbsp;Apache HBase Performance Tuning</th><td width="20%" align="right">&nbsp;<a accesskey="n" href="perf.deleting.html">Next</a></td></tr></table><hr></div><div class="section" title="12.9.&nbsp;Reading from HBase"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="perf.reading"></a>12.9.&nbsp;Reading from HBase</h2></div></div></div><p>The mailing list can help if you are having performance issues.
    For example, here is a good general thread on what to look at addressing
    read-time issues: <a class="link" href="http://search-hadoop.com/m/qOo2yyHtCC1" target="_top">HBase Random Read latency &gt; 100ms</a></p><div class="section" title="12.9.1.&nbsp;Scan Caching"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.client.caching"></a>12.9.1.&nbsp;Scan Caching</h3></div></div></div><p>If HBase is used as an input source for a MapReduce job, for
      example, make sure that the input <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Scan.html" target="_top">Scan</a>
      instance to the MapReduce job has <code class="methodname">setCaching</code> set to something greater
      than the default (which is 1). Using the default value means that the
      map-task will make call back to the region-server for every record
      processed. Setting this value to 500, for example, will transfer 500
      rows at a time to the client to be processed. There is a cost/benefit to
      have the cache value be large because it costs more in memory for both
      client and RegionServer, so bigger isn't always better.</p><div class="section" title="12.9.1.1.&nbsp;Scan Caching in MapReduce Jobs"><div class="titlepage"><div><div><h4 class="title"><a name="perf.hbase.client.caching.mr"></a>12.9.1.1.&nbsp;Scan Caching in MapReduce Jobs</h4></div></div></div><p>Scan settings in MapReduce jobs deserve special attention.  Timeouts can result (e.g., UnknownScannerException)
        in Map tasks if it takes longer to process a batch of records before the client goes back to the RegionServer for the
        next set of data.  This problem can occur because there is non-trivial processing occuring per row.  If you process
        rows quickly, set caching higher.  If you process rows more slowly (e.g., lots of transformations per row, writes),
        then set caching lower.
        </p><p>Timeouts can also happen in a non-MapReduce use case (i.e., single threaded HBase client doing a Scan), but the
        processing that is often performed in MapReduce jobs tends to exacerbate this issue.
        </p></div></div><div class="section" title="12.9.2.&nbsp;Scan Attribute Selection"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.client.selection"></a>12.9.2.&nbsp;Scan Attribute Selection</h3></div></div></div><p>Whenever a Scan is used to process large numbers of rows (and especially when used
      as a MapReduce source), be aware of which attributes are selected.   If <code class="code">scan.addFamily</code> is called
      then <span class="emphasis"><em>all</em></span> of the attributes in the specified ColumnFamily will be returned to the client.
      If only a small number of the available attributes are to be processed, then only those attributes should be specified
      in the input scan because attribute over-selection is a non-trivial performance penalty over large datasets.
      </p></div><div class="section" title="12.9.3.&nbsp;Avoid scan seeks"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.client.seek"></a>12.9.3.&nbsp;Avoid scan seeks</h3></div></div></div><p>When columns are selected explicitly with <code class="code">scan.addColumn</code>, HBase will schedule seek operations to seek between the
      selected columns. When rows have few columns and each column has only a few versions this can be inefficient. A seek operation is generally
      slower if does not seek at least past 5-10 columns/versions or 512-1024 bytes.</p><p>In order to opportunistically look ahead a few columns/versions to see if the next column/version can be found that
      way before a seek operation is scheduled, a new attribute <code class="code">Scan.HINT_LOOKAHEAD</code> can be set the on Scan object. The following code instructs the
      RegionServer to attempt two iterations of next before a seek is scheduled:</p><pre class="programlisting">
Scan scan = new Scan();
scan.addColumn(...);
scan.setAttribute(Scan.HINT_LOOKAHEAD, Bytes.toBytes(2));
table.getScanner(scan);
</pre></div><div class="section" title="12.9.4.&nbsp;MapReduce - Input Splits"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.mr.input"></a>12.9.4.&nbsp;MapReduce - Input Splits</h3></div></div></div><p>For MapReduce jobs that use HBase tables as a source, if there a pattern where the "slow" map tasks seem to
        have the same Input Split (i.e., the RegionServer serving the data), see the
        Troubleshooting Case Study in <a class="xref" href="casestudies.perftroub.html#casestudies.slownode" title="14.3.1.&nbsp;Case Study #1 (Performance Issue On A Single Node)">Section&nbsp;14.3.1, &#8220;Case Study #1 (Performance Issue On A Single Node)&#8221;</a>.
        </p></div><div class="section" title="12.9.5.&nbsp;Close ResultScanners"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.client.scannerclose"></a>12.9.5.&nbsp;Close ResultScanners</h3></div></div></div><p>This isn't so much about improving performance but rather
      <span class="emphasis"><em>avoiding</em></span> performance problems. If you forget to
      close <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/ResultScanner.html" target="_top">ResultScanners</a>
      you can cause problems on the RegionServers. Always have ResultScanner
      processing enclosed in try/catch blocks... </p><pre class="programlisting">
Scan scan = new Scan();
// set attrs...
ResultScanner rs = htable.getScanner(scan);
try {
  for (Result r = rs.next(); r != null; r = rs.next()) {
  // process result...
} finally {
  rs.close();  // always close the ResultScanner!
}
htable.close();</pre></div><div class="section" title="12.9.6.&nbsp;Block Cache"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.client.blockcache"></a>12.9.6.&nbsp;Block Cache</h3></div></div></div><p><a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Scan.html" target="_top">Scan</a>
      instances can be set to use the block cache in the RegionServer via the
      <code class="methodname">setCacheBlocks</code> method. For input Scans to MapReduce jobs, this should be
      <code class="varname">false</code>. For frequently accessed rows, it is advisable to use the block
      cache.</p></div><div class="section" title="12.9.7.&nbsp;Optimal Loading of Row Keys"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.client.rowkeyonly"></a>12.9.7.&nbsp;Optimal Loading of Row Keys</h3></div></div></div><p>When performing a table <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Scan.html" target="_top">scan</a>
            where only the row keys are needed (no families, qualifiers, values or timestamps), add a FilterList with a
            <code class="varname">MUST_PASS_ALL</code> operator to the scanner using <code class="methodname">setFilter</code>. The filter list
            should include both a <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/FirstKeyOnlyFilter.html" target="_top">FirstKeyOnlyFilter</a>
            and a <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/KeyOnlyFilter.html" target="_top">KeyOnlyFilter</a>.
            Using this filter combination will result in a worst case scenario of a RegionServer reading a single value from disk
            and minimal network traffic to the client for a single row.
      </p></div><div class="section" title="12.9.8.&nbsp;Concurrency: Monitor Data Spread"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.read.dist"></a>12.9.8.&nbsp;Concurrency:  Monitor Data Spread</h3></div></div></div><p>When performing a high number of concurrent reads, monitor the data spread of the target tables.  If the target table(s) have
      too few regions then the reads could likely be served from too few nodes.  </p><p>See <a class="xref" href="perf.writing.html#precreate.regions" title="12.8.2.&nbsp; Table Creation: Pre-Creating Regions">Section&nbsp;12.8.2, &#8220;
    Table Creation: Pre-Creating Regions
    &#8221;</a>, as well as <a class="xref" href="perf.configurations.html" title="12.4.&nbsp;HBase Configurations">Section&nbsp;12.4, &#8220;HBase Configurations&#8221;</a> </p></div><div class="section" title="12.9.9.&nbsp;Bloom Filters"><div class="titlepage"><div><div><h3 class="title"><a name="blooms"></a>12.9.9.&nbsp;Bloom Filters</h3></div></div></div><p>Enabling Bloom Filters can save your having to go to disk and
         can help improve read latencies.</p><p><a class="link" href="http://en.wikipedia.org/wiki/Bloom_filter" target="_top">Bloom filters</a> were developed over in <a class="link" href="https://issues.apache.org/jira/browse/HBASE-1200" target="_top">HBase-1200
    Add bloomfilters</a>.<sup>[<a name="d366e9233" href="#ftn.d366e9233" class="footnote">29</a>]</sup><sup>[<a name="d366e9245" href="#ftn.d366e9245" class="footnote">30</a>]</sup></p><p>See also <a class="xref" href="perf.schema.html#schema.bloom" title="12.6.4.&nbsp;Bloom Filters">Section&nbsp;12.6.4, &#8220;Bloom Filters&#8221;</a>.
        </p><div class="section" title="12.9.9.1.&nbsp;Bloom StoreFile footprint"><div class="titlepage"><div><div><h4 class="title"><a name="bloom_footprint"></a>12.9.9.1.&nbsp;Bloom StoreFile footprint</h4></div></div></div><p>Bloom filters add an entry to the <code class="classname">StoreFile</code>
      general <code class="classname">FileInfo</code> data structure and then two
      extra entries to the <code class="classname">StoreFile</code> metadata
      section.</p><div class="section" title="12.9.9.1.1.&nbsp;BloomFilter in the StoreFile FileInfo data structure"><div class="titlepage"><div><div><h5 class="title"><a name="d366e9269"></a>12.9.9.1.1.&nbsp;BloomFilter in the <code class="classname">StoreFile</code>
        <code class="classname">FileInfo</code> data structure</h5></div></div></div><p><code class="classname">FileInfo</code> has a
          <code class="varname">BLOOM_FILTER_TYPE</code> entry which is set to
          <code class="varname">NONE</code>, <code class="varname">ROW</code> or
          <code class="varname">ROWCOL.</code></p></div><div class="section" title="12.9.9.1.2.&nbsp;BloomFilter entries in StoreFile metadata"><div class="titlepage"><div><div><h5 class="title"><a name="d366e9293"></a>12.9.9.1.2.&nbsp;BloomFilter entries in <code class="classname">StoreFile</code>
        metadata</h5></div></div></div><p><code class="varname">BLOOM_FILTER_META</code> holds Bloom Size, Hash
          Function used, etc. Its small in size and is cached on
          <code class="classname">StoreFile.Reader</code> load</p><p><code class="varname">BLOOM_FILTER_DATA</code> is the actual bloomfilter
          data. Obtained on-demand. Stored in the LRU cache, if it is enabled
          (Its enabled by default).</p></div></div><div class="section" title="12.9.9.2.&nbsp;Bloom Filter Configuration"><div class="titlepage"><div><div><h4 class="title"><a name="config.bloom"></a>12.9.9.2.&nbsp;Bloom Filter Configuration</h4></div></div></div><div class="section" title="12.9.9.2.1.&nbsp;io.hfile.bloom.enabled global kill switch"><div class="titlepage"><div><div><h5 class="title"><a name="d366e9313"></a>12.9.9.2.1.&nbsp;<code class="varname">io.hfile.bloom.enabled</code> global kill
        switch</h5></div></div></div><p><code class="code">io.hfile.bloom.enabled</code> in
        <code class="classname">Configuration</code> serves as the kill switch in case
        something goes wrong. Default = <code class="varname">true</code>.</p></div><div class="section" title="12.9.9.2.2.&nbsp;io.hfile.bloom.error.rate"><div class="titlepage"><div><div><h5 class="title"><a name="d366e9328"></a>12.9.9.2.2.&nbsp;<code class="varname">io.hfile.bloom.error.rate</code></h5></div></div></div><p><code class="varname">io.hfile.bloom.error.rate</code> = average false
        positive rate. Default = 1%. Decrease rate by &frac12; (e.g. to .5%) == +1
        bit per bloom entry.</p></div><div class="section" title="12.9.9.2.3.&nbsp;io.hfile.bloom.max.fold"><div class="titlepage"><div><div><h5 class="title"><a name="d366e9336"></a>12.9.9.2.3.&nbsp;<code class="varname">io.hfile.bloom.max.fold</code></h5></div></div></div><p><code class="varname">io.hfile.bloom.max.fold</code> = guaranteed minimum
        fold rate. Most people should leave this alone. Default = 7, or can
        collapse to at least 1/128th of original size. See the
        <span class="emphasis"><em>Development Process</em></span> section of the document <a class="link" href="https://issues.apache.org/jira/secure/attachment/12444007/Bloom_Filters_in_HBase.pdf" target="_top">BloomFilters
        in HBase</a> for more on what this option means.</p></div></div></div><div class="footnotes"><br><hr width="100" align="left"><div class="footnote"><p><sup>[<a id="ftn.d366e9233" href="#d366e9233" class="para">29</a>] </sup>For description of the development process -- why static blooms
        rather than dynamic -- and for an overview of the unique properties
        that pertain to blooms in HBase, as well as possible future
        directions, see the <span class="emphasis"><em>Development Process</em></span> section
        of the document <a class="link" href="https://issues.apache.org/jira/secure/attachment/12444007/Bloom_Filters_in_HBase.pdf" target="_top">BloomFilters
        in HBase</a> attached to <a class="link" href="https://issues.apache.org/jira/browse/HBASE-1200" target="_top">HBase-1200</a>.</p></div><div class="footnote"><p><sup>[<a id="ftn.d366e9245" href="#d366e9245" class="para">30</a>] </sup>The bloom filters described here are actually version two of
        blooms in HBase. In versions up to 0.19.x, HBase had a dynamic bloom
        option based on work done by the <a class="link" href="http://www.one-lab.org" target="_top">European Commission One-Lab
        Project 034819</a>. The core of the HBase bloom work was later
        pulled up into Hadoop to implement org.apache.hadoop.io.BloomMapFile.
        Version 1 of HBase blooms never worked that well. Version 2 is a
        rewrite from scratch though again it starts with the one-lab
        work.</p></div></div></div><div id="disqus_thread"></div><script type="text/javascript">
    var disqus_shortname = 'hbase'; // required: replace example with your forum shortname
    var disqus_url = 'http://hbase.apache.org/book';
    var disqus_identifier = 'perf.reading';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript><a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a><div class="navfooter"><hr><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="perf.writing.html">Prev</a>&nbsp;</td><td width="20%" align="center"><a accesskey="u" href="performance.html">Up</a></td><td width="40%" align="right">&nbsp;<a accesskey="n" href="perf.deleting.html">Next</a></td></tr><tr><td width="40%" align="left" valign="top">12.8.&nbsp;Writing to HBase&nbsp;</td><td width="20%" align="center"><a accesskey="h" href="book.html">Home</a></td><td width="40%" align="right" valign="top">&nbsp;12.10.&nbsp;Deleting from HBase</td></tr></table></div></body></html>